{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import backoff\n",
    "import spacy\n",
    "import warnings\n",
    "import json\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_lg # Using word vectors using a large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all OpenML datasets for semantic tagging\n",
    "all_datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "\n",
    "# List dataset 'did' to be used as an identifier \n",
    "data_id = []\n",
    "for i in range(len(all_datasets)):\n",
    "  data_id.append(all_datasets.iloc[i]['did'])\n",
    "\n",
    "# dictonary to hold {'did': dataset_decription}\n",
    "all_data_description = dict.fromkeys(data_id, \"\") \n",
    "\n",
    "\n",
    "for i in range(len(all_datasets)):\n",
    "  dataset_name = all_datasets.iloc[i]['name']\n",
    "  try:\n",
    "    data = openml.datasets.get_dataset(dataset_name, download_data = False, download_features_meta_data = False, download_qualities = False)\n",
    "    all_data_description[all_datasets.iloc[i]['did']] = data.description\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "print(len(all_data_description))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Alternatvely all openml dataset description is saved in a file, to be used later\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# # # Save data description as pickle file\n",
    "# # with open('dataset_description.pickle', 'wb') as f:\n",
    "# #     pickle.dump(all_data_description, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Load data decription \n",
    "# infile = open('dataset_description.pickle','rb')\n",
    "# all_data_description = pickle.load(infile)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tags\n",
    "\n",
    "tags = [\n",
    "\"Agriculture\",\n",
    "\"Astronomy\",\n",
    "\"Chemistry\",\n",
    "\"Computational Universe\",\n",
    "\"Computer Systems\",\n",
    "\"Culture\",\n",
    "\"Demographics\",\n",
    "\"Earth Science\",\n",
    "\"Economics\",\n",
    "\"Education\",\n",
    "\"Geography\",\n",
    "\"Government\",\n",
    "\"Health\",\n",
    "\"History\",\n",
    "\"Human Activities\",\n",
    "\"Images\",\n",
    "\"Language\",\n",
    "\"Life Science\",\n",
    "\"Machine Learning\",\n",
    "\"Manufacturing\",\n",
    "\"Mathematics\",\n",
    "\"Medicine\",\n",
    "\"Meteorology\",\n",
    "\"Physical Sciences\",\n",
    "\"Politics\",\n",
    "\"Social Media\",\n",
    "\"Sociology\",\n",
    "\"Statistics\",\n",
    "\"Text & Literature\",\n",
    "\"Transportation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompting GPT-3.5-turbo for picking semantic tag from the given list of tags, given dataset description\n",
    "\n",
    "# set up OpenAI API key\n",
    "client = OpenAI(api_key = \" \")\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 5  # seconds\n",
    "\n",
    "@backoff.on_exception(backoff.expo,\n",
    " (openai.InternalServerError, openai.OpenAIError, openai.APIStatusError, openai.RateLimitError, openai.APIError, openai.APIConnectionError, openai.Timeout, openai.APIResponseValidationError),\n",
    "                      max_tries = MAX_RETRIES,\n",
    "                      on_backoff=lambda details: print(f\"Retrying in {RETRY_DELAY} seconds...\"),\n",
    "                      on_giveup=lambda e: print(f\"Max retries reached. Unable to complete the request: {e}\")\n",
    "                      )\n",
    "def completion_using_message(messages, model = \"gpt-3.5-turbo\"):\n",
    "  completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def clean(response):\n",
    "  \"\"\"\n",
    "  clean GPT response\n",
    "  \"\"\"\n",
    "  response = response.strip().replace(\"Tags\",'').replace(\"Tag\", '').strip(\":\").strip().strip(\"'\").strip(\"[\").strip(\"]\").strip(\"'\").strip()\n",
    "  response = response.split(\",\")\n",
    "  for i in range(len(response)):\n",
    "    response[i] = response[i].strip().strip(\"'\").strip(\"'\").strip()\n",
    "\n",
    "  return response\n",
    "\n",
    "\n",
    "def find_closest_tag(word, tags):\n",
    "  \"\"\"\n",
    "  Fing tag semantically similar to GPT generated word by calculating similarity\n",
    "  \"\"\"\n",
    "\n",
    "  nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "  word_token = nlp(word)\n",
    "\n",
    "  highest_similarity = 0\n",
    "  semantic_tag = []\n",
    "  for i in range(len(tags)):\n",
    "    tag_token = nlp(tags[i])\n",
    "    simli = word_token.similarity(tag_token)\n",
    "    if simli > highest_similarity:\n",
    "      highest_similarity = simli\n",
    "      semantic_tag = tags[i]\n",
    "\n",
    "  return semantic_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query GPT one by one for each dataset description to get corresponding semantic tag. If the dataset decsription == '' or None, semantic_tag == ['No description']\n",
    "\n",
    "output = {} # dictionay with {'did':[semantic tags]}\n",
    "null_output = [] #  response by GPT == NULL and dataset description != None\n",
    "exceptions_dict = {} #  error produced by GPT, contains {'did':[Expection]}\n",
    "no_description = [] #  if dataset description == None\n",
    "\n",
    "max_token = 4097 # 1 token ~= 4 chars in English, source: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them \n",
    "\n",
    "with open('output_files/temp.txt', 'w') as writefile:\n",
    "  for key, value in all_data_description.items():\n",
    "    \n",
    "    semantic_tag = []\n",
    "    \n",
    "    if value == None:\n",
    "      semantic_tag = [\"No description\"]\n",
    "      output.update({key:['No description']})\n",
    "      no_description.append(key)\n",
    "\n",
    "    else:\n",
    "      \n",
    "      try:\n",
    "        \n",
    "        # To avoid token limit error, the dataset description is shortened to len = 8000, determined experimentally. \n",
    "        if len(value) > 8000: \n",
    "          value = value[:8000]\n",
    "\n",
    "        # GPT Prompt attempt 1\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert in assigning one or two semantic tags from given list of tags to each given dataset description. You will reply by only picking tags from the given list of tags. Answer as concisely as possible.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Dataset Description: {value}\\nTags: {tags}\\nTags: \",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = completion_using_message(messages)\n",
    "\n",
    "        # GPT Prompt attempt 2\n",
    "        if response == []:\n",
    "          messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert in assigning one semantic tag from given list of tags to each given dataset description. You will reply by only picking one tag from the given list of tags. Only pick tag from the given list of tags. Answer as concisely as possible.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Dataset Description: {value}\\nTags: {tags}\\n Assign one Tag to the dataset description: \",\n",
    "            },\n",
    "        ]\n",
    "          response = completion_using_message(messages)\n",
    "\n",
    "\n",
    "        if response != []:\n",
    "          response = clean(response)\n",
    "          for r in response:\n",
    "            r = r.strip()\n",
    "            if r not in tags:\n",
    "              word = find_closest_tag(r, tags)\n",
    "              if word not in semantic_tag and word != []:\n",
    "                semantic_tag.append(word)\n",
    "            else:\n",
    "              semantic_tag.append(r)\n",
    "\n",
    "        # Each dataset can have a maximum of 2 semantic tags\n",
    "        if len(semantic_tag) > 2:\n",
    "          semantic_tag = semantic_tag[:2]\n",
    "\n",
    "        if semantic_tag == []:\n",
    "          null_output.append(key)\n",
    "\n",
    "        print(f\"Tag for dataset: {key} is {semantic_tag}\")\n",
    "        output.update({key:semantic_tag})\n",
    "\n",
    "      except openai.OpenAIError as e:\n",
    "        exceptions_dict.update({key:e})\n",
    "        print(f\"exception for dataset: {key} is {e}\")\n",
    "        \n",
    "      except Exception as e:\n",
    "        exceptions_dict.update({key:e})\n",
    "        print(f\"exception for dataset: {key} is {e}\")\n",
    "\n",
    "\n",
    "  # Save the output as a json object.\n",
    "  json_object = json.dumps(output, indent=4)\n",
    "\n",
    "  writefile.write('{}\\n'.format(json_object))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
