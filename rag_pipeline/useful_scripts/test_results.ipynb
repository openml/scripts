{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\".\"), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from modules.llm import *\n",
    "from modules.utils import *\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\"../data/.langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finding device.\n",
      "[INFO] Device found: cpu\n",
      "{'rqa_prompt_template': 'This database is a list of metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}', 'num_return_documents': 50, 'embedding_model': 'BAAI/bge-large-en-v1.5', 'data_dir': '../data/', 'persist_dir': '../data/chroma_db', 'testing_flag': False, 'ignore_downloading_data': False, 'test_subset_2000': False, 'data_download_n_jobs': 20, 'training': False, 'temperature': 0.95, 'top_p': 0.95, 'search_type': 'similarity', 'reranking': False, 'long_context_reorder': False, 'device': 'cpu', 'type_of_data': 'dataset'}\n",
      "[INFO] Loading metadata from file.\n",
      "[INFO] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the configuration and device\n",
    "config = load_config_and_device(\"../config.json\")\n",
    "config[\"training\"] = False\n",
    "config[\"type_of_data\"] = \"dataset\"\n",
    "config[\"persist_dir\"] = \"../data/chroma_db\"\n",
    "config[\"data_dir\"] = \"../data/\"\n",
    "# load the persistent database using ChromaDB\n",
    "client = chromadb.PersistentClient(path=config[\"persist_dir\"])\n",
    "print(config)\n",
    "# Loading the metadata for all types\n",
    "\n",
    "# Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\n",
    "qa_dataset = setup_vector_db_and_qa(config=config, data_type=\"dataset\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    # model=\"phi3\"\n",
    "    model = \"qwen2:1.5b\" # super duper tiny model \n",
    ")  \n",
    "# assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableMap    \n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"\n",
    "# 1. Use the following pieces of context to answer the question at the end.\n",
    "# 2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\\n\n",
    "# 3. Keep the answer crisp and limited to 3,4 sentences.\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Helpful Answer:\"\"\"\n",
    "prompt = \"\"\"\n",
    "Give a short summary of the following retriever results. Be concise and do not include any information that is not present in the context. \\n Context : {context} \\n\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "prompt = \"\"\"\n",
    "1. Use the following pieces of context to answer the question at the end.\n",
    "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\\n\n",
    "3. Keep the answer crisp and limited to 3,4 sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Give a short summary of the following retriever results. Be concise and do not include any information that is not present in the context. \\n Context : {context} \\n\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "    template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this content: {context}\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_stuff_documents_chain(llm, QA_CHAIN_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679b5dbc39a246efb3513fce8bf14407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"eating disorder\"\n",
    "docs_res = qa_dataset.invoke(input = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please summarize the content concisely.\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_prompt = hub.pull(\"rlm/reduce-prompt\")\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'docs': [Document(page_content='(numeric)], 50018 : [50018 - Var50019 (numeric)], 50019 : [50019 - Var50020 (numeric)], 50020 : [50020 - Var50021 (numeric)], 50021 : [50021 - Var50022 (numeric)], 50022 : [50022 - Var50023 (numeric)], 50023 : [50023 - Var50024 (numeric)], 50024 : [50024 - Var50025 (numeric)], 50025 : [50025 - Var50026 (numeric)], 50026 : [50026 - Var50027 (numeric)], 50027 : [50027 - Var50028 (numeric)], 50028 : [50028 - Var50029 (numeric)], 50029 : [50029 - Var50030 (numeric)], 50030 : [50030 - Var50031 (numeric)], 50031 : [50031 - Var50032 (numeric)], 50032 : [50032 - Var50033 (numeric)], 50033 : [50033 - Var50034 (numeric)], 50034 : [50034 - Var50035 (numeric)], 50035 : [50035 - Var50036 (numeric)], 50036 : [50036 - Var50037 (numeric)], 50037 : [50037 - Var50038 (numeric)], 50038 : [50038 - Var50039 (numeric)], 50039 : [50039 - Var50040 (numeric)], 50040 : [50040 - Var50041 (numeric)], 50041 : [50041 - Var50042 (numeric)], 50042 : [50042 - Var50043 (numeric)], 50043 : [50043 - Var50044 (numeric)],', metadata={'did': 1080, 'name': 'rsctc2010_4'}),\n",
       "  Document(page_content='(numeric)], 50016 : [50016 - Var50017 (numeric)], 50017 : [50017 - Var50018 (numeric)], 50018 : [50018 - Var50019 (numeric)], 50019 : [50019 - Var50020 (numeric)], 50020 : [50020 - Var50021 (numeric)], 50021 : [50021 - Var50022 (numeric)], 50022 : [50022 - Var50023 (numeric)], 50023 : [50023 - Var50024 (numeric)], 50024 : [50024 - Var50025 (numeric)], 50025 : [50025 - Var50026 (numeric)], 50026 : [50026 - Var50027 (numeric)], 50027 : [50027 - Var50028 (numeric)], 50028 : [50028 - Var50029 (numeric)], 50029 : [50029 - Var50030 (numeric)], 50030 : [50030 - Var50031 (numeric)], 50031 : [50031 - Var50032 (numeric)], 50032 : [50032 - Var50033 (numeric)], 50033 : [50033 - Var50034 (numeric)], 50034 : [50034 - Var50035 (numeric)], 50035 : [50035 - Var50036 (numeric)], 50036 : [50036 - Var50037 (numeric)], 50037 : [50037 - Var50038 (numeric)], 50038 : [50038 - Var50039 (numeric)], 50039 : [50039 - Var50040 (numeric)], 50040 : [50040 - Var50041 (numeric)], 50041 : [50041 - Var50042 (numeric)],', metadata={'did': 1086, 'name': 'ovarianTumour'}),\n",
       "  Document(page_content='(numeric)], 249 : [249 - FCFP6_1024_248 (numeric)], 250 : [250 - FCFP6_1024_249 (numeric)], 251 : [251 - FCFP6_1024_250 (numeric)], 252 : [252 - FCFP6_1024_251 (numeric)], 253 : [253 - FCFP6_1024_252 (numeric)], 254 : [254 - FCFP6_1024_253 (numeric)], 255 : [255 - FCFP6_1024_254 (numeric)], 256 : [256 - FCFP6_1024_255 (numeric)], 257 : [257 - FCFP6_1024_256 (numeric)], 258 : [258 - FCFP6_1024_257 (numeric)], 259 : [259 - FCFP6_1024_258 (numeric)], 260 : [260 - FCFP6_1024_259 (numeric)], 261 : [261 - FCFP6_1024_260 (numeric)], 262 : [262 - FCFP6_1024_261 (numeric)], 263 : [263 - FCFP6_1024_262 (numeric)], 264 : [264 - FCFP6_1024_263 (numeric)], 265 : [265 - FCFP6_1024_264 (numeric)], 266 : [266 - FCFP6_1024_265 (numeric)], 267 : [267 - FCFP6_1024_266 (numeric)], 268 : [268 - FCFP6_1024_267 (numeric)], 269 : [269 - FCFP6_1024_268 (numeric)], 270 : [270 - FCFP6_1024_269 (numeric)], 271 : [271 - FCFP6_1024_270 (numeric)], 272 : [272 - FCFP6_1024_271 (numeric)], 273 : [273 - FCFP6_1024_272', metadata={'did': 3083, 'name': 'QSAR-TID-103169'}),\n",
       "  Document(page_content='(numeric)], 261 : [261 - RMI2_16_11438294_11439294 (numeric)], 262 : [262 - MYH11_16_15950887_15951887 (numeric)], 263 : [263 - PALB2_16_23652678_23653678 (numeric)], 264 : [264 - PRKCB_16_23846299_23847299 (numeric)], 265 : [265 - FUS_16_31190430_31191430 (numeric)], 266 : [266 - CYLD_16_50774960_50775960 (numeric)], 267 : [267 - CYLD_16_50775028_50776028 (numeric)], 268 : [268 - HERPUD1_16_56965001_56966001 (numeric)], 269 : [269 - CDH11_16_65156040_65157040 (numeric)], 270 : [270 - CBFB_16_67062049_67063049 (numeric)], 271 : [271 - CTCF_16_67595309_67596309 (numeric)], 272 : [272 - CDH1_16_68770192_68771192 (numeric)], 273 : [273 - ZFHX3_16_73082274_73083274 (numeric)], 274 : [274 - ZFHX3_16_73092534_73093534 (numeric)], 275 : [275 - RFWD3_16_74700779_74701779 (numeric)], 276 : [276 - MAF_16_79634622_79635622 (numeric)], 277 : [277 - CBFA2T3_16_89007608_89008608 (numeric)], 278 : [278 - CBFA2T3_16_89043504_89044504 (numeric)], 279 : [279 - FANCA_16_89883065_89884065 (numeric)], 280', metadata={'did': 46139, 'name': 'Cancer_Drug_Response_methylation'}),\n",
       "  Document(page_content='(numeric)], 33812 : [33812 - att_33813 (numeric)], 33813 : [33813 - att_33814 (numeric)], 33814 : [33814 - att_33815 (numeric)], 33815 : [33815 - att_33816 (numeric)], 33816 : [33816 - att_33817 (numeric)], 33817 : [33817 - att_33818 (numeric)], 33818 : [33818 - att_33819 (numeric)], 33819 : [33819 - att_33820 (numeric)], 33820 : [33820 - att_33821 (numeric)], 33821 : [33821 - att_33822 (numeric)], 33822 : [33822 - att_33823 (numeric)], 33823 : [33823 - att_33824 (numeric)], 33824 : [33824 - att_33825 (numeric)], 33825 : [33825 - att_33826 (numeric)], 33826 : [33826 - att_33827 (numeric)], 33827 : [33827 - att_33828 (numeric)], 33828 : [33828 - att_33829 (numeric)], 33829 : [33829 - att_33830 (numeric)], 33830 : [33830 - att_33831 (numeric)], 33831 : [33831 - att_33832 (numeric)], 33832 : [33832 - att_33833 (numeric)], 33833 : [33833 - att_33834 (numeric)], 33834 : [33834 - att_33835 (numeric)], 33835 : [33835 - att_33836 (numeric)], 33836 : [33836 - att_33837 (numeric)], 33837 :', metadata={'did': 1577, 'name': 'rcv1.binary'}),\n",
       "  Document(page_content='(numeric)], 248 : [248 - FCFP6_1024_247 (numeric)], 249 : [249 - FCFP6_1024_248 (numeric)], 250 : [250 - FCFP6_1024_249 (numeric)], 251 : [251 - FCFP6_1024_250 (numeric)], 252 : [252 - FCFP6_1024_251 (numeric)], 253 : [253 - FCFP6_1024_252 (numeric)], 254 : [254 - FCFP6_1024_253 (numeric)], 255 : [255 - FCFP6_1024_254 (numeric)], 256 : [256 - FCFP6_1024_255 (numeric)], 257 : [257 - FCFP6_1024_256 (numeric)], 258 : [258 - FCFP6_1024_257 (numeric)], 259 : [259 - FCFP6_1024_258 (numeric)], 260 : [260 - FCFP6_1024_259 (numeric)], 261 : [261 - FCFP6_1024_260 (numeric)], 262 : [262 - FCFP6_1024_261 (numeric)], 263 : [263 - FCFP6_1024_262 (numeric)], 264 : [264 - FCFP6_1024_263 (numeric)], 265 : [265 - FCFP6_1024_264 (numeric)], 266 : [266 - FCFP6_1024_265 (numeric)], 267 : [267 - FCFP6_1024_266 (numeric)], 268 : [268 - FCFP6_1024_267 (numeric)], 269 : [269 - FCFP6_1024_268 (numeric)], 270 : [270 - FCFP6_1024_269 (numeric)], 271 : [271 - FCFP6_1024_270 (numeric)], 272 : [272 - FCFP6_1024_271', metadata={'did': 3072, 'name': 'QSAR-TID-100671'}),\n",
       "  Document(page_content='(numeric)], 203 : [203 - FCFP6_1024_202 (numeric)], 204 : [204 - FCFP6_1024_203 (numeric)], 205 : [205 - FCFP6_1024_204 (numeric)], 206 : [206 - FCFP6_1024_205 (numeric)], 207 : [207 - FCFP6_1024_206 (numeric)], 208 : [208 - FCFP6_1024_207 (numeric)], 209 : [209 - FCFP6_1024_208 (numeric)], 210 : [210 - FCFP6_1024_209 (numeric)], 211 : [211 - FCFP6_1024_210 (numeric)], 212 : [212 - FCFP6_1024_211 (numeric)], 213 : [213 - FCFP6_1024_212 (numeric)], 214 : [214 - FCFP6_1024_213 (numeric)], 215 : [215 - FCFP6_1024_214 (numeric)], 216 : [216 - FCFP6_1024_215 (numeric)], 217 : [217 - FCFP6_1024_216 (numeric)], 218 : [218 - FCFP6_1024_217 (numeric)], 219 : [219 - FCFP6_1024_218 (numeric)], 220 : [220 - FCFP6_1024_219 (numeric)], 221 : [221 - FCFP6_1024_220 (numeric)], 222 : [222 - FCFP6_1024_221 (numeric)], 223 : [223 - FCFP6_1024_222 (numeric)], 224 : [224 - FCFP6_1024_223 (numeric)], 225 : [225 - FCFP6_1024_224 (numeric)], 226 : [226 - FCFP6_1024_225 (numeric)], 227 : [227 - FCFP6_1024_226', metadata={'did': 3502, 'name': 'QSAR-TID-100993'}),\n",
       "  Document(page_content='262 : [262 - MORGAN_2246340824 (numeric)], 263 : [263 - MORGAN_2246703798 (numeric)], 264 : [264 - MORGAN_2246728737 (numeric)], 265 : [265 - MORGAN_2246997334 (numeric)], 266 : [266 - MORGAN_2251845666 (numeric)], 267 : [267 - MORGAN_2257970297 (numeric)], 268 : [268 - MORGAN_2258843522 (numeric)], 269 : [269 - MORGAN_2273403373 (numeric)], 270 : [270 - MORGAN_2296493092 (numeric)], 271 : [271 - MORGAN_2296565457 (numeric)], 272 : [272 - MORGAN_2297887526 (numeric)], 273 : [273 - MORGAN_2302345434 (numeric)], 274 : [274 - MORGAN_2308348490 (numeric)], 275 : [275 - MORGAN_2309124039 (numeric)], 276 : [276 - MORGAN_2309545239 (numeric)], 277 : [277 - MORGAN_2353112200 (numeric)], 278 : [278 - MORGAN_2355616982 (numeric)], 279 : [279 - MORGAN_2360741695 (numeric)], 280 : [280 - MORGAN_2360831564 (numeric)], 281 : [281 - MORGAN_2370996728 (numeric)], 282 : [282 - MORGAN_2409787606 (numeric)], 283 : [283 - MORGAN_2423543607 (numeric)], 284 : [284 - MORGAN_2424973678 (numeric)], 285 : [285', metadata={'did': 43260, 'name': 'train_bbb'}),\n",
       "  Document(page_content='(numeric)], 247 : [247 - FCFP6_1024_247 (numeric)], 248 : [248 - FCFP6_1024_248 (numeric)], 249 : [249 - FCFP6_1024_249 (numeric)], 250 : [250 - FCFP6_1024_250 (numeric)], 251 : [251 - FCFP6_1024_251 (numeric)], 252 : [252 - FCFP6_1024_252 (numeric)], 253 : [253 - FCFP6_1024_253 (numeric)], 254 : [254 - FCFP6_1024_254 (numeric)], 255 : [255 - FCFP6_1024_255 (numeric)], 256 : [256 - FCFP6_1024_256 (numeric)], 257 : [257 - FCFP6_1024_257 (numeric)], 258 : [258 - FCFP6_1024_258 (numeric)], 259 : [259 - FCFP6_1024_259 (numeric)], 260 : [260 - FCFP6_1024_260 (numeric)], 261 : [261 - FCFP6_1024_261 (numeric)], 262 : [262 - FCFP6_1024_262 (numeric)], 263 : [263 - FCFP6_1024_263 (numeric)], 264 : [264 - FCFP6_1024_264 (numeric)], 265 : [265 - FCFP6_1024_265 (numeric)], 266 : [266 - FCFP6_1024_266 (numeric)], 267 : [267 - FCFP6_1024_267 (numeric)], 268 : [268 - FCFP6_1024_268 (numeric)], 269 : [269 - FCFP6_1024_269 (numeric)], 270 : [270 - FCFP6_1024_270 (numeric)], 271 : [271 - FCFP6_1024_271', metadata={'did': 44988, 'name': 'QSAR_TID_11'}),\n",
       "  Document(page_content='(numeric)], 267 : [267 - FCFP6_1024_266 (numeric)], 268 : [268 - FCFP6_1024_267 (numeric)], 269 : [269 - FCFP6_1024_268 (numeric)], 270 : [270 - FCFP6_1024_269 (numeric)], 271 : [271 - FCFP6_1024_270 (numeric)], 272 : [272 - FCFP6_1024_271 (numeric)], 273 : [273 - FCFP6_1024_272 (numeric)], 274 : [274 - FCFP6_1024_273 (numeric)], 275 : [275 - FCFP6_1024_274 (numeric)], 276 : [276 - FCFP6_1024_275 (numeric)], 277 : [277 - FCFP6_1024_276 (numeric)], 278 : [278 - FCFP6_1024_277 (numeric)], 279 : [279 - FCFP6_1024_278 (numeric)], 280 : [280 - FCFP6_1024_279 (numeric)], 281 : [281 - FCFP6_1024_280 (numeric)], 282 : [282 - FCFP6_1024_281 (numeric)], 283 : [283 - FCFP6_1024_282 (numeric)], 284 : [284 - FCFP6_1024_283 (numeric)], 285 : [285 - FCFP6_1024_284 (numeric)], 286 : [286 - FCFP6_1024_285 (numeric)], 287 : [287 - FCFP6_1024_286 (numeric)], 288 : [288 - FCFP6_1024_287 (numeric)], 289 : [289 - FCFP6_1024_288 (numeric)], 290 : [290 - FCFP6_1024_289 (numeric)], 291 : [291 - FCFP6_1024_290', metadata={'did': 3502, 'name': 'QSAR-TID-100993'})],\n",
       " 'question': 'eating disorder',\n",
       " 'text': 'This list contains a collection of documents related to various topics such as chemistry, machine learning, and statistics. It includes documents with unique identifiers that span different areas, from molecular structures (CFFP6) to statistical modeling using FCFP6. Additionally, there is another document titled \"QSAR_TID_11\" which could be a specific reference or set of information related to a particular quality standard or testing for something else within the context of chemistry and possibly broader fields like pharmacology or material science.'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_chain.invoke({\"docs\": docs_res, \"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='did - 44343, name - Meta_Album_BTS_Extended, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 8930.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 1802.0, NumberOfClasses - 26.0, NumberOfFeatures - 3.0, NumberOfInstances - 138367.0, NumberOfInstancesWithMissingValues - 138367.0, NumberOfMissingValues - 138367.0, NumberOfNumericFeatures - 1.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Boats Dataset (Extended)**\\n***', metadata={'did': 44343, 'name': 'Meta_Album_BTS_Extended'}),\n",
       "  Document(page_content='did - 44279, name - Meta_Album_BTS_Micro, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 20.0, NumberOfFeatures - 3.0, NumberOfInstances - 800.0, NumberOfInstancesWithMissingValues - 800.0, NumberOfMissingValues - 800.0, NumberOfNumericFeatures - 1.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Boats Dataset (Micro)**\\n***', metadata={'did': 44279, 'name': 'Meta_Album_BTS_Micro'}),\n",
       "  Document(page_content='did - 44309, name - Meta_Album_BTS_Mini, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 26.0, NumberOfFeatures - 3.0, NumberOfInstances - 1040.0, NumberOfInstancesWithMissingValues - 1040.0, NumberOfMissingValues - 1040.0, NumberOfNumericFeatures - 1.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Boats Dataset (Mini)**\\n***', metadata={'did': 44309, 'name': 'Meta_Album_BTS_Mini'}),\n",
       "  Document(page_content='***\\nThe original version of the Meta-Album boats dataset is called MARVEL dataset (https://github.com/avaapm/marveldataset2016). It has more than 138 000 images of 26 different maritime vessels in their natural background. Each class can have 1 802 to 8 930 images of variable resolutions. To preprocess this dataset, we either duplicate the top and bottom-most 3 rows or the left and right most 3 columns based on the orientation of the original image to create square images. No cropping was applied because the boats occupy most of the image, and applying this technique will lead to incomplete images. Finally, the square images were resized into 128x128 px using an anti-aliasing filter', metadata={'did': 44279, 'name': 'Meta_Album_BTS_Micro'}),\n",
       "  Document(page_content='did - 44282, name - Meta_Album_PLK_Mini, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 86.0, NumberOfFeatures - 3.0, NumberOfInstances - 3440.0, NumberOfInstancesWithMissingValues - 3440.0, NumberOfMissingValues - 3440.0, NumberOfNumericFeatures - 1.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Plankton Dataset (Mini)**\\n***', metadata={'did': 44282, 'name': 'Meta_Album_PLK_Mini'}),\n",
       "  Document(page_content='did - 44238, name - Meta_Album_PLK_Micro, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 20.0, NumberOfFeatures - 3.0, NumberOfInstances - 800.0, NumberOfInstancesWithMissingValues - 800.0, NumberOfMissingValues - 800.0, NumberOfNumericFeatures - 1.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Plankton Dataset (Micro)**\\n***', metadata={'did': 44238, 'name': 'Meta_Album_PLK_Micro'}),\n",
       "  Document(page_content='did - 44317, name - Meta_Album_PLK_Extended, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 187384.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 2.0, NumberOfClasses - 102.0, NumberOfFeatures - 3.0, NumberOfInstances - 473273.0, NumberOfInstancesWithMissingValues - 473273.0, NumberOfMissingValues - 473273.0, NumberOfNumericFeatures - 1.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Plankton Dataset (Extended)**\\n***', metadata={'did': 44317, 'name': 'Meta_Album_PLK_Extended'}),\n",
       "  Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/BTS.png)\\n\\n**Meta Album ID**: VCL.BTS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/BTS.html](https://meta-album.github.io/datasets/BTS.html)  \\n**Domain ID**: VCL  \\n**Domain Name**: Vehicles  \\n**Dataset ID**: BTS  \\n**Dataset Name**: Boats  \\n**Short Description**: Dataset with images of different boats  \\n**\\\\# Classes**: 26  \\n**\\\\# Images**: 138367  \\n**Keywords**: vehicles, boats  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Cite paper to use dataset  \\n**License (Meta-Album data release)**: CC BY-NC 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)', metadata={'did': 44343, 'name': 'Meta_Album_BTS_Extended'}),\n",
       "  Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/BTS.png)\\n\\n**Meta Album ID**: VCL.BTS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/BTS.html](https://meta-album.github.io/datasets/BTS.html)  \\n**Domain ID**: VCL  \\n**Domain Name**: Vehicles  \\n**Dataset ID**: BTS  \\n**Dataset Name**: Boats  \\n**Short Description**: Dataset with images of different boats  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: vehicles, boats  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Cite paper to use dataset  \\n**License (Meta-Album data release)**: CC BY-NC 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)', metadata={'did': 44279, 'name': 'Meta_Album_BTS_Micro'}),\n",
       "  Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/BTS.png)\\n\\n**Meta Album ID**: VCL.BTS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/BTS.html](https://meta-album.github.io/datasets/BTS.html)  \\n**Domain ID**: VCL  \\n**Domain Name**: Vehicles  \\n**Dataset ID**: BTS  \\n**Dataset Name**: Boats  \\n**Short Description**: Dataset with images of different boats  \\n**\\\\# Classes**: 26  \\n**\\\\# Images**: 1040  \\n**Keywords**: vehicles, boats  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Cite paper to use dataset  \\n**License (Meta-Album data release)**: CC BY-NC 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)', metadata={'did': 44309, 'name': 'Meta_Album_BTS_Mini'})],\n",
       " 'question': 'titanic',\n",
       " 'output_text': 'The original Meta Album dataset is called MARVEL dataset, which has more than 138,000 images of different maritime vessels in their natural backgrounds. The dataset was preprocessed by duplicating the top and bottom-most 3 rows or left and right most 3 columns based on image orientation to create square images without cropping. The images were resized into 128x128 px using an anti-aliasing filter for further processing. The license of this data set is CC BY-NC 4.0, with the original dataset having a Cite paper requirement and the Meta Album version being publicly available under the same license.'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input_documents\": docs_res, \"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableMap    \n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "                  llm=llm, \n",
    "                  prompt=QA_CHAIN_PROMPT, \n",
    "                  callbacks=None, \n",
    "                  verbose=True)\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"did\"],\n",
    "    template=\"Context:\\ncontent:{page_content}\\ndid:{did}\",\n",
    ")\n",
    "# document_prompt = PromptTemplate(\n",
    "#     input_variables=[\"page_content\"],\n",
    "#     template=\"Context:\\ncontent:{page_content}\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_documents_chain = StuffDocumentsChain(\n",
    "                  llm_chain=llm_chain,\n",
    "                  document_variable_name=\"context\",\n",
    "                  document_prompt=document_prompt,\n",
    "                  callbacks=None,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset.search_kwargs = {\"k\":10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocs_res\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:242\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m, docs: List[Document], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stuff all documents into one prompt and pass to LLM.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:198\u001b[0m, in \u001b[0;36mStuffDocumentsChain._get_inputs\u001b[0;34m(self, docs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03mFormat and then join all the documents together into one input with name\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Format each document according to the prompt\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m doc_strings \u001b[38;5;241m=\u001b[39m [format_document(doc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_prompt) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     k: v\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m    204\u001b[0m }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03mFormat and then join all the documents together into one input with name\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Format each document according to the prompt\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m doc_strings \u001b[38;5;241m=\u001b[39m [\u001b[43mformat_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_prompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    201\u001b[0m     k: v\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[1;32m    204\u001b[0m }\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/prompts/base.py:332\u001b[0m, in \u001b[0;36mformat_document\u001b[0;34m(doc, prompt)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_document\u001b[39m(doc: Document, prompt: BasePromptTemplate[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format a document into a string based on a prompt template.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    First, this pulls information from the document from two sources:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m            >>> \"Page 1: This is a joke\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43m_get_document_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/prompts/base.py:284\u001b[0m, in \u001b[0;36m_get_document_info\u001b[0;34m(doc, prompt)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_document_info\u001b[39m(doc: Document, prompt: BasePromptTemplate[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m--> 284\u001b[0m     base_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdoc\u001b[38;5;241m.\u001b[39mmetadata}\n\u001b[1;32m    285\u001b[0m     missing_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(prompt\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(base_info)\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_metadata) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "qa = combine_documents_chain.invoke({\"input_documents\": docs_res})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa = RetrievalQA(\n",
    "                  combine_documents_chain=combine_documents_chain,\n",
    "                  verbose=True,\n",
    "                  retriever=qa_dataset,\n",
    "                  return_source_documents=True,\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab05b0e157d4e72889686a7437d84da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "1. Use the following pieces of context to answer the question at the end.\n",
      "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
      "\n",
      "3. Keep the answer crisp and limited to 3,4 sentences.\n",
      "\n",
      "Context: Context:\n",
      "content:### Description\n",
      "\n",
      "This dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\n",
      "\n",
      "### Source\n",
      "```\n",
      "(a) Origin: \n",
      "Mushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
      "\n",
      "(b) Donor: \n",
      "Jeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\n",
      "```\n",
      "\n",
      "### Dataset description\n",
      "\n",
      "This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\n",
      "did:24\n",
      "\n",
      "Context:\n",
      "content:**Source**: Danish Fungi Dataset  \n",
      "**Source URL**: https://sites.google.com/view/danish-fungi-dataset  \n",
      "  \n",
      "**Original Author**: Lukas Picek, Milan Sulc, Jiri Matas, Jacob Heilmann-Clausen, Thomas S. Jeppesen, Thomas Laessoe, Tobias Froslev  \n",
      "**Original contact**: lukaspicek@gmail.com  \n",
      "\n",
      "**Meta Album author**: Felix Herron  \n",
      "**Created Date**: 01 March 2022  \n",
      "**Contact Name**: Ihsan Ullah  \n",
      "**Contact Email**: meta-album@chalearn.org  \n",
      "**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \n",
      "\n",
      "\n",
      "\n",
      "### **Cite this dataset**\n",
      "```\n",
      "@article{picek2021danish,\n",
      "    title={Danish Fungi 2020 - Not Just Another Image Recognition Dataset},\n",
      "    author={Lukas Picek and Milan Sulc and Jiri Matas and Jacob Heilmann-Clausen and Thomas S. Jeppesen and Thomas Laessoe and Tobias Froslev},\n",
      "    year={2021},\n",
      "    eprint={2103.10107},\n",
      "    archivePrefix={arXiv},\n",
      "    primaryClass={cs.CV}\n",
      "}\n",
      "```\n",
      "did:44272\n",
      "\n",
      "Context:\n",
      "content:### **Dataset Details**\n",
      "![](https://meta-album.github.io/assets/img/samples/FNG.png)\n",
      "\n",
      "**Meta Album ID**: PLT.FNG  \n",
      "**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \n",
      "**Domain ID**: PLT  \n",
      "**Domain Name**: Plants  \n",
      "**Dataset ID**: FNG  \n",
      "**Dataset Name**: Fungi  \n",
      "**Short Description**: Fungi dataset from Denmark  \n",
      "**\\# Classes**: 25  \n",
      "**\\# Images**: 15122  \n",
      "**Keywords**: fungi, ecology, plants  \n",
      "**Data Format**: images  \n",
      "**Image size**: 128x128  \n",
      "\n",
      "**License (original data release)**: BSD-3-Clause License  \n",
      "**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\n",
      " \n",
      "**License (Meta-Album data release)**: BSD-3-Clause License  \n",
      "**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)\n",
      "did:44335\n",
      "\n",
      "Context:\n",
      "content:### **Dataset Details**\n",
      "![](https://meta-album.github.io/assets/img/samples/FNG.png)\n",
      "\n",
      "**Meta Album ID**: PLT.FNG  \n",
      "**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \n",
      "**Domain ID**: PLT  \n",
      "**Domain Name**: Plants  \n",
      "**Dataset ID**: FNG  \n",
      "**Dataset Name**: Fungi  \n",
      "**Short Description**: Fungi dataset from Denmark  \n",
      "**\\# Classes**: 25  \n",
      "**\\# Images**: 1000  \n",
      "**Keywords**: fungi, ecology, plants  \n",
      "**Data Format**: images  \n",
      "**Image size**: 128x128  \n",
      "\n",
      "**License (original data release)**: BSD-3-Clause License  \n",
      "**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\n",
      " \n",
      "**License (Meta-Album data release)**: BSD-3-Clause License  \n",
      "**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)\n",
      "did:44302\n",
      "\n",
      "Context:\n",
      "content:did - 24, name - mushroom, version - 1, uploader - 1, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - 12.0, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 2480.0, NumberOfMissingValues - 2480.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \n",
      "**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
      "\n",
      "\n",
      "### Description\n",
      "\n",
      "This dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\n",
      "did:24\n",
      "\n",
      "Context:\n",
      "content:### **Dataset Details**\n",
      "![](https://meta-album.github.io/assets/img/samples/FNG.png)\n",
      "\n",
      "**Meta Album ID**: PLT.FNG  \n",
      "**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \n",
      "**Domain ID**: PLT  \n",
      "**Domain Name**: Plants  \n",
      "**Dataset ID**: FNG  \n",
      "**Dataset Name**: Fungi  \n",
      "**Short Description**: Fungi dataset from Denmark  \n",
      "**\\# Classes**: 20  \n",
      "**\\# Images**: 800  \n",
      "**Keywords**: fungi, ecology, plants  \n",
      "**Data Format**: images  \n",
      "**Image size**: 128x128  \n",
      "\n",
      "**License (original data release)**: BSD-3-Clause License  \n",
      "**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\n",
      " \n",
      "**License (Meta-Album data release)**: BSD-3-Clause License  \n",
      "**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)\n",
      "did:44272\n",
      "\n",
      "Context:\n",
      "content:did - 44272, name - Meta_Album_FNG_Micro, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 20.0, NumberOfFeatures - 3.0, NumberOfInstances - 800.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Micro)**\n",
      "***\n",
      "did:44272\n",
      "\n",
      "Context:\n",
      "content:Meta-Album Fungi dataset is created by sampling the Danish Fungi 2020 dataset(https://arxiv.org/abs/2103.10107), itself a sampling of the Atlas of Danish Fungi repository. The images and labels which enter this database are sourced by a group consisting of 3 300 citizen botanists, then verified by their peers using a ranking of each person reliability, then finally verified by experts working at the Atlas. Of the 128 classes in the original Danish Fungi 2020 dataset, FNG retains the 25 most populous classes, belonging to six genera, for a total of 15 122 images total, with min 372, and max 1 221 images per class. Each image contains a colored 128x128 image of a fungus or a piece of a fungus from the corresponding class. Because the initial data were of widely varying sizes, we needed to crop a significant portion of the images, which we implemented by taking the largest possible square with center at the middle of the initial image. We then scaled each squared image to the 128x128\n",
      "did:44272\n",
      "\n",
      "Context:\n",
      "content:did - 44335, name - Meta_Album_FNG_Extended, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 1221.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 372.0, NumberOfClasses - 25.0, NumberOfFeatures - 3.0, NumberOfInstances - 15122.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Extended)**\n",
      "***\n",
      "did:44335\n",
      "\n",
      "Context:\n",
      "content:did - 43922, name - mushroom, version - 3, uploader - 30861, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, qualities - AutoCorrelation : 0.726332635725717, Dimensionality : 0.002831117676021664, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NumberOfBinaryFeatures : 6.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 0.0,\n",
      "did:43922\n",
      "\n",
      "Question: Find me datasets about mushrooms and explain why they are relavant\n",
      "\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"Find me datasets about mushrooms and explain why they are relavant\"\n",
    "result = qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Find me datasets about mushrooms and explain why they are relavant',\n",
       " 'result': \"The mushroom dataset is a great example of a data set used for machine learning tasks in biology or other fields where the data is directly related to the research or application. Here's how it can be relevant:\\n\\n1. **Research**: It can serve as a benchmark for researchers in the field to test their algorithms and models. For instance, comparing results from different ML models on mushroom dataset could help identify which approaches are most effective.\\n\\n2. **Prediction and Classification**: By training models on mushroom datasets, practitioners can predict whether an image belongs to a certain genus or species. This can be crucial for applications like mushroom identification, forensics (where identifying the type of mushrooms in crime scenes), or even food quality control.\\n\\n3. **Data Science Methods**: The dataset can also be used as part of data science techniques such as anomaly detection or pattern recognition for improving machine learning models and algorithms. This is especially useful when dealing with a large volume of images, where computational resources become a bottleneck.\\n\\n4. **Real-World Applications**: In industries like agriculture, it could help in understanding the health of plants and crop yields based on mushroom characteristics. Similarly, environmental science or forensic analysis can use mushroom data to understand fungi-related diseases and their spread.\\n\\nIn summary, the mushroom dataset is relevant due to its direct relevance to biological research, machine learning applications, and real-world problems such as agriculture, food safety, or forensic investigations.\",\n",
       " 'source_documents': [Document(page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", metadata={'did': 24, 'name': 'mushroom'}),\n",
       "  Document(page_content='**Source**: Danish Fungi Dataset  \\n**Source URL**: https://sites.google.com/view/danish-fungi-dataset  \\n  \\n**Original Author**: Lukas Picek, Milan Sulc, Jiri Matas, Jacob Heilmann-Clausen, Thomas S. Jeppesen, Thomas Laessoe, Tobias Froslev  \\n**Original contact**: lukaspicek@gmail.com  \\n\\n**Meta Album author**: Felix Herron  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n@article{picek2021danish,\\n    title={Danish Fungi 2020 - Not Just Another Image Recognition Dataset},\\n    author={Lukas Picek and Milan Sulc and Jiri Matas and Jacob Heilmann-Clausen and Thomas S. Jeppesen and Thomas Laessoe and Tobias Froslev},\\n    year={2021},\\n    eprint={2103.10107},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CV}\\n}\\n```', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n",
       "  Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 15122  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n",
       "  Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 1000  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44302, 'name': 'Meta_Album_FNG_Mini'}),\n",
       "  Document(page_content='did - 24, name - mushroom, version - 1, uploader - 1, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - 12.0, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 2480.0, NumberOfMissingValues - 2480.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - **Author**: [Jeff Schlimmer](Jeffrey.Schlimmer@a.gp.cs.cmu.edu)  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/mushroom) - 1981     \\n**Please cite**:  The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n\\n### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.', metadata={'did': 24, 'name': 'mushroom'}),\n",
       "  Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n",
       "  Document(page_content='did - 44272, name - Meta_Album_FNG_Micro, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 40.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 40.0, NumberOfClasses - 20.0, NumberOfFeatures - 3.0, NumberOfInstances - 800.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Micro)**\\n***', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n",
       "  Document(page_content='Meta-Album Fungi dataset is created by sampling the Danish Fungi 2020 dataset(https://arxiv.org/abs/2103.10107), itself a sampling of the Atlas of Danish Fungi repository. The images and labels which enter this database are sourced by a group consisting of 3 300 citizen botanists, then verified by their peers using a ranking of each person reliability, then finally verified by experts working at the Atlas. Of the 128 classes in the original Danish Fungi 2020 dataset, FNG retains the 25 most populous classes, belonging to six genera, for a total of 15 122 images total, with min 372, and max 1 221 images per class. Each image contains a colored 128x128 image of a fungus or a piece of a fungus from the corresponding class. Because the initial data were of widely varying sizes, we needed to crop a significant portion of the images, which we implemented by taking the largest possible square with center at the middle of the initial image. We then scaled each squared image to the 128x128', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n",
       "  Document(page_content='did - 44335, name - Meta_Album_FNG_Extended, version - 1, uploader - 30980, status - active, format - arff, MajorityClassSize - 1221.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 372.0, NumberOfClasses - 25.0, NumberOfFeatures - 3.0, NumberOfInstances - 15122.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - ## **Meta-Album Fungi Dataset (Extended)**\\n***', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n",
       "  Document(page_content='did - 43922, name - mushroom, version - 3, uploader - 30861, status - active, format - ARFF, MajorityClassSize - 4208.0, MaxNominalAttDistinctValues - nan, MinorityClassSize - 3916.0, NumberOfClasses - 2.0, NumberOfFeatures - 23.0, NumberOfInstances - 8124.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 23.0, description - Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf, qualities - AutoCorrelation : 0.726332635725717, Dimensionality : 0.002831117676021664, MajorityClassPercentage : 51.7971442639094, MajorityClassSize : 4208.0, MinorityClassPercentage : 48.20285573609059, MinorityClassSize : 3916.0, NumberOfBinaryFeatures : 6.0, NumberOfClasses : 2.0, NumberOfFeatures : 23.0, NumberOfInstances : 8124.0, NumberOfInstancesWithMissingValues : 0.0, NumberOfMissingValues : 0.0, NumberOfNumericFeatures : 0.0,', metadata={'did': 43922, 'name': 'mushroom'})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided contexts for meta-album fungi datasets in Danish Fungi, here are two relevant datasets regarding mushrooms along with their relevance:\n",
      "\n",
      "1. Meta_Album_FNG_Micro (did: 44272): The Micro dataset is a subset of the larger Danish Fungi Dataset, focusing on micro-level data analysis using images and labels sourced from citizen botanists verified by their peers and experts. This dataset contains information about fungi samples in various classes (representing different species) with 15,122 instances (images). The Micro dataset is relevant for mushroom research because it provides a large number of images, enabling analysis through machine learning techniques to classify, identify and study the characteristics of specific mushroom species.\n",
      "\n",
      "2. Meta_Album_FNG_Extended (did: 44335): This dataset is an extended version of the Micro dataset that focuses on even fewer classes but with a larger number of instances per class, enabling more detailed analysis of individual species. The Extended dataset comprises 25 major mushroom classes and contains images sized at 128x1cvc, which facilitates image processing algorithms to extract features for classification tasks. Its relevance lies in its ability to provide a focused view on the most populous mushroom species within the Danish Fungi dataset, making it useful for studying those specific species and their unique characteristics.\n",
      "\n",
      "Both datasets offer valuable insights into mushrooms' diversity, distribution, and classification through citizen science collaborations with experts in mycology. They serve as excellent resources for developing machine learning models to improve fungal identification accuracy while allowing researchers to study the various properties of mushroom species within a Danish context.\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded.\n"
     ]
    }
   ],
   "source": [
    "embeddings = load_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ae66ef087f4becaa083f30a44da9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_query = \"How to create a pipeline object?\"\n",
    "query_vector = embeddings.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x39c654310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqa_dataset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = qa_dataset.invoke(\"Mushroom dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'did': 44272, 'name': 'Meta_Album_FNG_Micro'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='(numeric)], 18 : [18 - classe_circonference (nominal)], 19 : [19 - port_arbre (nominal)], 20 : [20 - vigueur_pousse (nominal)], 21 : [21 - champignon_collet (nominal)], 22 : [22 - insecte_collet (nominal)], 23 : [23 - plaie_collet (nominal)], 24 : [24 - observation_collet (numeric)], 25 : [25 - champignon_tronc (nominal)], 26 : [26 - insecte_tronc (nominal)], 27 : [27 - fissure_tronc (nominal)], 28 : [28 - rejet_tronc (nominal)], 29 : [29 - tuteurage_arbre (nominal)], 30 : [30 - canisse_arbre (nominal)], 31 : [31 - plaie_tronc (nominal)], 32 : [32 - observation_tronc (numeric)], 33 : [33 - champignon_houppier (nominal)], 34 : [34 - insecte_houppier (nominal)], 35 : [35 - fissure_houppier (nominal)], 36 : [36 - ecorce_incluse_houppier (nominal)], 37 : [37 - bois_mort_houppier (nominal)], 38 : [38 - plaie_houppier (nominal)], 39 : [39 - observation_houppier (numeric)], 40 : [40 - esperance_maintien (numeric)], 41 : [41 - contrainte (nominal)], 42 : [42 - classification_diagnostic', metadata={'did': 43025, 'name': 'dgf_96f4164d-956d-4c1c-b161-68724eb0ccdc'}),\n",
       " Document(page_content='- observation_houppier (numeric)], 40 : [40 - esperance_maintien (numeric)], 41 : [41 - contrainte (nominal)], 42 : [42 - classification_diagnostic (nominal)], 43 : [43 - date_diagnostic (nominal)], 44 : [44 - prescription_1 (nominal)], 45 : [45 - prescription_2 (nominal)], 46 : [46 - prescription_3 (numeric)], 47 : [47 - observation_travaux (numeric)], 48 : [48 - type_delai_1 (numeric)], 49 : [49 - delai_annee_programmation (numeric)], 50 : [50 - type_delai_2 (nominal)], 51 : [51 - delai_preconisation_2 (numeric)], 52 : [52 - delai_saison_programmation_2 (nominal)], 53 : [53 - delai_annee_programmation_2 (numeric)], 54 : [54 - reference_photo (numeric)], 55 : [55 - long (numeric)], 56 : [56 - lat (numeric)],,', metadata={'did': 43025, 'name': 'dgf_96f4164d-956d-4c1c-b161-68724eb0ccdc'})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in test if x.metadata[\"did\"] in [43025]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading metadata from file.\n",
      "[INFO] Metadata loaded.\n"
     ]
    }
   ],
   "source": [
    "openml_data_object, data_id, all_metadata = get_all_metadata_from_openml(\n",
    "        config=config\n",
    "    )\n",
    "# Create the combined metadata dataframe\n",
    "metadata_df, all_metadata = create_metadata_dataframe(\n",
    "    openml_data_object, data_id, all_metadata, config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the vector store\n",
    "vectordb = load_document_and_create_vector_store(\n",
    "    metadata_df, config=config, chroma_client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Meta-Album Fungi dataset is created by sampling the Danish Fungi 2020 dataset(https://arxiv.org/abs/2103.10107), itself a sampling of the Atlas of Danish Fungi repository. The images and labels which enter this database are sourced by a group consisting of 3 300 citizen botanists, then verified by their peers using a ranking of each person reliability, then finally verified by experts working at the Atlas. Of the 128 classes in the original Danish Fungi 2020 dataset, FNG retains the 25 most populous classes, belonging to six genera, for a total of 15 122 images total, with min 372, and max 1 221 images per class. Each image contains a colored 128x128 image of a fungus or a piece of a fungus from the corresponding class. Because the initial data were of widely varying sizes, we needed to crop a significant portion of the images, which we implemented by taking the largest possible square with center at the middle of the initial image. We then scaled each squared image to the 128x128', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/FNG.png)\\n\\n**Meta Album ID**: PLT.FNG  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/FNG.html](https://meta-album.github.io/datasets/FNG.html)  \\n**Domain ID**: PLT  \\n**Domain Name**: Plants  \\n**Dataset ID**: FNG  \\n**Dataset Name**: Fungi  \\n**Short Description**: Fungi dataset from Denmark  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 15122  \\n**Keywords**: fungi, ecology, plants  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: BSD-3-Clause License  \\n**License URL(original data release)**: https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE\\n \\n**License (Meta-Album data release)**: BSD-3-Clause License  \\n**License URL (Meta-Album data release)**: [https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE](https://github.com/picekl/DanishFungiDataset/blob/main/LICENSE)', metadata={'did': 44335, 'name': 'Meta_Album_FNG_Extended'}),\n",
       " Document(page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", metadata={'did': 24, 'name': 'mushroom'}),\n",
       " Document(page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", metadata={'did': 24, 'name': 'mushroom'})]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vector = embeddings.embed_query(\"mushroom dataset\")\n",
    "docs = vectordb.similarity_search_by_vector(embedding_vector)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mvectordb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:612\u001b[0m, in \u001b[0;36mChroma.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m include\n\u001b[0;32m--> 612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/api/models/Collection.py:211\u001b[0m, in \u001b[0;36mCollection.get\u001b[0;34m(self, ids, where, limit, offset, where_document, include)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m include:\n\u001b[1;32m    209\u001b[0m     valid_include\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m get_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_where\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_where_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_include\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m get_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m     get_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_loader(get_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muris\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:143\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/rate_limiting/__init__.py:45\u001b[0m, in \u001b[0;36mrate_limit.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Dict[Any, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# If not rate limiting provider is present, just run and return the function.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_system\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mchroma_rate_limiting_provider_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subject \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m     48\u001b[0m         subject_value \u001b[38;5;241m=\u001b[39m kwargs[subject]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/api/segment.py:517\u001b[0m, in \u001b[0;36mSegmentAPI._get\u001b[0;34m(self, collection_id, ids, where, sort, limit, offset, page, page_size, where_document, include)\u001b[0m\n\u001b[1;32m    514\u001b[0m     offset \u001b[38;5;241m=\u001b[39m (page \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m page_size\n\u001b[1;32m    515\u001b[0m     limit \u001b[38;5;241m=\u001b[39m page_size\n\u001b[0;32m--> 517\u001b[0m records \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata_segment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(records) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# Nothing to return if there are no records\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GetResult(\n\u001b[1;32m    528\u001b[0m         ids\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    529\u001b[0m         embeddings\u001b[38;5;241m=\u001b[39m[] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m         data\u001b[38;5;241m=\u001b[39m[] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m include \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/telemetry/opentelemetry/__init__.py:143\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/segment/impl/metadata/sqlite.py:216\u001b[0m, in \u001b[0;36mSqliteMetadataSegment.get_metadata\u001b[0;34m(self, where, where_document, ids, limit, offset)\u001b[0m\n\u001b[1;32m    212\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mwhere(embeddings_t\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39misin(embeddings_q))\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_db\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# Execute the query with the limit and offset already applied\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/chromadb/segment/impl/metadata/sqlite.py:225\u001b[0m, in \u001b[0;36mSqliteMetadataSegment._records\u001b[0;34m(self, cur, q)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given a cursor and a QueryBuilder, yield a generator of records. Assumes\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mcursor returns rows in ID order.\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m sql, params \u001b[38;5;241m=\u001b[39m get_sql(q)\n\u001b[0;32m--> 225\u001b[0m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m cur_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(cur\u001b[38;5;241m.\u001b[39mfetchone, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    228\u001b[0m group_iterator \u001b[38;5;241m=\u001b[39m groupby(cur_iterator, \u001b[38;5;28;01mlambda\u001b[39;00m r: \u001b[38;5;28mint\u001b[39m(r[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "source_nodes = vectordb.get('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BM25 retriever with nodes\n",
    "bm25_retriever = BM25Retriever.from_defaults(nodes=nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df[\"did\"][:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       **Author**: Unknown. Donated by David Sterling...\n",
       "1       Author: Alen Shapiro\\nSource: [UCI](https://ar...\n",
       "2       **Author**: Unknown\\n**Source**: Collective Ba...\n",
       "3       **Author**: H. Altay Guvenir, Burak Acar, Hald...\n",
       "4       **Author**: David J. Slate  \\n**Source**: [UCI...\n",
       "                              ...                        \n",
       "5661    **flare** dataset from the **KEEL** repository...\n",
       "5662    **flare** dataset from the **KEEL** repository...\n",
       "5663    daily pickup data for 329 FHV companies from J...\n",
       "5664    Monthly sales car parts. 2674 series. Jan 1998...\n",
       "5665                                            test rats\n",
       "Name: description, Length: 5666, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa = initialize_llm_chain(vectordb=vectordb, config=config)\n",
    "\n",
    "qa = vectordb.as_retriever(\n",
    "        search_type=config[\"search_type\"],\n",
    "        search_kwargs={\"k\": 5},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at BAAI/bge-base-en-v1.5 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = \"BAAI/bge-base-en-v1.5\"\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import llamacpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PromptTemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m(\n\u001b[1;32m      2\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    You are an intelligent agent. Answer the questions based on the given context.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    Context: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    Question: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    Answer:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt_template)\n\u001b[1;32m     11\u001b[0m pipe\u001b[38;5;241m=\u001b[39mpipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m               model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m               tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     14\u001b[0m               max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     15\u001b[0m               )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PromptTemplate' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at BAAI/bge-base-en-v1.5 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=config[\"embedding_model\"], \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline = question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectordb.as_retriever(), return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n* Dataset:\n\nData Set Information:\n\nData Set Information:\n\n### **Dataset Details**\n![](https://meta-album.github.io/assets/img/samples/RSD.png)\n\n**Meta Album ID**: REM_SEN.RSD  \n**Meta Album URL**: [https://meta-album.github.io/datasets/RSD.html](https://meta-album.github.io/datasets/RSD.html)  \n**Domain ID**: REM_SEN  \n**Domain Name**: Remote Sensing  \n**Dataset ID**: RSD  \n**Dataset Name**: RSD  \n**Short Description**: Remote sensing dataset  \n**\\# Classes**: 38  \n**\\# Images**: 1520  \n**Keywords**: remote sensing, satellite image, aerial image, land cover  \n**Data Format**: images  \n**Image size**: 128x128  \n\n**License (original data release)**: Open for research and non-profit purposes  \n**License (Meta-Album data release)**: CC BY-NC 4.0  \n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)\n\nQuestion: Find me a disaster dataset\nHelpful Answer: argument needs to be of type (SquadExample, dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind me a disaster dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:595\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    596\u001b[0m         _output_key\n\u001b[1;32m    597\u001b[0m     ]\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:145\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    596\u001b[0m         _output_key\n\u001b[1;32m    597\u001b[0m     ]\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:244\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/llm.py:316\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:819\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    808\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    809\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    810\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[1;32m    818\u001b[0m     ]\n\u001b[0;32m--> 819\u001b[0m     new_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m     llm_output \u001b[38;5;241m=\u001b[39m update_cache(\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache,\n\u001b[1;32m    824\u001b[0m         existing_prompts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    828\u001b[0m         prompts,\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    830\u001b[0m     run_info \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    831\u001b[0m         [RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id) \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers]\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/llms/huggingface_pipeline.py:267\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:391\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03mAnswer the question(s) given as inputs by using the context(s).\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    - **answer** (`str`) -- The answer to the question.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Convert inputs to features\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:219\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[0;32m--> 219\u001b[0m     inputs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/pipelines/question_answering.py:172\u001b[0m, in \u001b[0;36mQuestionAnsweringArgumentHandler.normalize\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QuestionAnsweringPipeline\u001b[38;5;241m.\u001b[39mcreate_sample(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mitem)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m argument needs to be of type (SquadExample, dict)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n* Dataset:\n\nData Set Information:\n\nData Set Information:\n\n### **Dataset Details**\n![](https://meta-album.github.io/assets/img/samples/RSD.png)\n\n**Meta Album ID**: REM_SEN.RSD  \n**Meta Album URL**: [https://meta-album.github.io/datasets/RSD.html](https://meta-album.github.io/datasets/RSD.html)  \n**Domain ID**: REM_SEN  \n**Domain Name**: Remote Sensing  \n**Dataset ID**: RSD  \n**Dataset Name**: RSD  \n**Short Description**: Remote sensing dataset  \n**\\# Classes**: 38  \n**\\# Images**: 1520  \n**Keywords**: remote sensing, satellite image, aerial image, land cover  \n**Data Format**: images  \n**Image size**: 128x128  \n\n**License (original data release)**: Open for research and non-profit purposes  \n**License (Meta-Album data release)**: CC BY-NC 4.0  \n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)\n\nQuestion: Find me a disaster dataset\nHelpful Answer: argument needs to be of type (SquadExample, dict)"
     ]
    }
   ],
   "source": [
    "question = \"Find me a disaster dataset\"\n",
    "result = qa.run({\"query\": question})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nprompt\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectordb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmushroom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:106\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 106\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:73\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_stuff_chain\u001b[39m(\n\u001b[1;32m     64\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     65\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StuffDocumentsChain:\n\u001b[1;32m     72\u001b[0m     _prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m stuff_prompt\u001b[38;5;241m.\u001b[39mPROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(llm)\n\u001b[0;32m---> 73\u001b[0m     llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[1;32m     82\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[1;32m     83\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     emit_warning()\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LLMChain\nprompt\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=vectordb.as_retriever(), chain_type_kwargs={\"prompt\": \"mushroom\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unsupported chain type: <module 'langchain_community.llms.llamacpp' from '/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/llms/llamacpp.py'>. Should be one of dict_keys(['stuff', 'map_reduce', 'refine', 'map_rerank'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retrieval_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mllamacpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectordb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:106\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 106\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:245\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m loader_mapping: Mapping[\u001b[38;5;28mstr\u001b[39m, LoadingCallable] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_stuff_chain,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_map_reduce_chain,\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefine\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_refine_chain,\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_rerank\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_map_rerank_chain,\n\u001b[1;32m    243\u001b[0m }\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[1;32m    250\u001b[0m     llm, verbose\u001b[38;5;241m=\u001b[39mverbose, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    251\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Got unsupported chain type: <module 'langchain_community.llms.llamacpp' from '/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/llms/llamacpp.py'>. Should be one of dict_keys(['stuff', 'map_reduce', 'refine', 'map_rerank'])"
     ]
    }
   ],
   "source": [
    "retrieval_chain = RetrievalQA.from_chain_type(model,llamacpp, retriever=vectordb.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retrieval_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectordb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvectordb\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmushroom dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectordb' is not defined"
     ]
    }
   ],
   "source": [
    "vectordb.similarity_search_with_relevance_scores(\"mushroom dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Meta-Album Fungi dataset is created by sampling the Danish Fungi 2020 dataset(https://arxiv.org/abs/2103.10107), itself a sampling of the Atlas of Danish Fungi repository. The images and labels which enter this database are sourced by a group consisting of 3 300 citizen botanists, then verified by their peers using a ranking of each person reliability, then finally verified by experts working at the Atlas. Of the 128 classes in the original Danish Fungi 2020 dataset, FNG retains the 25 most populous classes, belonging to six genera, for a total of 15 122 images total, with min 372, and max 1 221 images per class. Each image contains a colored 128x128 image of a fungus or a piece of a fungus from the corresponding class. Because the initial data were of widely varying sizes, we needed to crop a significant portion of the images, which we implemented by taking the largest possible square with center at the middle of the initial image. We then scaled each squared image to the 128x128', metadata={'did': 44272, 'name': 'Meta_Album_FNG_Micro'}),\n",
       " Document(page_content=\"### Description\\n\\nThis dataset describes mushrooms in terms of their physical characteristics. They are classified into: poisonous or edible.\\n\\n### Source\\n```\\n(a) Origin: \\nMushroom records are drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \\n\\n(b) Donor: \\nJeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\\n```\\n\\n### Dataset description\\n\\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ``leaflets three, let it be'' for Poisonous Oak and Ivy.\", metadata={'did': 24, 'name': 'mushroom'}),\n",
       " Document(page_content='* Dataset:', metadata={'did': 1558, 'name': 'bank-marketing'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44314, 'name': 'Meta_Album_MED_LF_Micro'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb.max_marginal_relevance_search(\"mushroom dataset\", lambda_mult=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the most frequently used words in the dataset descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_system = \"\"\"You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, \\\n",
    "answer the user question and provide citations. If none of the articles answer the question, just say you don't know.\n",
    "\n",
    "Remember, you must return both an answer and citations. A citation consists of a VERBATIM quote that \\\n",
    "justifies the answer and the ID of the quote article. Return a citation for every quote across all articles \\\n",
    "that justify the answer. Use the following format for your final output:\n",
    "\n",
    "<cited_answer>\n",
    "    <answer></answer>\n",
    "    <citations>\n",
    "        <citation><source_id></source_id><quote></quote></citation>\n",
    "        <citation><source_id></source_id><quote></quote></citation>\n",
    "        ...\n",
    "    </citations>\n",
    "</cited_answer>\n",
    "\n",
    "Here are the Wikipedia articles:{context}\"\"\"\n",
    "xml_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", xml_system), (\"human\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"rqa_prompt_template\"] = \"This database is a list of metadata. Use the following pieces of {context} to find the relevant document. Answer only from the context given using the {question} given. If you can't find the answer directly then return the document that best answers the question. Order the answers by relevance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"rqa_prompt_template\"] = xml_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading metadata from file.\n",
      "[INFO] Metadata loaded.\n",
      "[INFO] Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\n",
    "qa_dataset = setup_vector_db_and_qa(config=config, data_type=\"dataset\", client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>command</th>\n",
       "      <th>OpenML URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43339</td>\n",
       "      <td>Chocolate-Bar-Ratings</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43339)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>did - 43339, name - Chocolate-Bar-Ratings, ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43446</td>\n",
       "      <td>Online-Food-Delivery-Preferences-Bangalore-region</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43446)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>(string)], 28 : [28 - Unavailability (string)]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43825</td>\n",
       "      <td>Nutritional-values-for-common-foods-and-products</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43825)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>- serine (string)], 54 : [54 - threonine (stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42133</td>\n",
       "      <td>cacao_flavor</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(42133)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>did - 42133, name - cacao_flavor, version - 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43600</td>\n",
       "      <td>Updated-Wine-Enthusiast-Reviews</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43600)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>(numeric)], 4 : [4 - price (numeric)], 5 : [5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>42089</td>\n",
       "      <td>vancouver_employee</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(42089)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>2 : [2 - review_time (numeric)], 3 : [3 - revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>985</td>\n",
       "      <td>squash-unstored</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(985)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>10 : [10 - groundspot_a* (numeric)], 11 : [11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42078</td>\n",
       "      <td>beer_reviews</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(42078)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>(numeric)], 3 : [3 - review_overall (numeric)]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1498</td>\n",
       "      <td>sa-heart</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(1498)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>sbp  systolic blood pressure   \\ntobacco  cumu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>340</td>\n",
       "      <td>squash-stored</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(340)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>25. Acceptability - the acceptability of the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>43651</td>\n",
       "      <td>Winedata</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43651)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>(numeric)], 1 : [1 - country (string)], 2 : [2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>43589</td>\n",
       "      <td>combined-wine-data</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43589)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>Title: Wine Quality\\nSources Created by: Paulo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40498</td>\n",
       "      <td>wine-quality-white</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(40498)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>Available at: [@Elsevier] http://dx.doi.org/10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>43536</td>\n",
       "      <td>Chennai-Zomato-Restaurants-Data</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43536)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>- Zomato_URL (string)], 1 : [1 - Name_of_Resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>43758</td>\n",
       "      <td>Worldwide-Meat-Consumption</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43758)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>Meat consumption is related to living standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>372</td>\n",
       "      <td>internet_usage</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(372)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>2. http://www.public.iastate.edu/~dicook/\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>43739</td>\n",
       "      <td>Country_data</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43739)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>- Obesity_-_adult_prevalence_rate (string)], 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>43695</td>\n",
       "      <td>Red-Wine-Quality</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43695)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>Inspiration\\nUse machine learning to determine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>511</td>\n",
       "      <td>plasma_retinol</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(511)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>Summary:\\nObservational studies have suggested...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>24</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(24)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>### Description\\n\\nThis dataset describes mush...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1527</td>\n",
       "      <td>volcanoes-a1</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(1527)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>[Web Link], qualities - AutoCorrelation : 0.94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>43986</td>\n",
       "      <td>wine_quality</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43986)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>1. Title: Wine Quality \\n\\n2. Sources\\nCreated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>560</td>\n",
       "      <td>bodyfat</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(560)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>Behnke, A.R. and Wilmore, J.H. (1974). _Evalua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>43499</td>\n",
       "      <td>Bechdel-Cast-Data</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43499)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>[2 - title (string)], 3 : [3 - guest_name (str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>43574</td>\n",
       "      <td>Marginal-Revolution-Blog-Post-Data</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43574)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>1 : [1 - comment.count (numeric)], 2 : [2 - da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>171</td>\n",
       "      <td>primary-tumor</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(171)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>From: 22                      To: breast, qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>179</td>\n",
       "      <td>adult</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(179)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>e-mail: ronnyk '@' live.com for questions., qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>43607</td>\n",
       "      <td>WMO-Hurricane-Survival-Dataset</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43607)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>(string)], 4 : [4 - EDU_DATA (string)], 5 : [5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>393</td>\n",
       "      <td>la2s.wc</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(393)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>: [10173 - smell (numeric)], 10174 : [10174 - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>43767</td>\n",
       "      <td>Development-Category-(10k-courses)-from-Udemy</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(43767)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>2 : [2 - url (string)], 3 : [3 - is_paid (nomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>40</td>\n",
       "      <td>sonar</td>\n",
       "      <td>dataset = openml.datasets.get_dataset(40)</td>\n",
       "      <td>&lt;a href=\"https://www.openml.org/search?type=da...</td>\n",
       "      <td>From: M                       To: Mine, qualit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               name  \\\n",
       "0   43339                              Chocolate-Bar-Ratings   \n",
       "1   43446  Online-Food-Delivery-Preferences-Bangalore-region   \n",
       "2   43825   Nutritional-values-for-common-foods-and-products   \n",
       "3   42133                                       cacao_flavor   \n",
       "4   43600                    Updated-Wine-Enthusiast-Reviews   \n",
       "5   42089                                 vancouver_employee   \n",
       "6     985                                    squash-unstored   \n",
       "7   42078                                       beer_reviews   \n",
       "10   1498                                           sa-heart   \n",
       "11    340                                      squash-stored   \n",
       "14  43651                                           Winedata   \n",
       "17  43589                                 combined-wine-data   \n",
       "19  40498                                 wine-quality-white   \n",
       "21  43536                    Chennai-Zomato-Restaurants-Data   \n",
       "24  43758                         Worldwide-Meat-Consumption   \n",
       "26    372                                     internet_usage   \n",
       "27  43739                                       Country_data   \n",
       "28  43695                                   Red-Wine-Quality   \n",
       "29    511                                     plasma_retinol   \n",
       "30     24                                           mushroom   \n",
       "34   1527                                       volcanoes-a1   \n",
       "36  43986                                       wine_quality   \n",
       "37    560                                            bodyfat   \n",
       "39  43499                                  Bechdel-Cast-Data   \n",
       "40  43574                 Marginal-Revolution-Blog-Post-Data   \n",
       "41    171                                      primary-tumor   \n",
       "43    179                                              adult   \n",
       "45  43607                     WMO-Hurricane-Survival-Dataset   \n",
       "46    393                                            la2s.wc   \n",
       "47  43767      Development-Category-(10k-courses)-from-Udemy   \n",
       "48     40                                              sonar   \n",
       "\n",
       "                                         command  \\\n",
       "0   dataset = openml.datasets.get_dataset(43339)   \n",
       "1   dataset = openml.datasets.get_dataset(43446)   \n",
       "2   dataset = openml.datasets.get_dataset(43825)   \n",
       "3   dataset = openml.datasets.get_dataset(42133)   \n",
       "4   dataset = openml.datasets.get_dataset(43600)   \n",
       "5   dataset = openml.datasets.get_dataset(42089)   \n",
       "6     dataset = openml.datasets.get_dataset(985)   \n",
       "7   dataset = openml.datasets.get_dataset(42078)   \n",
       "10   dataset = openml.datasets.get_dataset(1498)   \n",
       "11    dataset = openml.datasets.get_dataset(340)   \n",
       "14  dataset = openml.datasets.get_dataset(43651)   \n",
       "17  dataset = openml.datasets.get_dataset(43589)   \n",
       "19  dataset = openml.datasets.get_dataset(40498)   \n",
       "21  dataset = openml.datasets.get_dataset(43536)   \n",
       "24  dataset = openml.datasets.get_dataset(43758)   \n",
       "26    dataset = openml.datasets.get_dataset(372)   \n",
       "27  dataset = openml.datasets.get_dataset(43739)   \n",
       "28  dataset = openml.datasets.get_dataset(43695)   \n",
       "29    dataset = openml.datasets.get_dataset(511)   \n",
       "30     dataset = openml.datasets.get_dataset(24)   \n",
       "34   dataset = openml.datasets.get_dataset(1527)   \n",
       "36  dataset = openml.datasets.get_dataset(43986)   \n",
       "37    dataset = openml.datasets.get_dataset(560)   \n",
       "39  dataset = openml.datasets.get_dataset(43499)   \n",
       "40  dataset = openml.datasets.get_dataset(43574)   \n",
       "41    dataset = openml.datasets.get_dataset(171)   \n",
       "43    dataset = openml.datasets.get_dataset(179)   \n",
       "45  dataset = openml.datasets.get_dataset(43607)   \n",
       "46    dataset = openml.datasets.get_dataset(393)   \n",
       "47  dataset = openml.datasets.get_dataset(43767)   \n",
       "48     dataset = openml.datasets.get_dataset(40)   \n",
       "\n",
       "                                           OpenML URL  \\\n",
       "0   <a href=\"https://www.openml.org/search?type=da...   \n",
       "1   <a href=\"https://www.openml.org/search?type=da...   \n",
       "2   <a href=\"https://www.openml.org/search?type=da...   \n",
       "3   <a href=\"https://www.openml.org/search?type=da...   \n",
       "4   <a href=\"https://www.openml.org/search?type=da...   \n",
       "5   <a href=\"https://www.openml.org/search?type=da...   \n",
       "6   <a href=\"https://www.openml.org/search?type=da...   \n",
       "7   <a href=\"https://www.openml.org/search?type=da...   \n",
       "10  <a href=\"https://www.openml.org/search?type=da...   \n",
       "11  <a href=\"https://www.openml.org/search?type=da...   \n",
       "14  <a href=\"https://www.openml.org/search?type=da...   \n",
       "17  <a href=\"https://www.openml.org/search?type=da...   \n",
       "19  <a href=\"https://www.openml.org/search?type=da...   \n",
       "21  <a href=\"https://www.openml.org/search?type=da...   \n",
       "24  <a href=\"https://www.openml.org/search?type=da...   \n",
       "26  <a href=\"https://www.openml.org/search?type=da...   \n",
       "27  <a href=\"https://www.openml.org/search?type=da...   \n",
       "28  <a href=\"https://www.openml.org/search?type=da...   \n",
       "29  <a href=\"https://www.openml.org/search?type=da...   \n",
       "30  <a href=\"https://www.openml.org/search?type=da...   \n",
       "34  <a href=\"https://www.openml.org/search?type=da...   \n",
       "36  <a href=\"https://www.openml.org/search?type=da...   \n",
       "37  <a href=\"https://www.openml.org/search?type=da...   \n",
       "39  <a href=\"https://www.openml.org/search?type=da...   \n",
       "40  <a href=\"https://www.openml.org/search?type=da...   \n",
       "41  <a href=\"https://www.openml.org/search?type=da...   \n",
       "43  <a href=\"https://www.openml.org/search?type=da...   \n",
       "45  <a href=\"https://www.openml.org/search?type=da...   \n",
       "46  <a href=\"https://www.openml.org/search?type=da...   \n",
       "47  <a href=\"https://www.openml.org/search?type=da...   \n",
       "48  <a href=\"https://www.openml.org/search?type=da...   \n",
       "\n",
       "                                          Description  \n",
       "0   did - 43339, name - Chocolate-Bar-Ratings, ver...  \n",
       "1   (string)], 28 : [28 - Unavailability (string)]...  \n",
       "2   - serine (string)], 54 : [54 - threonine (stri...  \n",
       "3   did - 42133, name - cacao_flavor, version - 3,...  \n",
       "4   (numeric)], 4 : [4 - price (numeric)], 5 : [5 ...  \n",
       "5   2 : [2 - review_time (numeric)], 3 : [3 - revi...  \n",
       "6   10 : [10 - groundspot_a* (numeric)], 11 : [11 ...  \n",
       "7   (numeric)], 3 : [3 - review_overall (numeric)]...  \n",
       "10  sbp  systolic blood pressure   \\ntobacco  cumu...  \n",
       "11  25. Acceptability - the acceptability of the f...  \n",
       "14  (numeric)], 1 : [1 - country (string)], 2 : [2...  \n",
       "17  Title: Wine Quality\\nSources Created by: Paulo...  \n",
       "19  Available at: [@Elsevier] http://dx.doi.org/10...  \n",
       "21  - Zomato_URL (string)], 1 : [1 - Name_of_Resta...  \n",
       "24  Meat consumption is related to living standard...  \n",
       "26  2. http://www.public.iastate.edu/~dicook/\\n   ...  \n",
       "27  - Obesity_-_adult_prevalence_rate (string)], 7...  \n",
       "28  Inspiration\\nUse machine learning to determine...  \n",
       "29  Summary:\\nObservational studies have suggested...  \n",
       "30  ### Description\\n\\nThis dataset describes mush...  \n",
       "34  [Web Link], qualities - AutoCorrelation : 0.94...  \n",
       "36  1. Title: Wine Quality \\n\\n2. Sources\\nCreated...  \n",
       "37  Behnke, A.R. and Wilmore, J.H. (1974). _Evalua...  \n",
       "39  [2 - title (string)], 3 : [3 - guest_name (str...  \n",
       "40  1 : [1 - comment.count (numeric)], 2 : [2 - da...  \n",
       "41  From: 22                      To: breast, qual...  \n",
       "43  e-mail: ronnyk '@' live.com for questions., qu...  \n",
       "45  (string)], 4 : [4 - EDU_DATA (string)], 5 : [5...  \n",
       "46  : [10173 - smell (numeric)], 10174 : [10174 - ...  \n",
       "47  2 : [2 - url (string)], 3 : [3 - is_paid (nomi...  \n",
       "48  From: M                       To: Mine, qualit...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result_from_query(\"find me a dataset about food preferences\", qa=qa_dataset, \n",
    "type_of_query=\"dataset\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"long_context_reorder\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "69.2 ms ± 21.1 ms per loop (mean ± std. dev. of 7 runs, 20 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 20\n",
    "get_result_from_query(\"find me a dataset about food preferences\", qa=qa_dataset, \n",
    "type_of_query=\"dataset\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/all_dataset_description.csv\", \"r\") as f:\n",
    "    dataset_descriptions = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'did,description,qualities,features\\n2,\"**Author**: Unknown. Donated by David Sterling and Wray Buntin'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_descriptions[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate and test multiple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n",
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Reordering results...\n",
      "[INFO] Reordering complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Aggregate results from multiple queries\n",
    "queries = [\"Find datasets related to COVID-19\", \"Find datasets related to COVID-19 and India\", \"COVID-19 dataset\", \"COVID-19 dataset India\", \"Mexico historical covid\"]\n",
    "combined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>43733</td>\n",
       "      <td>Covid-19--historical-data</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>43367</td>\n",
       "      <td>COVID-19-Indonesia-Dataset</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>43509</td>\n",
       "      <td>COVID-19-Rio-de-Janeiro-(City)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>43365</td>\n",
       "      <td>Covid-19-Case-Surveillance-Public-Use-Dataset</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>43400</td>\n",
       "      <td>COVID-19-community-mobility-reports</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>43405</td>\n",
       "      <td>Covid-19-Turkey-Daily-Details-Dataset</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>43410</td>\n",
       "      <td>Coronavirus-Disease-(COVID-19)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>43412</td>\n",
       "      <td>COVID-19-Visualisation-and-Epidemic-Analysis-Data</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43428</td>\n",
       "      <td>Mexico-COVID-19-clinical-data</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43457</td>\n",
       "      <td>COVID19-Dataset-with-100-World-Countries</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               name  query\n",
       "56  43733                          Covid-19--historical-data      5\n",
       "34  43367                         COVID-19-Indonesia-Dataset      4\n",
       "47  43509                     COVID-19-Rio-de-Janeiro-(City)      4\n",
       "33  43365      Covid-19-Case-Surveillance-Public-Use-Dataset      4\n",
       "35  43400                COVID-19-community-mobility-reports      4\n",
       "37  43405              Covid-19-Turkey-Daily-Details-Dataset      4\n",
       "38  43410                     Coronavirus-Disease-(COVID-19)      4\n",
       "39  43412  COVID-19-Visualisation-and-Epidemic-Analysis-Data      4\n",
       "41  43428                      Mexico-COVID-19-clinical-data      4\n",
       "42  43457           COVID19-Dataset-with-100-World-Countries      4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>44272</td>\n",
       "      <td>Meta_Album_FNG_Micro</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>44302</td>\n",
       "      <td>Meta_Album_FNG_Mini</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>44335</td>\n",
       "      <td>Meta_Album_FNG_Extended</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>44303</td>\n",
       "      <td>Meta_Album_PLT_DOC_Mini</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>44242</td>\n",
       "      <td>Meta_Album_PLT_VIL_Micro</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>44321</td>\n",
       "      <td>Meta_Album_PLT_VIL_Extended</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>44273</td>\n",
       "      <td>Meta_Album_PLT_DOC_Micro</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>44336</td>\n",
       "      <td>Meta_Album_PLT_DOC_Extended</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>44237</td>\n",
       "      <td>Meta_Album_BCT_Micro</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                         name  query\n",
       "80   44272         Meta_Album_FNG_Micro      6\n",
       "97   44302          Meta_Album_FNG_Mini      6\n",
       "1       24                     mushroom      6\n",
       "113  44335      Meta_Album_FNG_Extended      6\n",
       "98   44303      Meta_Album_PLT_DOC_Mini      5\n",
       "71   44242     Meta_Album_PLT_VIL_Micro      5\n",
       "108  44321  Meta_Album_PLT_VIL_Extended      5\n",
       "81   44273     Meta_Album_PLT_DOC_Micro      5\n",
       "114  44336  Meta_Album_PLT_DOC_Extended      5\n",
       "67   44237         Meta_Album_BCT_Micro      5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"Find me datasets related to mushrooms\", \"Fungi dataset\", \"Mushroom dataset\", \"shroom data\", \"types of mushroom\", \"earth fungus\", \"low features mushroom dataset\"]\n",
    "combined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\")\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44154</td>\n",
       "      <td>iris_reproduced</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>44299</td>\n",
       "      <td>Meta_Album_MED_LF_Mini</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>44273</td>\n",
       "      <td>Meta_Album_PLT_DOC_Micro</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44242</td>\n",
       "      <td>Meta_Album_PLT_VIL_Micro</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40983</td>\n",
       "      <td>wilt</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>44286</td>\n",
       "      <td>Meta_Album_PLT_VIL_Mini</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>44293</td>\n",
       "      <td>Meta_Album_PLT_NET_Mini</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1493</td>\n",
       "      <td>one-hundred-plants-texture</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1492</td>\n",
       "      <td>one-hundred-plants-shape</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1491</td>\n",
       "      <td>one-hundred-plants-margin</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                        name  query\n",
       "22  44154             iris_reproduced      3\n",
       "34  44299      Meta_Album_MED_LF_Mini      3\n",
       "29  44273    Meta_Album_PLT_DOC_Micro      3\n",
       "24  44242    Meta_Album_PLT_VIL_Micro      3\n",
       "20  40983                        wilt      3\n",
       "32  44286     Meta_Album_PLT_VIL_Mini      3\n",
       "33  44293     Meta_Album_PLT_NET_Mini      3\n",
       "16   1493  one-hundred-plants-texture      3\n",
       "15   1492    one-hundred-plants-shape      3\n",
       "14   1491   one-hundred-plants-margin      3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"plant datasets, low features\", \"plant, less number of features\", \"plant dataset, tiny\"]\n",
    "combined_df = aggregate_multiple_queries_and_count(queries,qa_dataset=qa_dataset, config=config, group_cols = [\"id\", \"name\"], sort_by=\"query\")\n",
    "combined_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
