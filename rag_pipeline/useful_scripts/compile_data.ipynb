{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pathlib in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (1.0.1)\n",
      "Requirement already satisfied: openml in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (0.14.1)\n",
      "Requirement already satisfied: pandas in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: pqdm in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (0.2.0)\n",
      "Collecting oslo.concurrency\n",
      "  Using cached oslo.concurrency-6.0.0-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: liac-arff>=2.4.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.6.2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.26.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.11.4)\n",
      "Requirement already satisfied: minio in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (7.1.13)\n",
      "Requirement already satisfied: pyarrow in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (16.1.0)\n",
      "Requirement already satisfied: requests in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.32.3)\n",
      "Requirement already satisfied: xmltodict in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (0.13.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pqdm) (4.11.0)\n",
      "Requirement already satisfied: tqdm in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pqdm) (4.66.1)\n",
      "Requirement already satisfied: bounded-pool-executor in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pqdm) (0.0.3)\n",
      "Collecting fasteners>=0.7.0\n",
      "  Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Collecting oslo.utils>=3.33.0\n",
      "  Using cached oslo.utils-7.1.0-py3-none-any.whl (101 kB)\n",
      "Collecting oslo.i18n>=3.15.3\n",
      "  Using cached oslo.i18n-6.3.0-py3-none-any.whl (46 kB)\n",
      "Collecting oslo.config>=5.2.0\n",
      "  Using cached oslo.config-9.4.0-py3-none-any.whl (128 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Using cached pbr-6.0.0-py2.py3-none-any.whl (107 kB)\n",
      "Collecting debtcollector>=1.2.0\n",
      "  Using cached debtcollector-3.0.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from oslo.config>=5.2.0->oslo.concurrency) (6.0.1)\n",
      "Collecting rfc3986>=1.2.0\n",
      "  Using cached rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting stevedore>=1.20.0\n",
      "  Using cached stevedore-5.2.0-py3-none-any.whl (49 kB)\n",
      "Collecting netaddr>=0.7.18\n",
      "  Downloading netaddr-1.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m334.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting iso8601>=0.1.11\n",
      "  Using cached iso8601-2.1.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting netifaces>=0.10.4\n",
      "  Using cached netifaces-0.11.0-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from oslo.utils>=3.33.0->oslo.concurrency) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from oslo.utils>=3.33.0->oslo.concurrency) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from python-dateutil->openml) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->openml) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->openml) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->openml) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->openml) (3.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn>=0.18->openml) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn>=0.18->openml) (1.2.0)\n",
      "Requirement already satisfied: wrapt>=1.7.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from debtcollector>=1.2.0->oslo.config>=5.2.0->oslo.concurrency) (1.16.0)\n",
      "Installing collected packages: netifaces, rfc3986, pbr, netaddr, iso8601, fasteners, debtcollector, stevedore, oslo.i18n, oslo.utils, oslo.config, oslo.concurrency\n",
      "Successfully installed debtcollector-3.0.0 fasteners-0.19 iso8601-2.1.0 netaddr-1.3.0 netifaces-0.11.0 oslo.concurrency-6.0.0 oslo.config-9.4.0 oslo.i18n-6.3.0 oslo.utils-7.1.0 pbr-6.0.0 rfc3986-2.0.0 stevedore-5.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pathlib openml pandas pqdm oslo.concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "# from pqdm.processes import pqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "import openml\n",
    "import pandas as pd\n",
    "from pqdm.threads import pqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_description(dataset_id, download_data = False) -> openml.datasets.dataset.OpenMLDataset:\n",
    "\n",
    "    \"\"\"\n",
    "    Get the dataset description from OpenML using the dataset id\n",
    "\n",
    "    Input: dataset_id (int) : The dataset id\n",
    "\n",
    "    Returns: data (openml.datasets.dataset.OpenMLDataset) : The dataset object from OpenML\n",
    "    \"\"\"\n",
    "    # TODO : Check for objects that do not have qualities being not downloaded properly\n",
    "    # try:\n",
    "    \n",
    "    data = openml.datasets.get_dataset(\n",
    "        dataset_id=dataset_id,\n",
    "        download_data=download_data,\n",
    "        download_qualities=True,\n",
    "        download_features_meta_data=True,\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package oslo.concurrency to ensure thread safety\n",
    "def get_all_metadata_from_openml(n_jobs = 10, download_data = False) -> Union[List, List]:\n",
    "    \"\"\"\n",
    "    Description: Gets all the metadata from OpenML for the type of data specified in the config.\n",
    "    \n",
    "    This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.\n",
    "\n",
    "    Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    save_filename = f\"kaggle_all_dataset_metadata.pkl\"\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(save_filename):\n",
    "        print(\"[INFO] File already exists. Loading from file.\")\n",
    "        with open(save_filename, \"rb\") as f:\n",
    "            openml_data_object, data_id, all_objects = pickle.load(f)\n",
    "        return openml_data_object, data_id, all_objects\n",
    "    else:\n",
    "        # the id column name is different for dataset and flow, so we need to handle that\n",
    "        id_column_name = \"did\"\n",
    "\n",
    "        # Gather all OpenML objects of the type of data\n",
    "        print(\"[INFO] Getting dataset metadata.\")\n",
    "        all_objects = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "    \n",
    "        print(\"[INFO] Checking downloaded files and skipping them.\")\n",
    "\n",
    "        data_id = [\n",
    "            int(all_objects.iloc[i][id_column_name])\n",
    "            for i in range(len(all_objects))\n",
    "        ]\n",
    "\n",
    "        # Initialize cache before using parallel (following OpenML python API documentation)\n",
    "        print(\"[INFO] Initializing cache.\")\n",
    "        get_dataset_description(data_id[0])\n",
    "\n",
    "        # Get all object metadata using n_jobs parallel threads from openml\n",
    "        print(\"[INFO] Getting dataset metadata from OpenML.\")\n",
    "        openml_data_object = pqdm(\n",
    "            data_id, get_dataset_description, n_jobs=10, download_data = download_data\n",
    "        )\n",
    "        \n",
    "        # Save the metadata to a file\n",
    "        print(\"[INFO] Saving metadata to file.\")\n",
    "        with open(save_filename, \"wb\") as f:\n",
    "            pickle.dump((openml_data_object, data_id, all_objects), f)\n",
    "\n",
    "        return openml_data_object, data_id, all_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with combined attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attribute(attribute, attr_name):\n",
    "    \"\"\"\n",
    "    Description: Extract an attribute from the OpenML object.\n",
    "\n",
    "    Input: attribute (object) : The OpenML object\n",
    "\n",
    "    Returns: The attribute value if it exists, else an empty string.\n",
    "    \"\"\"\n",
    "    return getattr(attribute, attr_name, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_information_df(\n",
    "    data_id, descriptions, joined_qualities, joined_features\n",
    "):\n",
    "    \"\"\"\n",
    "    Description: Create a dataframe with the combined information of the OpenML object.\n",
    "\n",
    "    Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object\n",
    "\n",
    "    Returns: The dataframe with the combined information of the OpenML object.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"did\": data_id,\n",
    "            \"description\": descriptions,\n",
    "            \"qualities\": joined_qualities,\n",
    "            \"features\": joined_features,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_metadata(all_dataset_metadata, all_data_description_df):\n",
    "    \"\"\"\n",
    "    Description: Combine the descriptions with the metadata table.\n",
    "\n",
    "    Input: all_dataset_metadata (pd.DataFrame) : The metadata table, \n",
    "    all_data_description_df (pd.DataFrame) : The descriptions\n",
    "\n",
    "    Returns: The combined metadata table.\n",
    "    \"\"\"\n",
    "    # Combine the descriptions with the metadata table\n",
    "    all_dataset_metadata = pd.merge(\n",
    "        all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n",
    "\n",
    "    # all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n",
    "    #     merge_all_columns_to_string, axis=1\n",
    "    # )\n",
    "    return all_dataset_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def join_attributes(attribute, attr_name):\n",
    "    \"\"\"\n",
    "    Description: Join the attributes of the OpenML object.\n",
    "\n",
    "    Input: attribute (object) : The OpenML object\n",
    "\n",
    "    Returns: The joined attributes if they exist, else an empty string.\n",
    "    example: \"column - value, column - value, ...\"\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        \" \".join([f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()])\n",
    "        if hasattr(attribute, attr_name)\n",
    "        else \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_attributes(attribute_str):\n",
    "    \"\"\"\n",
    "    Reverse the join_attributes function\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    for item in attribute_str.split(','):\n",
    "        if ':' in item:\n",
    "            try:\n",
    "                k, v = item.split(':')\n",
    "                attributes[k.strip()] = v.strip()\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(\n",
    "    openml_data_object, data_id, all_dataset_metadata, use_cache = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Description: Creates a dataframe with all the metadata, joined columns with all information for the type of data specified in the config. \n",
    "    \n",
    "    Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, config (dict) : The config dictionary\n",
    "\n",
    "    Returns: The combined metadata dataframe and the metadata table.\n",
    "    \"\"\"\n",
    "    if use_cache == True:\n",
    "        # If we are not training, we do not need to recreate the cache and can load the metadata from the files. If the files do not exist, raise an exception.\n",
    "        try:\n",
    "            with open(f\"kaggle_all_dataset_metadata.csv\", \"r\") as f:\n",
    "                all_data_description_df = pd.read_csv(f)\n",
    "            return all_data_description_df\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Metadata files do not exist. Please run without use_cache.\"\n",
    "            )\n",
    "    else:\n",
    "        descriptions = [\n",
    "            extract_attribute(attr, \"description\") for attr in openml_data_object\n",
    "        ]\n",
    "        joined_qualities = [\n",
    "            join_attributes(attr, \"qualities\") for attr in openml_data_object\n",
    "        ]\n",
    "        joined_features = [\n",
    "            join_attributes(attr, \"features\") for attr in openml_data_object\n",
    "        ]\n",
    "\n",
    "        all_data_description_df = create_combined_information_df(\n",
    "            data_id, descriptions, joined_qualities, joined_features\n",
    "        )\n",
    "        all_dataset_metadata = combine_metadata(\n",
    "            all_dataset_metadata, all_data_description_df\n",
    "        )\n",
    "\n",
    "\n",
    "        # Expand the qualities column into multiple columns\n",
    "        qualities_expanded = all_dataset_metadata['qualities'].apply(parse_attributes)\n",
    "        expanded_df = pd.DataFrame(qualities_expanded.tolist())\n",
    "        expanded_df = pd.concat([all_dataset_metadata, expanded_df], axis=1)\n",
    "\n",
    "        expanded_df.to_csv(\n",
    "            f\"kaggle_all_dataset_metadata.csv\", index=False\n",
    "        )\n",
    "\n",
    "        return expanded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataset_hash(ds1, ds2, hash_fn):\n",
    "    return hash_fn(ds1.to_string()) == hash_fn(ds2.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sum(df, column_list):\n",
    "    \"\"\"\n",
    "    Description: Get the sum of the columns in the dataframe\n",
    "    \"\"\"\n",
    "    # convert all columns to numeric\n",
    "    # df[column_list] = df[column_list].apply(pd.to_numeric, errors='coerce')\n",
    "    df.loc[:, column_list] = df.loc[:, column_list].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return abs(df[column_list].values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_by_func(df1, df2,id1, id2, column_list1, column_list2, func):\n",
    "    \"\"\"\n",
    "    Description: Compare two dataframes using a function given two ids and a list of columns to compare\n",
    "    TODO : For df2, change the id column to whatever is needed, or add an argument\n",
    "    \"\"\"\n",
    "    return func(df1[df1[\"did\"] == id1], column_list1) == func(df2[df2[\"did\"] == id2], column_list2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate everything\n",
    "- This takes a bit of time, even with the dataframes downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File already exists. Loading from file.\n"
     ]
    }
   ],
   "source": [
    "openml_data_object, data_id, all_metadata = get_all_metadata_from_openml(\n",
    "        n_jobs=10, download_data=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined metadata dataframe\n",
    "metadata_df  = create_metadata_dataframe(\n",
    "    openml_data_object, data_id, all_metadata, use_cache=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mushroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1 is the oldest one on openml\n",
    "mushroom = metadata_df[(metadata_df['name'] == 'mushroom') & (metadata_df['version'] == 1)]\n",
    "mushroom.to_csv('mushroom_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>uploader</th>\n",
       "      <th>status</th>\n",
       "      <th>format</th>\n",
       "      <th>MajorityClassSize</th>\n",
       "      <th>MaxNominalAttDistinctValues</th>\n",
       "      <th>MinorityClassSize</th>\n",
       "      <th>NumberOfClasses</th>\n",
       "      <th>...</th>\n",
       "      <th>RandomTreeDepth2AUC</th>\n",
       "      <th>RandomTreeDepth2ErrRate</th>\n",
       "      <th>RandomTreeDepth2Kappa</th>\n",
       "      <th>RandomTreeDepth3AUC</th>\n",
       "      <th>RandomTreeDepth3ErrRate</th>\n",
       "      <th>RandomTreeDepth3Kappa</th>\n",
       "      <th>StdvNominalAttDistinctValues</th>\n",
       "      <th>kNN1NAUC</th>\n",
       "      <th>kNN1NErrRate</th>\n",
       "      <th>kNN1NKappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>mushroom</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>active</td>\n",
       "      <td>ARFF</td>\n",
       "      <td>4208.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3916.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.999014</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.999014</td>\n",
       "      <td>3.180971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    did      name  version  uploader  status format  MajorityClassSize  \\\n",
       "19   24  mushroom        1         1  active   ARFF             4208.0   \n",
       "\n",
       "    MaxNominalAttDistinctValues  MinorityClassSize  NumberOfClasses  ...  \\\n",
       "19                         12.0             3916.0              2.0  ...   \n",
       "\n",
       "    RandomTreeDepth2AUC  RandomTreeDepth2ErrRate  RandomTreeDepth2Kappa  \\\n",
       "19             0.999525                 0.000492               0.999014   \n",
       "\n",
       "    RandomTreeDepth3AUC  RandomTreeDepth3ErrRate  RandomTreeDepth3Kappa  \\\n",
       "19             0.999525                 0.000492               0.999014   \n",
       "\n",
       "   StdvNominalAttDistinctValues kNN1NAUC kNN1NErrRate  kNN1NKappa  \n",
       "19                     3.180971      1.0          0.0         1.0  \n",
       "\n",
       "[1 rows x 126 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mushroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the actual CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_data = get_dataset_description(24, download_data=True)\n",
    "X, y, _, _ = downloaded_data.get_data(dataset_format='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('mushroom_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare dataset hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test using python inbuilt hash function, feel free to use your own\n",
    "compare_dataset_hash(X, X.T, hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparision_list = ['NumberOfInstances', 'NumberOfFeatures', 'NumberOfMissingValues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing for different datasets\n",
    "compare_by_func(metadata_df, metadata_df, 24, 25, comparision_list, comparision_list, get_dataset_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
