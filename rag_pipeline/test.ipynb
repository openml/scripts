{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel\n",
    "\n",
    "HUGGINGFACEHUB_API_KEY = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import (\n",
    "    HuggingFaceEmbeddings,\n",
    "    SentenceTransformerEmbeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path= \"data/chroma_db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs,\n",
    "    )\n",
    "\n",
    "collection = Chroma(\n",
    "            client=client,\n",
    "            persist_directory=\"data/chroma_db/\",\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=\"datasets\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = collection.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.76.tar.gz (49.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from llama-cpp-python) (1.26.2)\n",
      "Collecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp39-cp39-macosx_14_0_arm64.whl size=3479674 sha256=05b380c5dc517ec14d989f12aeebb43eb048bff2340d2c14d922958d698125a0\n",
      "  Stored in directory: /Users/eragon/Library/Caches/pip/wheels/9c/2d/1a/41fa1d661840353d7863b5a09d286d6519669bd6158ea478f6\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.76\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 387 tensors from /Users/eragon/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat-GGUF/snapshots/79b781af68a192ee2ec43cded6c3b5448a70df66/./qwen1_5-7b-chat-q2_k.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  10:                qwen2.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 10\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q2_K:  129 tensors\n",
      "llama_model_loader: - type q3_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 2.89 GiB (3.21 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.18 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2954.36 MiB\n",
      "...............................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   304.75 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1126\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.file_type': '10', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.model': 'gpt2', 'qwen2.use_parallel_residual': 'true', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.embedding_length': '4096', 'qwen2.attention.head_count_kv': '32', 'qwen2.context_length': '32768', 'qwen2.attention.head_count': '32', 'general.architecture': 'qwen2', 'qwen2.block_count': '32', 'qwen2.feed_forward_length': '11008', 'general.name': 'Qwen1.5-7B-Chat-AWQ-fp16'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "llama = Llama.from_pretrained(\n",
    "    # repo_id = \"Qwen/Qwen1.5-0.5B-Chat-GGUF\",\n",
    "    # filename=\"*q8_0.gguf\",\n",
    "    repo_id=  \"Qwen/Qwen1.5-7B-Chat-GGUF\",\n",
    "    filename=\"*q2_k.gguf\",\n",
    "    # repo_id = \"TheBloke/zephyr-7B-beta-GGUF\",\n",
    "    # filename=\"zephyr-7b-beta.Q4_0.gguf\",\n",
    "    # repo_id= \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "    # filename=\"mistral-7b-instruct-v0.1.Q4_0.gguf\",\n",
    "    # filename=\"mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 387 tensors from /Users/eragon/.cache/huggingface/hub/models--Qwen--Qwen1.5-7B-Chat-GGUF/snapshots/79b781af68a192ee2ec43cded6c3b5448a70df66/./qwen1_5-7b-chat-q2_k.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  10:                qwen2.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 10\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q2_K:  129 tensors\n",
      "llama_model_loader: - type q3_K:   96 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 2.89 GiB (3.21 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =    71.05 MiB, (  226.48 /  5461.34)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2954.36 MiB\n",
      "llm_load_tensors:      Metal buffer size =    71.04 MiB\n",
      "...............................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2\n",
      "ggml_metal_init: picking default device: Apple M2\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M2\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1984.00 MiB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =    18.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    19.05 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1126\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.file_type': '10', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.model': 'gpt2', 'qwen2.use_parallel_residual': 'true', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.embedding_length': '4096', 'qwen2.attention.head_count_kv': '32', 'qwen2.context_length': '32768', 'qwen2.attention.head_count': '32', 'general.architecture': 'qwen2', 'qwen2.block_count': '32', 'qwen2.feed_forward_length': '11008', 'general.name': 'Qwen1.5-7B-Chat-AWQ-fp16'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path =  llama.model_path, temperature=1, verbose=True, callback_manager=callback_manager, n_ctx=4096,n_gpu_layers = 1, top_p=.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict({\n",
    "    \"rqa_prompt_template\" : \"This database is a list of dataset metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}\",\n",
    "    \"num_return_documents\" : 50,\n",
    "    \"embedding_model\": \"BAAI/bge-base-en-v1.5\",\n",
    "    \"llm_model\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"persist_dir\": \"./data/chroma_db/\",\n",
    "    \"data_download_n_jobs\" : 20,\n",
    "    \"training\" : False,\n",
    "    \"search_type\" : \"similarity\"\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flashrank\n",
      "  Downloading FlashRank-0.2.5-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: onnxruntime in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from flashrank) (1.17.3)\n",
      "Requirement already satisfied: tqdm in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from flashrank) (4.66.1)\n",
      "Requirement already satisfied: tokenizers in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from flashrank) (0.19.1)\n",
      "Collecting llama-cpp-python==0.2.67\n",
      "  Downloading llama_cpp_python-0.2.67.tar.gz (42.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from flashrank) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from flashrank) (2.32.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from llama-cpp-python==0.2.67->flashrank) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from llama-cpp-python==0.2.67->flashrank) (4.11.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from llama-cpp-python==0.2.67->flashrank) (5.6.3)\n",
      "Requirement already satisfied: coloredlogs in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnxruntime->flashrank) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnxruntime->flashrank) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnxruntime->flashrank) (4.25.3)\n",
      "Requirement already satisfied: packaging in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnxruntime->flashrank) (23.2)\n",
      "Requirement already satisfied: sympy in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnxruntime->flashrank) (1.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->flashrank) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->flashrank) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->flashrank) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests->flashrank) (2022.12.7)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from tokenizers->flashrank) (0.23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (2023.6.0)\n",
      "Requirement already satisfied: filelock in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (3.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.67->flashrank) (2.1.5)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from coloredlogs->onnxruntime->flashrank) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from sympy->onnxruntime->flashrank) (1.3.0)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.67-cp39-cp39-macosx_14_0_arm64.whl size=3319614 sha256=a7bb91403f760faaf8a2494a08e9c5ef4ff5148d41e460eb83df0dbd0ecc7572\n",
      "  Stored in directory: /Users/eragon/Library/Caches/pip/wheels/47/62/18/bef9ab6ded5cba0227b7072fdd9598359d1d41e6eff029065e\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: llama-cpp-python, flashrank\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.76\n",
      "    Uninstalling llama_cpp_python-0.2.76:\n",
      "      Successfully uninstalled llama_cpp_python-0.2.76\n",
      "Successfully installed flashrank-0.2.5 llama-cpp-python-0.2.67\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flashrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import FlashrankRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank import Ranker, RerankRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Ranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.invoke(input=\"Find a dataset about mushrooms?\", config=\n",
    "# {\"temperature\" : .75, \"top-p\":.95})\n",
    "query = \"Find a dataset that was made by chinese authors\"\n",
    "# results = retriever.invoke(input=query, config = {\"temperature\" : .75, \"top-p\":.95})\n",
    "results = retriever.invoke(input=query, config = {\"temperature\" : .75, \"top-p\":.95,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Creators: \\nFang Zhou (fang.zhou '@' nottingham.edu.cn) \\nThe University of Nottinghan, Ningbo, China \\n\\nDonors of the Dataset: \\nFang Zhou (fang.zhou '@' nottingham.edu.cn) \\nClaire Q (eskoala '@' gmail.com) \\nRoss D. King (ross.king '@' manchester.ac.uk)\\n\\n\\nData Set Information:\\n\\nThe dataset was built from a personal collection of 1059 tracks covering 33 countries/area. The music used is traditional, ethnic or `world' only, as classified by the publishers of the product on which it appears. Any Western music is not included because its influence is global - what we seek are the aspects of music that most influence location. Thus, being able to specify a location with strong influence on the music is central. \\n\\nThe geographical location of origin was manually collected the information from the CD sleeve notes, and when this information was inadequate we searched other information sources. The location data is limited in precision to the country of origin.\", metadata={'did': 4544, 'name': 'GeographicalOriginalofMusic'}),\n",
       " Document(page_content='Acknowledgements\\nThese datasets have been gathered and cleaned up by Petra Isenberg, Pierre Dragicevic and Yvonne Jansen.  The original source can be found here.', metadata={'did': 43714, 'name': '1000-Cameras-Dataset'}),\n",
       " Document(page_content='This submission consists of 38 files, plus this README file.\\nEach file represents a data set analyzed in the book. The names\\nof the files correspond to the names used in the book. The data\\nfiles are written in plain ASCII (character) text. Missing\\nvalues are represented by \"M\" in all data files.\\n\\nMore information about the data sets and the book can be\\nobtained via gopher at the address\\nswis.stern.nyu.edu\\n\\nThe information is filed under\\n---> Academic Departments & Research Centers\\n---> Statistics and Operations Research\\n---> Publications\\n---> A Casebook for a First Course in Statistics and Data Analysis\\n---> Welcome!\\n\\nIt can also be accessed from the World Wide Web (WWW) using a\\nWWW browser (e.g., netscape) starting from the URL address\\nhttp://www.stern.nyu.edu/SOR/Casebook\\n\\n\\n\\nNOTICE: These datasets may be used freely for scientific,\\neducational and/or non-commercial purposes, provided suitable\\nacknowledgment is given (by citing the Chatterjee, Handcock and\\nSimonoff reference above).', metadata={'did': 664, 'name': 'chscase_census6'}),\n",
       " Document(page_content='## Reference\\n\\nYou can cite this dataset as below.\\n\\n```\\n@article{DBLP:journals/corr/QinL13,\\n  author    = {Tao Qin and\\n               Tie{-}Yan Liu},\\n  title     = {Introducing {LETOR} 4.0 Datasets},\\n  journal   = {CoRR},\\n  volume    = {abs/1306.2597},\\n  year      = {2013},\\n  url       = {http://arxiv.org/abs/1306.2597},\\n  timestamp = {Mon, 01 Jul 2013 20:31:25 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/QinL13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n```\\n\\n## Note:', metadata={'did': 45579, 'name': 'Microsoft'}),\n",
       " Document(page_content=\"Acknowledgements\\nThis data was entirely scraped via the Webdriver\\nInspiration\\nThe reason behind creating this dataset is pretty straightforward, I'm listing the books for all who need computer books, irrespective of the language and publication and all of that. So go ahead and use it to your liking, find out what book you should be reading next,  all possible approaches to exploring this dataset are welcome.\\nI started creating this dataset on Jan 18, 2021, and intend to update it frequently.\\nP.S. If you like this, please don't forget to give an upvote!\\nNotes\", metadata={'did': 43785, 'name': 'Goodreads-Computer-Books'}),\n",
       " Document(page_content=\"So I decided to make a dataset that would match my purposes.\\nFirst, I got ISBNs from Soumik's Goodreads-books dataset. Using those identifiers, I crawled the Google Books API to extract the books' information.\\nThen, I merged those results with some of the original columns from the dataset and after some cleaning I got the dataset you see here.\\nWhat can I do with this?\\nDifferent Exploratory Data Analysis, clustering of books by topics/category, content-based recommendation engine using different fields from the book's description. \\nWhy is this dataset smaller than Soumik's Goodreads-books?\\nMany of the ISBNs of that dataset did not return valid results from the Google Books API. I plan to update this in the future, using more fields (e.g., title, author) in the API requests, as to have a bigger dataset.\\nWhat did you use to build this dataset?\\nCheck out the repoistory here Google Books Crawler\\nAcknowledgements\", metadata={'did': 43542, 'name': '7k-Books'}),\n",
       " Document(page_content='The original dataset was obtained here:', metadata={'did': 43413, 'name': 'Store-20-Retail-Data-Analytics'}),\n",
       " Document(page_content='Thanks to the datasets comprehensiveness in terms of citing the source information of the text along with author names, date of publication and labels., qualities - AutoCorrelation : nan, Dimensionality : 0.0057251908396946565, MajorityClassPercentage : nan, MajorityClassSize : nan, MinorityClassPercentage : nan, MinorityClassSize : nan, NumberOfBinaryFeatures : 0.0, NumberOfClasses : nan, NumberOfFeatures : 12.0, NumberOfInstances : 2096.0, NumberOfInstancesWithMissingValues : 51.0, NumberOfMissingValues : 104.0, NumberOfNumericFeatures : 1.0, NumberOfSymbolicFeatures : 0.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 2.433206106870229, PercentageOfMissingValues : 0.4134860050890585, PercentageOfNumericFeatures : 8.333333333333332, PercentageOfSymbolicFeatures : 0.0,, features - 0 : [0 - author (string)], 1 : [1 - published (string)], 2 : [2 - title (string)], 3 : [3 - text (string)], 4 : [4 - language (string)], 5 : [5 - site_url (string)], 6 : [6 -', metadata={'did': 43394, 'name': 'Source-based-Fake-News-Classification'}),\n",
       " Document(page_content=\"Source:\\n\\na. Original owners \\nThe original data set we used is a subset of the well-known Reuters text categorization benchmark. The data was originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text categorization system. It is hosted by the UCI KDD repository: http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html. David D. Lewis is hosting valuable resources about this data (see http://www.daviddlewis.com/resources/testcollections/reuters21578/). We used the &ldquo;corporate acquisition&rdquo; text classification class pre-processed by Thorsten Joachims &lt;thorsten '@' joachims.org&gt;. The data is one of the examples of the software package SVM-Light., see http://svmlight.joachims.org/. The example can be downloaded from ftp://ftp-ai.cs.uni-dortmund.de/pub/Users/thorsten/svm_light/examples/example1.tar.gz.\", metadata={'did': 4136, 'name': 'Dexter'}),\n",
       " Document(page_content='did - 43785, name - Goodreads-Computer-Books, version - 1, uploader - 30125, status - active, format - arff, MajorityClassSize - nan, MaxNominalAttDistinctValues - nan, MinorityClassSize - nan, NumberOfClasses - 0.0, NumberOfFeatures - 9.0, NumberOfInstances - 1234.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 6.0, NumberOfSymbolicFeatures - 0.0, description - Context\\nThe reason for creating this dataset is the requirement of a good clean dataset of computer books. I had searched for datasets on books in Kaggle and I found out that while most of the datasets had a good amount of books listed, there were either major columns missing or grossly unclean data. I mean, you can\\'t determine how good a book is just from a few text reviews. So I collected this data from the Goodreads website from the \"Computer\" category to help people who are like this type of book.\\nAcknowledgements\\nThis data was entirely scraped via the Webdriver', metadata={'did': 43785, 'name': 'Goodreads-Computer-Books'}),\n",
       " Document(page_content='### Data Set Information\\n\\nThis is a data set containing 1080 documents of free text business descriptions of Brazilian companies categorized into a \\nsubset of 9 categories cataloged in a table called National Classification of Economic Activities (Classificação Nacional de \\nAtividade Econômicas - CNAE). The original texts were preprocessed to obtain the current data set: initially, it was kept only letters and then it was removed prepositions of the texts. Next, the words were transformed to their canonical form. Finally, \\neach document was represented as a vector, where the weight of each word is its frequency in the document. \\nThis data set is highly sparse (99.22% of the matrix is filled with zeros).\\n\\n### Attribute Information\\n\\nIn the dataset there are 857 attributes, 1 attributes with the class of instance and 856 with word frequency:\\n ```\\n1. category: range 1 - 9 (integer)   \\n2. 857. word frequency: (integer)\\n```\\n\\n### Relevant Papers', metadata={'did': 1468, 'name': 'cnae-9'}),\n",
       " Document(page_content='**Source**: OmniPrint  \\n**Source URL**: https://github.com/SunHaozhe/OmniPrint  \\n  \\n**Original Author**: Haozhe Sun  \\n**Original contact**: sunhaozhe275940200@gmail.com  \\n\\n**Meta Album author**: Haozhe Sun  \\n**Created Date**: 25 June 2021  \\n**Contact Name**: Haozhe Sun  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n@inproceedings{sun2021omniprint,\\n    title={OmniPrint: A Configurable Printed Character Synthesizer},\\n    author={Haozhe Sun and Wei-Wei Tu and Isabelle M Guyon},\\n    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\\n    year={2021},\\n    url={https://openreview.net/forum?id=R07XwJPmgpl}\\n}\\n```', metadata={'did': 44243, 'name': 'Meta_Album_MD_MIX_Micro'}),\n",
       " Document(page_content='did - 43791, name - Best-Books-of-the-19th-Century, version - 1, uploader - 30125, status - active, format - arff, MajorityClassSize - nan, MaxNominalAttDistinctValues - nan, MinorityClassSize - nan, NumberOfClasses - nan, NumberOfFeatures - 5.0, NumberOfInstances - 1101.0, NumberOfInstancesWithMissingValues - 4.0, NumberOfMissingValues - 4.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - This dataset has been scrapped off Goodreads to obtain land information about the best books of the 19th Century.  \\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nBook_Name\\nthe title of the book\\n\\n\\nAuthor_Name\\nthe author(s) of the book\\n\\n\\nDescription\\na brief description about the book\\n\\n\\nRating\\nrating given by Goodreads users', metadata={'did': 43791, 'name': 'Best-Books-of-the-19th-Century'}),\n",
       " Document(page_content=\"This dataset relied heavily on Soumik's Goodreads-books dataset., qualities - AutoCorrelation : nan, Dimensionality : 0.001762114537444934, MajorityClassPercentage : nan, MajorityClassSize : nan, MinorityClassPercentage : nan, MinorityClassSize : nan, NumberOfBinaryFeatures : 0.0, NumberOfClasses : nan, NumberOfFeatures : 12.0, NumberOfInstances : 6810.0, NumberOfInstancesWithMissingValues : 4630.0, NumberOfMissingValues : 5336.0, NumberOfNumericFeatures : 5.0, NumberOfSymbolicFeatures : 0.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 67.98825256975036, PercentageOfMissingValues : 6.529613313754283, PercentageOfNumericFeatures : 41.66666666666667, PercentageOfSymbolicFeatures : 0.0,, features - 0 : [0 - isbn13 (numeric)], 1 : [1 - isbn10 (string)], 2 : [2 - title (string)], 3 : [3 - subtitle (string)], 4 : [4 - authors (string)], 5 : [5 - categories (string)], 6 : [6 - thumbnail (string)], 7 : [7 - description (string)], 8 : [8 - published_year\", metadata={'did': 43542, 'name': '7k-Books'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_6.png)\\n\\n**Meta Album ID**: OCR.MD_6  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_6.html](https://meta-album.github.io/datasets/MD_6.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_6  \\n**Dataset Name**: OmniPrint-MD-6  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 703  \\n**\\\\# Images**: 28120  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44310, 'name': 'Meta_Album_MD_6_Mini'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_5_BIS.png)\\n\\n**Meta Album ID**: OCR.MD_5_BIS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_5_BIS.html](https://meta-album.github.io/datasets/MD_5_BIS.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_5_BIS  \\n**Dataset Name**: OmniPrint-MD-5-bis  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44252, 'name': 'Meta_Album_MD_5_BIS_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_5_BIS.png)\\n\\n**Meta Album ID**: OCR.MD_5_BIS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_5_BIS.html](https://meta-album.github.io/datasets/MD_5_BIS.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_5_BIS  \\n**Dataset Name**: OmniPrint-MD-5-bis  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 706  \\n**\\\\# Images**: 28240  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44296, 'name': 'Meta_Album_MD_5_BIS_Mini'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_6.png)\\n\\n**Meta Album ID**: OCR.MD_6  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_6.html](https://meta-album.github.io/datasets/MD_6.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_6  \\n**Dataset Name**: OmniPrint-MD-6  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44280, 'name': 'Meta_Album_MD_6_Micro'}),\n",
       " Document(page_content='Please cite the following paper if you use this dataset:', metadata={'did': 4540, 'name': 'ParkinsonSpeechDatasetwithMultipleTypesofSoundRecordings'}),\n",
       " Document(page_content='This is a dataset i generated during a hackathon for project purpose. Here i have scrapped data from Coursera official web site.  Our project aims to help any new learner get the right course to learn by just answering a few questions. It is an intelligent course recommendation system. Hence we had to scrap data from few educational websites. This is data scrapped from Coursera website. For the project visit: https://github.com/Siddharth1698/Coursu . Please do show your support by following us. I have just started to learn on data science and hope this dataset will be helpful to someone for his/her personal purposes. The scrapping code is here : https://github.com/Siddharth1698/Coursera-Course-Dataset\\nArticle about the dataset generation : https://medium.com/analytics-vidhya/web-scraping-and-coursera-8db6af45d83f', metadata={'did': 43381, 'name': 'Coursera-Course-Dataset'}),\n",
       " Document(page_content='### **Other versions of this dataset**', metadata={'did': 44243, 'name': 'Meta_Album_MD_MIX_Micro'}),\n",
       " Document(page_content='I also excluded any removed comments / comments whose author got deleted and comments deemed too short (less than 4 tokens) and changed the format (json - csv).\\nThis is primarily a NLP dataset, but in addition to the comments I added the 3 features I deemed the most important, I also aimed for feature type variety.\\nThe information kept here is:', metadata={'did': 43504, 'name': '1-million-Reddit-comments-from-40-subreddits'}),\n",
       " Document(page_content=\"This Dataset, consist of Tweets regarding Machine Learning, posted by Prof.Andrew Ng,  Co-founder of Coursera, and an Adjunct Professor of Computer Science at Stanford University. His machine learning course is the MOOC that had led to the founding of Coursera!\\nHe's one of my idol, in this huge field, Learned tonnes of new things from him, so decided to come up with this specific Dataset, which is dedicated towards Natural Language Processing.\\nThank-you!\\nContent\\nThe Dataset Consist of the 8 Columns, specifically Id, username, Name, Tweet, conversation_id, timezone ,time, and Polarity.\\nSo, Basically Polarity column is the sentiment Analysed column, \\nwhere,\\n1 = Neutral Tweets\\n2 = Slightly Negative Tweets\\n3 = Negative Tweets\\n4 = Slightly Positive Tweets\\n5 = Positive Tweets\\nInspiration\\nThis Dataset would be wonderful to work on Sentiment Analysis and NLP.\\nThankyou so Much to work on this Dataset!\\nIf you like this Dataset,\", metadata={'did': 43570, 'name': 'AndrewNg-Machine-Learning-Tweets'}),\n",
       " Document(page_content='Data Set Information:\\n \\ndataset are derived from the customers reviews in Amazon Commerce Website for authorship identification. Most previous studies conducted the identification experiments for two to ten authors. But in the online context, reviews to be identified usually have more potential authors, and normally classification algorithms are not adapted to large number of target classes. To examine the robustness of classification algorithms, we identified 50 of the most active users (represented by a unique ID and username) who frequently posted reviews in these newsgroups. The number of reviews we collected for each author is 30.', metadata={'did': 1457, 'name': 'amazon-commerce-reviews'}),\n",
       " Document(page_content='* Dataset:', metadata={'did': 1558, 'name': 'bank-marketing'}),\n",
       " Document(page_content=\"### Sources\\n\\nThe dataset was created by Tactile Srl, Brescia, Italy (http://www.tattile.it) and donated in 1994 to Semeion Research Center of Sciences of Communication, Rome, Italy (http://www.semeion.it), for machine learning research. \\n\\nFor any questions, e-mail Massimo Buscema (m.buscema '@' semeion.it) or Stefano Terzi (s.terzi '@' semeion.it)\\n\\n### DataSet Information\\n\\nA total of 1593 handwritten digits from around 80 persons were scanned, stretched in a rectangular box 16x16 in a gray scale of 256 values. Then each pixel of each image was scaled into a boolean (1/0) value using a fixed threshold. \\n\\nEach person wrote in a paper all the digits from 0 to 9, twice. The commitment was to write the digit the first time in the normal way (trying to write each digit accurately) and the second time in a fast way (with no accuracy). \\n\\nThe best validation protocol for this dataset seems to be a 5x2CV, 50% Tune (Train +Test) and completely blind 50% Validation\\n\\n### Attribute Information\", metadata={'did': 1501, 'name': 'semeion'}),\n",
       " Document(page_content='Acknowledgements\\nPlease cite the following papers if you use this dataset:', metadata={'did': 43380, 'name': 'US-Weather-Events-(2016---2020)'}),\n",
       " Document(page_content='#Dataset from the LIBSVM data repository', metadata={'did': 1433, 'name': 'svmguide1'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 26  \\n**\\\\# Images**: 1596  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44332, 'name': 'Meta_Album_MED_LF_Extended'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 1000  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44299, 'name': 'Meta_Album_MED_LF_Mini'}),\n",
       " Document(page_content='Information about the dataset\\nCLASSTYPE: nominal', metadata={'did': 446, 'name': 'prnn_crabs'}),\n",
       " Document(page_content='Information about the dataset\\n CLASSTYPE: nominal', metadata={'did': 23, 'name': 'cmc'}),\n",
       " Document(page_content='**Source**: NWPU-RESISC45 Dataset  \\n**Source URL**: https://gcheng-nwpu.github.io/  \\n  \\n**Original Author**: Gong Cheng, Junwei Han, and Xiaoqiang Lu  \\n**Original contact**: chenggong1119@gmail.com  \\n\\n**Meta Album author**: Phan Anh VU  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)', metadata={'did': 44246, 'name': 'Meta_Album_RESISC_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_MIX.png)\\n\\n**Meta Album ID**: OCR.MD_MIX  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_MIX.html](https://meta-album.github.io/datasets/MD_MIX.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_MIX  \\n**Dataset Name**: OmniPrint-MD-mix  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44243, 'name': 'Meta_Album_MD_MIX_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44314, 'name': 'Meta_Album_MED_LF_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_MIX.png)\\n\\n**Meta Album ID**: OCR.MD_MIX  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_MIX.html](https://meta-album.github.io/datasets/MD_MIX.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_MIX  \\n**Dataset Name**: OmniPrint-MD-mix  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 706  \\n**\\\\# Images**: 28240  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44287, 'name': 'Meta_Album_MD_MIX_Mini'}),\n",
       " Document(page_content='This S dump contains 22 data sets from the\\nbook Visualizing Data published by\\nHobart Press (books@hobart.com).\\nThe dump was created by data.dump()\\nand can be read back into S by data.restore().\\nThe name of each S data set is the name of\\nthe data set used in the book. To find the\\ndescription of the data set in the book look\\nunder the entry - data, name - in the index.\\nFor example, one data set is barley.\\nTo find the description of barley, look\\nin the index under the entry - data, barley.\\n\\nFile: ../data/visualizing/soil.csv', metadata={'did': 44041, 'name': 'visualizing_soil'}),\n",
       " Document(page_content=\"In order to keep the text data clean, I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed.\\nInspiration\\nThe dataset is large and informative, I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!\\n\\nFit a regression model on reviews and score to see which words are more indicative to a higher/lower score\\nPerform a sentiment analysis on the reviews\\nFind correlation between reviewer's nationality and scores.\\nBeautiful and informative visualization on the dataset.\\nClustering hotels based on reviews\\nSimple recommendation engine to the guest who is fond of a special characteristic of hotel.\", metadata={'did': 43712, 'name': '515K-Hotel-Reviews-Data-in-Europe'}),\n",
       " Document(page_content='**Past Usage**  \\nS. D. Bay and M. J. Pazzani. (1999) \"Detecting Group Differences: Mining Contrast Sets\". submitted.\\n\\n**Copyright Information**  \\nAll persons are granted a limited license to use and distribute this documentation and the accompanying data, subject to the following conditions:\\n* No fee may be charged for use or distribution.\\n* Publications and research reports based on the database must cite it appropriately. The citation should include the following: Steven Ruggles and Matthew Sobek et. al. Integrated Public Use Microdata Series: Version 2.0 Minneapolis: Historical Census Projects, University of Minnesota, 1997\\n\\nIf possible, citations should also include the URL for the IPUMS site: http://www.ipums.umn.edu/.\\n\\nIn addition, we request that users send us a copy of any publications, research reports, or educational material making use of the data or documentation. Send all electronic material to ipums@hist.umn.edu\\n\\nReferences', metadata={'did': 378, 'name': 'ipums_la_99-small'}),\n",
       " Document(page_content='Acknowledgements\\nAny publications based on this dataset should acknowledge the following: \\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.', metadata={'did': 43435, 'name': 'Default-of-Credit-Card-Clients-Dataset'}),\n",
       " Document(page_content='Acknowledgements\\nThe dataset was collected from http://www.myanimelist.net\\nInspiration', metadata={'did': 43508, 'name': 'Anime-Data'}),\n",
       " Document(page_content=\"Source:\\n\\nThis dataset was created by Pedro F. B. Silva and Andre R. S. Marcal using leaf specimens collected by Rubim Almeida da Silva at the Faculty of Science, University of Porto, Portugal.\\n\\n\\nData Set Information:\\n\\nFor further details on this dataset and/or its attributes, please read the 'ReadMe.pdf' file included and/or consult the Master's Thesis 'Development of a System for Automatic Plant Species Recognition' available at [Web Link].\\n\\n\\nAttribute Information:\", metadata={'did': 1482, 'name': 'leaf'}),\n",
       " Document(page_content='Data Set Information:', metadata={'did': 294, 'name': 'satellite_image'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/PLT_DOC.png)\\n\\n**Meta Album ID**: PLT_DIS.PLT_DOC  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/PLT_DOC.html](https://meta-album.github.io/datasets/PLT_DOC.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: PLT_DOC  \\n**Dataset Name**: Plant Doc  \\n**Short Description**: Plant disease dataset  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: plants, plant diseases,  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Creative Commons Attribution 4.0 International  \\n**License URL(original data release)**: https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset/blob/master/LICENSE.txt\\n \\n**License (Meta-Album data release)**: Creative Commons Attribution 4.0 International  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44273, 'name': 'Meta_Album_PLT_DOC_Micro'}),\n",
       " Document(page_content='The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML., qualities - AutoCorrelation : 0.9967486640028651, CfsSubsetEval_DecisionStumpAUC : 0.867329467101716, CfsSubsetEval_DecisionStumpErrRate : 0.0007584083256380636, CfsSubsetEval_DecisionStumpKappa : 0.751920067162127, CfsSubsetEval_NaiveBayesAUC : 0.867329467101716, CfsSubsetEval_NaiveBayesErrRate : 0.0007584083256380636, CfsSubsetEval_NaiveBayesKappa : 0.751920067162127, CfsSubsetEval_kNN1NAUC : 0.867329467101716, CfsSubsetEval_kNN1NErrRate : 0.0007584083256380636, CfsSubsetEval_kNN1NKappa : 0.751920067162127, ClassEntropy : 0.018343407709838624, DecisionStumpAUC : 0.8365634010656622, DecisionStumpErrRate : 0.0013553037671124655,', metadata={'did': 1597, 'name': 'creditcard'}),\n",
       " Document(page_content='**Source**: Medicinal Leaf Dataset  \\n**Source URL**: https://data.mendeley.com/datasets/nnytj2v3n5/1  \\n  \\n**Original Author**: S Roopashree, J Anitha  \\n**Original contact**:   \\n\\n**Meta Album author**: Phan Anh VU  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n @article{s_j_2020, \\n     title={Medicinal Leaf Dataset}, \\n     url={https://data.mendeley.com/datasets/nnytj2v3n5/1}, \\n     author={S, Roopashree and J, Anitha}, \\n     year={2020}, \\n     month={Oct},\\n     doi={10.17632/nnytj2v3n5.1},\\n     version={1},\\n     publisher={Mendeley Data}   \\n} \\n```', metadata={'did': 44299, 'name': 'Meta_Album_MED_LF_Mini'}),\n",
       " Document(page_content=\"b. Donor of database \\nThis version of the database was prepared for the NIPS 2003 variable and feature selection benchmark by Isabelle Guyon, 955 Creston Road, Berkeley, CA 94708, USA (isabelle '@' clopinet.com). \\n\\n\\nData Set Information:\\n\\nThe original data were formatted by Thorsten Joachims in the &ldquo;bag-of-words&rdquo; representation. There were 9947 features (of which 2562 are always zeros for all the examples) representing frequencies of occurrence of word stems in text. The task is to learn which Reuters articles are about 'corporate acquisitions'. We added a number of distractor feature called 'probes' having no predictive power. The order of the features and patterns were randomized. \\n\\nDEXTER -- Positive ex. -- Negative ex. -- Total \\nTraining set --150 -- 150 -- 300 \\nValidation set -- 150 -- 150 -- 300 \\nTest set -- 1000 -- 1000 -- 2000 \\nAll -- 1300 -- 1300 -- 2600\", metadata={'did': 4136, 'name': 'Dexter'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/PLT_DOC.png)\\n\\n**Meta Album ID**: PLT_DIS.PLT_DOC  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/PLT_DOC.html](https://meta-album.github.io/datasets/PLT_DOC.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: PLT_DOC  \\n**Dataset Name**: Plant Doc  \\n**Short Description**: Plant disease dataset  \\n**\\\\# Classes**: 27  \\n**\\\\# Images**: 1080  \\n**Keywords**: plants, plant diseases,  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Creative Commons Attribution 4.0 International  \\n**License URL(original data release)**: https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset/blob/master/LICENSE.txt\\n \\n**License (Meta-Album data release)**: Creative Commons Attribution 4.0 International  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44303, 'name': 'Meta_Album_PLT_DOC_Mini'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/PLT_DOC.png)\\n\\n**Meta Album ID**: PLT_DIS.PLT_DOC  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/PLT_DOC.html](https://meta-album.github.io/datasets/PLT_DOC.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: PLT_DOC  \\n**Dataset Name**: Plant Doc  \\n**Short Description**: Plant disease dataset  \\n**\\\\# Classes**: 27  \\n**\\\\# Images**: 2549  \\n**Keywords**: plants, plant diseases,  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Creative Commons Attribution 4.0 International  \\n**License URL(original data release)**: https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset/blob/master/LICENSE.txt\\n \\n**License (Meta-Album data release)**: Creative Commons Attribution 4.0 International  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44336, 'name': 'Meta_Album_PLT_DOC_Extended'}),\n",
       " Document(page_content='Acknowledgements\\nThe dataset was extracted from investing.com\\nInspiration', metadata={'did': 43391, 'name': 'Ethereum-Cryptocurrency-Historical-Dataset'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerankrequest = RerankRequest(query=query, passages=[{\"id\":result.metadata[\"did\"], \"text\":result.page_content} for result in results])\n",
    "ranking = ranker.rerank(rerankrequest)\n",
    "ids = [result[\"id\"] for result in ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Creators: \\nFang Zhou (fang.zhou '@' nottingham.edu.cn) \\nThe University of Nottinghan, Ningbo, China \\n\\nDonors of the Dataset: \\nFang Zhou (fang.zhou '@' nottingham.edu.cn) \\nClaire Q (eskoala '@' gmail.com) \\nRoss D. King (ross.king '@' manchester.ac.uk)\\n\\n\\nData Set Information:\\n\\nThe dataset was built from a personal collection of 1059 tracks covering 33 countries/area. The music used is traditional, ethnic or `world' only, as classified by the publishers of the product on which it appears. Any Western music is not included because its influence is global - what we seek are the aspects of music that most influence location. Thus, being able to specify a location with strong influence on the music is central. \\n\\nThe geographical location of origin was manually collected the information from the CD sleeve notes, and when this information was inadequate we searched other information sources. The location data is limited in precision to the country of origin.\", metadata={'did': 4544, 'name': 'GeographicalOriginalofMusic'}),\n",
       " Document(page_content='Acknowledgements\\nThese datasets have been gathered and cleaned up by Petra Isenberg, Pierre Dragicevic and Yvonne Jansen.  The original source can be found here.', metadata={'did': 43714, 'name': '1000-Cameras-Dataset'}),\n",
       " Document(page_content='This submission consists of 38 files, plus this README file.\\nEach file represents a data set analyzed in the book. The names\\nof the files correspond to the names used in the book. The data\\nfiles are written in plain ASCII (character) text. Missing\\nvalues are represented by \"M\" in all data files.\\n\\nMore information about the data sets and the book can be\\nobtained via gopher at the address\\nswis.stern.nyu.edu\\n\\nThe information is filed under\\n---> Academic Departments & Research Centers\\n---> Statistics and Operations Research\\n---> Publications\\n---> A Casebook for a First Course in Statistics and Data Analysis\\n---> Welcome!\\n\\nIt can also be accessed from the World Wide Web (WWW) using a\\nWWW browser (e.g., netscape) starting from the URL address\\nhttp://www.stern.nyu.edu/SOR/Casebook\\n\\n\\n\\nNOTICE: These datasets may be used freely for scientific,\\neducational and/or non-commercial purposes, provided suitable\\nacknowledgment is given (by citing the Chatterjee, Handcock and\\nSimonoff reference above).', metadata={'did': 664, 'name': 'chscase_census6'}),\n",
       " Document(page_content='## Reference\\n\\nYou can cite this dataset as below.\\n\\n```\\n@article{DBLP:journals/corr/QinL13,\\n  author    = {Tao Qin and\\n               Tie{-}Yan Liu},\\n  title     = {Introducing {LETOR} 4.0 Datasets},\\n  journal   = {CoRR},\\n  volume    = {abs/1306.2597},\\n  year      = {2013},\\n  url       = {http://arxiv.org/abs/1306.2597},\\n  timestamp = {Mon, 01 Jul 2013 20:31:25 +0200},\\n  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/QinL13},\\n  bibsource = {dblp computer science bibliography, http://dblp.org}\\n}\\n```\\n\\n## Note:', metadata={'did': 45579, 'name': 'Microsoft'}),\n",
       " Document(page_content=\"Acknowledgements\\nThis data was entirely scraped via the Webdriver\\nInspiration\\nThe reason behind creating this dataset is pretty straightforward, I'm listing the books for all who need computer books, irrespective of the language and publication and all of that. So go ahead and use it to your liking, find out what book you should be reading next,  all possible approaches to exploring this dataset are welcome.\\nI started creating this dataset on Jan 18, 2021, and intend to update it frequently.\\nP.S. If you like this, please don't forget to give an upvote!\\nNotes\", metadata={'did': 43785, 'name': 'Goodreads-Computer-Books'}),\n",
       " Document(page_content=\"So I decided to make a dataset that would match my purposes.\\nFirst, I got ISBNs from Soumik's Goodreads-books dataset. Using those identifiers, I crawled the Google Books API to extract the books' information.\\nThen, I merged those results with some of the original columns from the dataset and after some cleaning I got the dataset you see here.\\nWhat can I do with this?\\nDifferent Exploratory Data Analysis, clustering of books by topics/category, content-based recommendation engine using different fields from the book's description. \\nWhy is this dataset smaller than Soumik's Goodreads-books?\\nMany of the ISBNs of that dataset did not return valid results from the Google Books API. I plan to update this in the future, using more fields (e.g., title, author) in the API requests, as to have a bigger dataset.\\nWhat did you use to build this dataset?\\nCheck out the repoistory here Google Books Crawler\\nAcknowledgements\", metadata={'did': 43542, 'name': '7k-Books'}),\n",
       " Document(page_content='The original dataset was obtained here:', metadata={'did': 43413, 'name': 'Store-20-Retail-Data-Analytics'}),\n",
       " Document(page_content='Thanks to the datasets comprehensiveness in terms of citing the source information of the text along with author names, date of publication and labels., qualities - AutoCorrelation : nan, Dimensionality : 0.0057251908396946565, MajorityClassPercentage : nan, MajorityClassSize : nan, MinorityClassPercentage : nan, MinorityClassSize : nan, NumberOfBinaryFeatures : 0.0, NumberOfClasses : nan, NumberOfFeatures : 12.0, NumberOfInstances : 2096.0, NumberOfInstancesWithMissingValues : 51.0, NumberOfMissingValues : 104.0, NumberOfNumericFeatures : 1.0, NumberOfSymbolicFeatures : 0.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 2.433206106870229, PercentageOfMissingValues : 0.4134860050890585, PercentageOfNumericFeatures : 8.333333333333332, PercentageOfSymbolicFeatures : 0.0,, features - 0 : [0 - author (string)], 1 : [1 - published (string)], 2 : [2 - title (string)], 3 : [3 - text (string)], 4 : [4 - language (string)], 5 : [5 - site_url (string)], 6 : [6 -', metadata={'did': 43394, 'name': 'Source-based-Fake-News-Classification'}),\n",
       " Document(page_content=\"Source:\\n\\na. Original owners \\nThe original data set we used is a subset of the well-known Reuters text categorization benchmark. The data was originally collected and labeled by Carnegie Group, Inc. and Reuters, Ltd. in the course of developing the CONSTRUE text categorization system. It is hosted by the UCI KDD repository: http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html. David D. Lewis is hosting valuable resources about this data (see http://www.daviddlewis.com/resources/testcollections/reuters21578/). We used the &ldquo;corporate acquisition&rdquo; text classification class pre-processed by Thorsten Joachims &lt;thorsten '@' joachims.org&gt;. The data is one of the examples of the software package SVM-Light., see http://svmlight.joachims.org/. The example can be downloaded from ftp://ftp-ai.cs.uni-dortmund.de/pub/Users/thorsten/svm_light/examples/example1.tar.gz.\", metadata={'did': 4136, 'name': 'Dexter'}),\n",
       " Document(page_content='did - 43785, name - Goodreads-Computer-Books, version - 1, uploader - 30125, status - active, format - arff, MajorityClassSize - nan, MaxNominalAttDistinctValues - nan, MinorityClassSize - nan, NumberOfClasses - 0.0, NumberOfFeatures - 9.0, NumberOfInstances - 1234.0, NumberOfInstancesWithMissingValues - 0.0, NumberOfMissingValues - 0.0, NumberOfNumericFeatures - 6.0, NumberOfSymbolicFeatures - 0.0, description - Context\\nThe reason for creating this dataset is the requirement of a good clean dataset of computer books. I had searched for datasets on books in Kaggle and I found out that while most of the datasets had a good amount of books listed, there were either major columns missing or grossly unclean data. I mean, you can\\'t determine how good a book is just from a few text reviews. So I collected this data from the Goodreads website from the \"Computer\" category to help people who are like this type of book.\\nAcknowledgements\\nThis data was entirely scraped via the Webdriver', metadata={'did': 43785, 'name': 'Goodreads-Computer-Books'}),\n",
       " Document(page_content='### Data Set Information\\n\\nThis is a data set containing 1080 documents of free text business descriptions of Brazilian companies categorized into a \\nsubset of 9 categories cataloged in a table called National Classification of Economic Activities (Classificação Nacional de \\nAtividade Econômicas - CNAE). The original texts were preprocessed to obtain the current data set: initially, it was kept only letters and then it was removed prepositions of the texts. Next, the words were transformed to their canonical form. Finally, \\neach document was represented as a vector, where the weight of each word is its frequency in the document. \\nThis data set is highly sparse (99.22% of the matrix is filled with zeros).\\n\\n### Attribute Information\\n\\nIn the dataset there are 857 attributes, 1 attributes with the class of instance and 856 with word frequency:\\n ```\\n1. category: range 1 - 9 (integer)   \\n2. 857. word frequency: (integer)\\n```\\n\\n### Relevant Papers', metadata={'did': 1468, 'name': 'cnae-9'}),\n",
       " Document(page_content='**Source**: OmniPrint  \\n**Source URL**: https://github.com/SunHaozhe/OmniPrint  \\n  \\n**Original Author**: Haozhe Sun  \\n**Original contact**: sunhaozhe275940200@gmail.com  \\n\\n**Meta Album author**: Haozhe Sun  \\n**Created Date**: 25 June 2021  \\n**Contact Name**: Haozhe Sun  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n@inproceedings{sun2021omniprint,\\n    title={OmniPrint: A Configurable Printed Character Synthesizer},\\n    author={Haozhe Sun and Wei-Wei Tu and Isabelle M Guyon},\\n    booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},\\n    year={2021},\\n    url={https://openreview.net/forum?id=R07XwJPmgpl}\\n}\\n```', metadata={'did': 44243, 'name': 'Meta_Album_MD_MIX_Micro'}),\n",
       " Document(page_content='did - 43791, name - Best-Books-of-the-19th-Century, version - 1, uploader - 30125, status - active, format - arff, MajorityClassSize - nan, MaxNominalAttDistinctValues - nan, MinorityClassSize - nan, NumberOfClasses - nan, NumberOfFeatures - 5.0, NumberOfInstances - 1101.0, NumberOfInstancesWithMissingValues - 4.0, NumberOfMissingValues - 4.0, NumberOfNumericFeatures - 0.0, NumberOfSymbolicFeatures - 0.0, description - This dataset has been scrapped off Goodreads to obtain land information about the best books of the 19th Century.  \\n\\n\\n\\nFeature\\nDescription\\n\\n\\n\\n\\nBook_Name\\nthe title of the book\\n\\n\\nAuthor_Name\\nthe author(s) of the book\\n\\n\\nDescription\\na brief description about the book\\n\\n\\nRating\\nrating given by Goodreads users', metadata={'did': 43791, 'name': 'Best-Books-of-the-19th-Century'}),\n",
       " Document(page_content=\"This dataset relied heavily on Soumik's Goodreads-books dataset., qualities - AutoCorrelation : nan, Dimensionality : 0.001762114537444934, MajorityClassPercentage : nan, MajorityClassSize : nan, MinorityClassPercentage : nan, MinorityClassSize : nan, NumberOfBinaryFeatures : 0.0, NumberOfClasses : nan, NumberOfFeatures : 12.0, NumberOfInstances : 6810.0, NumberOfInstancesWithMissingValues : 4630.0, NumberOfMissingValues : 5336.0, NumberOfNumericFeatures : 5.0, NumberOfSymbolicFeatures : 0.0, PercentageOfBinaryFeatures : 0.0, PercentageOfInstancesWithMissingValues : 67.98825256975036, PercentageOfMissingValues : 6.529613313754283, PercentageOfNumericFeatures : 41.66666666666667, PercentageOfSymbolicFeatures : 0.0,, features - 0 : [0 - isbn13 (numeric)], 1 : [1 - isbn10 (string)], 2 : [2 - title (string)], 3 : [3 - subtitle (string)], 4 : [4 - authors (string)], 5 : [5 - categories (string)], 6 : [6 - thumbnail (string)], 7 : [7 - description (string)], 8 : [8 - published_year\", metadata={'did': 43542, 'name': '7k-Books'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_6.png)\\n\\n**Meta Album ID**: OCR.MD_6  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_6.html](https://meta-album.github.io/datasets/MD_6.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_6  \\n**Dataset Name**: OmniPrint-MD-6  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 703  \\n**\\\\# Images**: 28120  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44310, 'name': 'Meta_Album_MD_6_Mini'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_5_BIS.png)\\n\\n**Meta Album ID**: OCR.MD_5_BIS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_5_BIS.html](https://meta-album.github.io/datasets/MD_5_BIS.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_5_BIS  \\n**Dataset Name**: OmniPrint-MD-5-bis  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44252, 'name': 'Meta_Album_MD_5_BIS_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_5_BIS.png)\\n\\n**Meta Album ID**: OCR.MD_5_BIS  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_5_BIS.html](https://meta-album.github.io/datasets/MD_5_BIS.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_5_BIS  \\n**Dataset Name**: OmniPrint-MD-5-bis  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 706  \\n**\\\\# Images**: 28240  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44296, 'name': 'Meta_Album_MD_5_BIS_Mini'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_6.png)\\n\\n**Meta Album ID**: OCR.MD_6  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_6.html](https://meta-album.github.io/datasets/MD_6.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_6  \\n**Dataset Name**: OmniPrint-MD-6  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44280, 'name': 'Meta_Album_MD_6_Micro'}),\n",
       " Document(page_content='Please cite the following paper if you use this dataset:', metadata={'did': 4540, 'name': 'ParkinsonSpeechDatasetwithMultipleTypesofSoundRecordings'}),\n",
       " Document(page_content='This is a dataset i generated during a hackathon for project purpose. Here i have scrapped data from Coursera official web site.  Our project aims to help any new learner get the right course to learn by just answering a few questions. It is an intelligent course recommendation system. Hence we had to scrap data from few educational websites. This is data scrapped from Coursera website. For the project visit: https://github.com/Siddharth1698/Coursu . Please do show your support by following us. I have just started to learn on data science and hope this dataset will be helpful to someone for his/her personal purposes. The scrapping code is here : https://github.com/Siddharth1698/Coursera-Course-Dataset\\nArticle about the dataset generation : https://medium.com/analytics-vidhya/web-scraping-and-coursera-8db6af45d83f', metadata={'did': 43381, 'name': 'Coursera-Course-Dataset'}),\n",
       " Document(page_content='### **Other versions of this dataset**', metadata={'did': 44243, 'name': 'Meta_Album_MD_MIX_Micro'}),\n",
       " Document(page_content='I also excluded any removed comments / comments whose author got deleted and comments deemed too short (less than 4 tokens) and changed the format (json - csv).\\nThis is primarily a NLP dataset, but in addition to the comments I added the 3 features I deemed the most important, I also aimed for feature type variety.\\nThe information kept here is:', metadata={'did': 43504, 'name': '1-million-Reddit-comments-from-40-subreddits'}),\n",
       " Document(page_content=\"This Dataset, consist of Tweets regarding Machine Learning, posted by Prof.Andrew Ng,  Co-founder of Coursera, and an Adjunct Professor of Computer Science at Stanford University. His machine learning course is the MOOC that had led to the founding of Coursera!\\nHe's one of my idol, in this huge field, Learned tonnes of new things from him, so decided to come up with this specific Dataset, which is dedicated towards Natural Language Processing.\\nThank-you!\\nContent\\nThe Dataset Consist of the 8 Columns, specifically Id, username, Name, Tweet, conversation_id, timezone ,time, and Polarity.\\nSo, Basically Polarity column is the sentiment Analysed column, \\nwhere,\\n1 = Neutral Tweets\\n2 = Slightly Negative Tweets\\n3 = Negative Tweets\\n4 = Slightly Positive Tweets\\n5 = Positive Tweets\\nInspiration\\nThis Dataset would be wonderful to work on Sentiment Analysis and NLP.\\nThankyou so Much to work on this Dataset!\\nIf you like this Dataset,\", metadata={'did': 43570, 'name': 'AndrewNg-Machine-Learning-Tweets'}),\n",
       " Document(page_content='Data Set Information:\\n \\ndataset are derived from the customers reviews in Amazon Commerce Website for authorship identification. Most previous studies conducted the identification experiments for two to ten authors. But in the online context, reviews to be identified usually have more potential authors, and normally classification algorithms are not adapted to large number of target classes. To examine the robustness of classification algorithms, we identified 50 of the most active users (represented by a unique ID and username) who frequently posted reviews in these newsgroups. The number of reviews we collected for each author is 30.', metadata={'did': 1457, 'name': 'amazon-commerce-reviews'}),\n",
       " Document(page_content='* Dataset:', metadata={'did': 1558, 'name': 'bank-marketing'}),\n",
       " Document(page_content=\"### Sources\\n\\nThe dataset was created by Tactile Srl, Brescia, Italy (http://www.tattile.it) and donated in 1994 to Semeion Research Center of Sciences of Communication, Rome, Italy (http://www.semeion.it), for machine learning research. \\n\\nFor any questions, e-mail Massimo Buscema (m.buscema '@' semeion.it) or Stefano Terzi (s.terzi '@' semeion.it)\\n\\n### DataSet Information\\n\\nA total of 1593 handwritten digits from around 80 persons were scanned, stretched in a rectangular box 16x16 in a gray scale of 256 values. Then each pixel of each image was scaled into a boolean (1/0) value using a fixed threshold. \\n\\nEach person wrote in a paper all the digits from 0 to 9, twice. The commitment was to write the digit the first time in the normal way (trying to write each digit accurately) and the second time in a fast way (with no accuracy). \\n\\nThe best validation protocol for this dataset seems to be a 5x2CV, 50% Tune (Train +Test) and completely blind 50% Validation\\n\\n### Attribute Information\", metadata={'did': 1501, 'name': 'semeion'}),\n",
       " Document(page_content='Acknowledgements\\nPlease cite the following papers if you use this dataset:', metadata={'did': 43380, 'name': 'US-Weather-Events-(2016---2020)'}),\n",
       " Document(page_content='#Dataset from the LIBSVM data repository', metadata={'did': 1433, 'name': 'svmguide1'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 26  \\n**\\\\# Images**: 1596  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44332, 'name': 'Meta_Album_MED_LF_Extended'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 25  \\n**\\\\# Images**: 1000  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44299, 'name': 'Meta_Album_MED_LF_Mini'}),\n",
       " Document(page_content='Information about the dataset\\nCLASSTYPE: nominal', metadata={'did': 446, 'name': 'prnn_crabs'}),\n",
       " Document(page_content='Information about the dataset\\n CLASSTYPE: nominal', metadata={'did': 23, 'name': 'cmc'}),\n",
       " Document(page_content='**Source**: NWPU-RESISC45 Dataset  \\n**Source URL**: https://gcheng-nwpu.github.io/  \\n  \\n**Original Author**: Gong Cheng, Junwei Han, and Xiaoqiang Lu  \\n**Original contact**: chenggong1119@gmail.com  \\n\\n**Meta Album author**: Phan Anh VU  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)', metadata={'did': 44246, 'name': 'Meta_Album_RESISC_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_MIX.png)\\n\\n**Meta Album ID**: OCR.MD_MIX  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_MIX.html](https://meta-album.github.io/datasets/MD_MIX.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_MIX  \\n**Dataset Name**: OmniPrint-MD-mix  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44243, 'name': 'Meta_Album_MD_MIX_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MED_LF.png)\\n\\n**Meta Album ID**: PLT_DIS.MED_LF  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MED_LF.html](https://meta-album.github.io/datasets/MED_LF.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: MED_LF  \\n**Dataset Name**: Medicinal Leaf  \\n**Short Description**: Healthy Medicinal Leaf  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: medicinal leaf, plants, plant diseases  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://data.mendeley.com/datasets/nnytj2v3n5/1\\nhttps://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44314, 'name': 'Meta_Album_MED_LF_Micro'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/MD_MIX.png)\\n\\n**Meta Album ID**: OCR.MD_MIX  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/MD_MIX.html](https://meta-album.github.io/datasets/MD_MIX.html)  \\n**Domain ID**: OCR  \\n**Domain Name**: Optical Character Recognition  \\n**Dataset ID**: MD_MIX  \\n**Dataset Name**: OmniPrint-MD-mix  \\n**Short Description**: Character images with a specific set of nuisance parameters  \\n**\\\\# Classes**: 706  \\n**\\\\# Images**: 28240  \\n**Keywords**: ocr  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: CC BY 4.0  \\n**License URL(original data release)**: https://creativecommons.org/licenses/by/4.0/\\n \\n**License (Meta-Album data release)**: CC BY 4.0  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44287, 'name': 'Meta_Album_MD_MIX_Mini'}),\n",
       " Document(page_content='This S dump contains 22 data sets from the\\nbook Visualizing Data published by\\nHobart Press (books@hobart.com).\\nThe dump was created by data.dump()\\nand can be read back into S by data.restore().\\nThe name of each S data set is the name of\\nthe data set used in the book. To find the\\ndescription of the data set in the book look\\nunder the entry - data, name - in the index.\\nFor example, one data set is barley.\\nTo find the description of barley, look\\nin the index under the entry - data, barley.\\n\\nFile: ../data/visualizing/soil.csv', metadata={'did': 44041, 'name': 'visualizing_soil'}),\n",
       " Document(page_content=\"In order to keep the text data clean, I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed.\\nInspiration\\nThe dataset is large and informative, I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!\\n\\nFit a regression model on reviews and score to see which words are more indicative to a higher/lower score\\nPerform a sentiment analysis on the reviews\\nFind correlation between reviewer's nationality and scores.\\nBeautiful and informative visualization on the dataset.\\nClustering hotels based on reviews\\nSimple recommendation engine to the guest who is fond of a special characteristic of hotel.\", metadata={'did': 43712, 'name': '515K-Hotel-Reviews-Data-in-Europe'}),\n",
       " Document(page_content='**Past Usage**  \\nS. D. Bay and M. J. Pazzani. (1999) \"Detecting Group Differences: Mining Contrast Sets\". submitted.\\n\\n**Copyright Information**  \\nAll persons are granted a limited license to use and distribute this documentation and the accompanying data, subject to the following conditions:\\n* No fee may be charged for use or distribution.\\n* Publications and research reports based on the database must cite it appropriately. The citation should include the following: Steven Ruggles and Matthew Sobek et. al. Integrated Public Use Microdata Series: Version 2.0 Minneapolis: Historical Census Projects, University of Minnesota, 1997\\n\\nIf possible, citations should also include the URL for the IPUMS site: http://www.ipums.umn.edu/.\\n\\nIn addition, we request that users send us a copy of any publications, research reports, or educational material making use of the data or documentation. Send all electronic material to ipums@hist.umn.edu\\n\\nReferences', metadata={'did': 378, 'name': 'ipums_la_99-small'}),\n",
       " Document(page_content='Acknowledgements\\nAny publications based on this dataset should acknowledge the following: \\nLichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.', metadata={'did': 43435, 'name': 'Default-of-Credit-Card-Clients-Dataset'}),\n",
       " Document(page_content='Acknowledgements\\nThe dataset was collected from http://www.myanimelist.net\\nInspiration', metadata={'did': 43508, 'name': 'Anime-Data'}),\n",
       " Document(page_content=\"Source:\\n\\nThis dataset was created by Pedro F. B. Silva and Andre R. S. Marcal using leaf specimens collected by Rubim Almeida da Silva at the Faculty of Science, University of Porto, Portugal.\\n\\n\\nData Set Information:\\n\\nFor further details on this dataset and/or its attributes, please read the 'ReadMe.pdf' file included and/or consult the Master's Thesis 'Development of a System for Automatic Plant Species Recognition' available at [Web Link].\\n\\n\\nAttribute Information:\", metadata={'did': 1482, 'name': 'leaf'}),\n",
       " Document(page_content='Data Set Information:', metadata={'did': 294, 'name': 'satellite_image'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/PLT_DOC.png)\\n\\n**Meta Album ID**: PLT_DIS.PLT_DOC  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/PLT_DOC.html](https://meta-album.github.io/datasets/PLT_DOC.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: PLT_DOC  \\n**Dataset Name**: Plant Doc  \\n**Short Description**: Plant disease dataset  \\n**\\\\# Classes**: 20  \\n**\\\\# Images**: 800  \\n**Keywords**: plants, plant diseases,  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Creative Commons Attribution 4.0 International  \\n**License URL(original data release)**: https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset/blob/master/LICENSE.txt\\n \\n**License (Meta-Album data release)**: Creative Commons Attribution 4.0 International  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44273, 'name': 'Meta_Album_PLT_DOC_Micro'}),\n",
       " Document(page_content='The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML., qualities - AutoCorrelation : 0.9967486640028651, CfsSubsetEval_DecisionStumpAUC : 0.867329467101716, CfsSubsetEval_DecisionStumpErrRate : 0.0007584083256380636, CfsSubsetEval_DecisionStumpKappa : 0.751920067162127, CfsSubsetEval_NaiveBayesAUC : 0.867329467101716, CfsSubsetEval_NaiveBayesErrRate : 0.0007584083256380636, CfsSubsetEval_NaiveBayesKappa : 0.751920067162127, CfsSubsetEval_kNN1NAUC : 0.867329467101716, CfsSubsetEval_kNN1NErrRate : 0.0007584083256380636, CfsSubsetEval_kNN1NKappa : 0.751920067162127, ClassEntropy : 0.018343407709838624, DecisionStumpAUC : 0.8365634010656622, DecisionStumpErrRate : 0.0013553037671124655,', metadata={'did': 1597, 'name': 'creditcard'}),\n",
       " Document(page_content='**Source**: Medicinal Leaf Dataset  \\n**Source URL**: https://data.mendeley.com/datasets/nnytj2v3n5/1  \\n  \\n**Original Author**: S Roopashree, J Anitha  \\n**Original contact**:   \\n\\n**Meta Album author**: Phan Anh VU  \\n**Created Date**: 01 March 2022  \\n**Contact Name**: Ihsan Ullah  \\n**Contact Email**: meta-album@chalearn.org  \\n**Contact URL**: [https://meta-album.github.io/](https://meta-album.github.io/)  \\n\\n\\n\\n### **Cite this dataset**\\n```\\n @article{s_j_2020, \\n     title={Medicinal Leaf Dataset}, \\n     url={https://data.mendeley.com/datasets/nnytj2v3n5/1}, \\n     author={S, Roopashree and J, Anitha}, \\n     year={2020}, \\n     month={Oct},\\n     doi={10.17632/nnytj2v3n5.1},\\n     version={1},\\n     publisher={Mendeley Data}   \\n} \\n```', metadata={'did': 44299, 'name': 'Meta_Album_MED_LF_Mini'}),\n",
       " Document(page_content=\"b. Donor of database \\nThis version of the database was prepared for the NIPS 2003 variable and feature selection benchmark by Isabelle Guyon, 955 Creston Road, Berkeley, CA 94708, USA (isabelle '@' clopinet.com). \\n\\n\\nData Set Information:\\n\\nThe original data were formatted by Thorsten Joachims in the &ldquo;bag-of-words&rdquo; representation. There were 9947 features (of which 2562 are always zeros for all the examples) representing frequencies of occurrence of word stems in text. The task is to learn which Reuters articles are about 'corporate acquisitions'. We added a number of distractor feature called 'probes' having no predictive power. The order of the features and patterns were randomized. \\n\\nDEXTER -- Positive ex. -- Negative ex. -- Total \\nTraining set --150 -- 150 -- 300 \\nValidation set -- 150 -- 150 -- 300 \\nTest set -- 1000 -- 1000 -- 2000 \\nAll -- 1300 -- 1300 -- 2600\", metadata={'did': 4136, 'name': 'Dexter'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/PLT_DOC.png)\\n\\n**Meta Album ID**: PLT_DIS.PLT_DOC  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/PLT_DOC.html](https://meta-album.github.io/datasets/PLT_DOC.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: PLT_DOC  \\n**Dataset Name**: Plant Doc  \\n**Short Description**: Plant disease dataset  \\n**\\\\# Classes**: 27  \\n**\\\\# Images**: 1080  \\n**Keywords**: plants, plant diseases,  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Creative Commons Attribution 4.0 International  \\n**License URL(original data release)**: https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset/blob/master/LICENSE.txt\\n \\n**License (Meta-Album data release)**: Creative Commons Attribution 4.0 International  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44303, 'name': 'Meta_Album_PLT_DOC_Mini'}),\n",
       " Document(page_content='### **Dataset Details**\\n![](https://meta-album.github.io/assets/img/samples/PLT_DOC.png)\\n\\n**Meta Album ID**: PLT_DIS.PLT_DOC  \\n**Meta Album URL**: [https://meta-album.github.io/datasets/PLT_DOC.html](https://meta-album.github.io/datasets/PLT_DOC.html)  \\n**Domain ID**: PLT_DIS  \\n**Domain Name**: Plant Diseases  \\n**Dataset ID**: PLT_DOC  \\n**Dataset Name**: Plant Doc  \\n**Short Description**: Plant disease dataset  \\n**\\\\# Classes**: 27  \\n**\\\\# Images**: 2549  \\n**Keywords**: plants, plant diseases,  \\n**Data Format**: images  \\n**Image size**: 128x128  \\n\\n**License (original data release)**: Creative Commons Attribution 4.0 International  \\n**License URL(original data release)**: https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset/blob/master/LICENSE.txt\\n \\n**License (Meta-Album data release)**: Creative Commons Attribution 4.0 International  \\n**License URL (Meta-Album data release)**: [https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)', metadata={'did': 44336, 'name': 'Meta_Album_PLT_DOC_Extended'}),\n",
       " Document(page_content='Acknowledgements\\nThe dataset was extracted from investing.com\\nInspiration', metadata={'did': 43391, 'name': 'Ethereum-Cryptocurrency-Historical-Dataset'})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort results by ranking\n",
    "results = [result for result in results if result.metadata[\"did\"] in ids]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "RQA_PROMPT = PromptTemplate(\n",
    "        template=config[\"rqa_prompt_template\"], input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": RQA_PROMPT},\n",
    "        return_source_documents=True,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me a dataset about human diseases\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:145\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:600\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    596\u001b[0m         _output_key\n\u001b[1;32m    597\u001b[0m     ]\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    601\u001b[0m         _output_key\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/combine_documents/stuff.py:244\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/llm.py:316\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:148\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     emit_warning()\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/language_models/llms.py:1317\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1316\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1317\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/llms/llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    289\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m    290\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    291\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    293\u001b[0m     ):\n\u001b[1;32m    294\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/llms/llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    342\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    343\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[1;32m    344\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    345\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[1;32m    346\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/llama_cpp/llama.py:1092\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1090\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1093\u001b[0m     prompt_tokens,\n\u001b[1;32m   1094\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1095\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1096\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1097\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1098\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1099\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1100\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1101\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1102\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1103\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1104\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1105\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1106\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1107\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1108\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1109\u001b[0m ):\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmodel, token):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/llama_cpp/llama.py:720\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    722\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    723\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    724\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    739\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/llama_cpp/llama.py:558\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    554\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    556\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    557\u001b[0m )\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/llama_cpp/_internals.py:317\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 317\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "qa.invoke({\"query\": \"Give me a dataset about human diseases\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
