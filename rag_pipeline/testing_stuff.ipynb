{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tempfile\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain, LLMChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "\n",
    "# from langchain.llms import VertexAI\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    PyPDFLoader,\n",
    "    PyPDFDirectoryLoader,\n",
    "    DirectoryLoader,\n",
    ")\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    TextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pqdm.threads import pqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Union, List\n",
    "\n",
    "import openml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_description(dataset_name) -> openml.datasets.dataset.OpenMLDataset:\n",
    "    try:\n",
    "        data = openml.datasets.get_dataset(dataset_name, download_data = False, download_qualities = False, download_features_meta_data = False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_dataset_metadata_from_openml(save_filename = \"all_dataset_metadata.pkl\") -> Union[List, List]:\n",
    "    # Gather all OpenML datasets\n",
    "    all_datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "\n",
    "    # List dataset 'did' to be used as an identifier \n",
    "    data_id = [all_datasets.iloc[i]['did'] for i in range(len(all_datasets))]\n",
    "\n",
    "    dataset_names = all_datasets['name'].tolist() # get a list of all dataset names\n",
    "\n",
    "    # if the file already exists, load it else get the metadata from openml\n",
    "    if os.path.exists(save_filename):\n",
    "        with open(save_filename, 'rb') as f:\n",
    "            all_data_descriptions = pickle.load(f)\n",
    "        return all_data_descriptions, data_id\n",
    "    else:\n",
    "        # Get all dataset metadata using n_jobs parallel threads from openml\n",
    "        all_data_descriptions = pqdm(dataset_names, get_dataset_description, n_jobs=10)\n",
    "\n",
    "        # Save the metadata to a file\n",
    "        with open(save_filename, 'wb') as f:\n",
    "            pickle.dump(all_data_descriptions, f)\n",
    "        \n",
    "        return all_data_descriptions, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata_dataframe(all_data_descriptions, data_id) -> pd.DataFrame:\n",
    "    descriptions = [all_data_descriptions[i].description for i in range(len(all_data_descriptions))]\n",
    "\n",
    "    all_data_description = dict(zip(data_id, descriptions))\n",
    "\n",
    "    return pd.DataFrame(list(all_data_description.items()),columns = ['did','description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metadata_dataframe(metadata_df) -> pd.DataFrame:\n",
    "    # remove rows with empty descriptions\n",
    "    metadata_df = metadata_df[metadata_df['description'].notna()]\n",
    "    return metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = create_metadata_dataframe(*get_all_dataset_metadata_from_openml())\n",
    "metadata_df = clean_metadata_dataframe(metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Author**: Mary McLeish & Matt Cecile, University of Guelph  \n",
      "Donor: Will Taylor (taylor@pluto.arc.nasa.gov)   \n",
      "**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Horse+Colic) - 8/6/89   \n",
      "\n",
      "**Horse Colic database**  \n",
      "Database of surgeries on horses. Possible class attributes: 24 (whether lesion is surgical), others include: 23, 25, 26, and 27\n",
      "\n",
      "Notes:\n",
      "* Hospital_Number is an identifier and should be ignored when modelling\n",
      "\n",
      "Attribute Information:\n",
      "> \n",
      "   1:  surgery?\n",
      "           1 = Yes, it had surgery\n",
      "           2 = It was treated without surgery  \n",
      "   2:  Age \n",
      "           1 = Adult horse\n",
      "           2 = Young (< 6 months)  \n",
      "   3:  Hospital Number \n",
      "           - numeric id\n",
      "           - the case number assigned to the horse\n",
      "             (may not be unique if the horse is treated > 1 time)  \n",
      "   4:  rectal temperature\n",
      "           - linear\n",
      "           - in degrees celsius.\n",
      "           - An elevated temp may occur due to infection.\n",
      "           - temperature may be reduced when the animal is in late shock\n",
      "           - normal temp is 37.8\n",
      "           - this parameter will usually change as the problem progresses\n",
      "                eg. may start out normal, then become elevated because of\n",
      "                    the lesion, passing back through the normal range as the\n",
      "                    horse goes into shock  \n",
      "   5:  pulse \n",
      "           - linear\n",
      "           - the heart rate in beats per minute\n",
      "           - is a reflection of the heart condition: 30 -40 is normal for adults\n",
      "           - rare to have a lower than normal rate although athletic horses\n",
      "             may have a rate of 20-25\n",
      "           - animals with painful lesions or suffering from circulatory shock\n",
      "             may have an elevated heart rate  \n",
      "   6:  respiratory rate\n",
      "           - linear\n",
      "           - normal rate is 8 to 10\n",
      "           - usefulness is doubtful due to the great fluctuations  \n",
      "   7:  temperature of extremities\n",
      "           - a subjective indication of peripheral circulation\n",
      "           - possible values:\n",
      "                1 = Normal\n",
      "                2 = Warm\n",
      "                3 = Cool\n",
      "                4 = Cold\n",
      "           - cool to cold extremities indicate possible shock\n",
      "           - hot extremities should correlate with an elevated rectal temp.  \n",
      "   8:  peripheral pulse\n",
      "           - subjective\n",
      "           - possible values are:\n",
      "                1 = normal\n",
      "                2 = increased\n",
      "                3 = reduced\n",
      "                4 = absent\n",
      "           - normal or increased p.p. are indicative of adequate circulation\n",
      "             while reduced or absent indicate poor perfusion  \n",
      "   9:  mucous membranes\n",
      "           - a subjective measurement of colour\n",
      "           - possible values are:\n",
      "                1 = normal pink\n",
      "                2 = bright pink\n",
      "                3 = pale pink\n",
      "                4 = pale cyanotic\n",
      "                5 = bright red / injected\n",
      "                6 = dark cyanotic\n",
      "           - 1 and 2 probably indicate a normal or slightly increased\n",
      "             circulation\n",
      "           - 3 may occur in early shock\n",
      "           - 4 and 6 are indicative of serious circulatory compromise\n",
      "           - 5 is more indicative of a septicemia  \n",
      "  10: capillary refill time\n",
      "           - a clinical judgement. The longer the refill, the poorer the\n",
      "             circulation\n",
      "           - possible values\n",
      "                1 = < 3 seconds\n",
      "                2 = >= 3 seconds  \n",
      "  11: pain - a subjective judgement of the horse's pain level\n",
      "           - possible values:\n",
      "                1 = alert, no pain\n",
      "                2 = depressed\n",
      "                3 = intermittent mild pain\n",
      "                4 = intermittent severe pain\n",
      "                5 = continuous severe pain\n",
      "           - should NOT be treated as a ordered or discrete variable!\n",
      "           - In general, the more painful, the more likely it is to require\n",
      "             surgery\n",
      "           - prior treatment of pain may mask the pain level to some extent  \n",
      "  12: peristalsis                              \n",
      "           - an indication of the activity in the horse's gut. As the gut\n",
      "             becomes more distended or the horse becomes more toxic, the\n",
      "             activity decreases\n",
      "           - possible values:\n",
      "                1 = hypermotile\n",
      "                2 = normal\n",
      "                3 = hypomotile\n",
      "                4 = absent  \n",
      "  13: abdominal distension\n",
      "           - An IMPORTANT parameter.\n",
      "           - possible values\n",
      "                1 = none\n",
      "                2 = slight\n",
      "                3 = moderate\n",
      "                4 = severe\n",
      "           - an animal with abdominal distension is likely to be painful and\n",
      "             have reduced gut motility.\n",
      "           - a horse with severe abdominal distension is likely to require\n",
      "             surgery just tio relieve the pressure  \n",
      "  14: nasogastric tube\n",
      "           - this refers to any gas coming out of the tube\n",
      "           - possible values:\n",
      "                1 = none\n",
      "                2 = slight\n",
      "                3 = significant\n",
      "           - a large gas cap in the stomach is likely to give the horse\n",
      "             discomfort  \n",
      "  15: nasogastric reflux\n",
      "           - possible values\n",
      "                1 = none\n",
      "                2 = > 1 liter\n",
      "                3 = < 1 liter\n",
      "           - the greater amount of reflux, the more likelihood that there is\n",
      "             some serious obstruction to the fluid passage from the rest of\n",
      "             the intestine  \n",
      "  16: nasogastric reflux PH\n",
      "           - linear\n",
      "           - scale is from 0 to 14 with 7 being neutral\n",
      "           - normal values are in the 3 to 4 range  \n",
      "  17: rectal examination - feces\n",
      "           - possible values\n",
      "                1 = normal\n",
      "                2 = increased\n",
      "                3 = decreased\n",
      "                4 = absent\n",
      "           - absent feces probably indicates an obstruction  \n",
      "  18: abdomen\n",
      "           - possible values\n",
      "                1 = normal\n",
      "                2 = other\n",
      "                3 = firm feces in the large intestine\n",
      "                4 = distended small intestine\n",
      "                5 = distended large intestine\n",
      "           - 3 is probably an obstruction caused by a mechanical impaction\n",
      "             and is normally treated medically\n",
      "           - 4 and 5 indicate a surgical lesion  \n",
      "  19: packed cell volume\n",
      "           - linear\n",
      "           - the # of red cells by volume in the blood\n",
      "           - normal range is 30 to 50. The level rises as the circulation\n",
      "             becomes compromised or as the animal becomes dehydrated.  \n",
      "  20: total protein\n",
      "           - linear\n",
      "           - normal values lie in the 6-7.5 (gms/dL) range\n",
      "           - the higher the value the greater the dehydration  \n",
      "  21: abdominocentesis appearance\n",
      "           - a needle is put in the horse's abdomen and fluid is obtained from\n",
      "             the abdominal cavity\n",
      "           - possible values:\n",
      "                1 = clear\n",
      "                2 = cloudy\n",
      "                3 = serosanguinous\n",
      "           - normal fluid is clear while cloudy or serosanguinous indicates\n",
      "             a compromised gut  \n",
      "  22: abdomcentesis total protein\n",
      "           - linear\n",
      "           - the higher the level of protein the more likely it is to have a\n",
      "             compromised gut. Values are in gms/dL  \n",
      "  23: outcome\n",
      "           - what eventually happened to the horse?\n",
      "           - possible values:\n",
      "                1 = lived\n",
      "                2 = died\n",
      "                3 = was euthanized  \n",
      "  24: surgical lesion?\n",
      "           - retrospectively, was the problem (lesion) surgical?\n",
      "           - all cases are either operated upon or autopsied so that\n",
      "             this value and the lesion type are always known\n",
      "           - possible values:\n",
      "                1 = Yes\n",
      "                2 = No  \n",
      "  25, 26, 27: type of lesion\n",
      "           - first number is site of lesion\n",
      "                1 = gastric\n",
      "                2 = sm intestine\n",
      "                3 = lg colon\n",
      "                4 = lg colon and cecum\n",
      "                5 = cecum\n",
      "                6 = transverse colon\n",
      "                7 = retum/descending colon\n",
      "                8 = uterus\n",
      "                9 = bladder\n",
      "                11 = all intestinal sites\n",
      "                00 = none\n",
      "           - second number is type\n",
      "                1 = simple\n",
      "                2 = strangulation\n",
      "                3 = inflammation\n",
      "                4 = other\n",
      "           - third number is subtype\n",
      "                1 = mechanical\n",
      "                2 = paralytic\n",
      "                0 = n/a\n",
      "           - fourth number is specific code\n",
      "                1 = obturation\n",
      "                2 = intrinsic\n",
      "                3 = extrinsic\n",
      "                4 = adynamic\n",
      "                5 = volvulus/torsion\n",
      "                6 = intussuption\n",
      "                7 = thromboembolic\n",
      "                8 = hernia\n",
      "                9 = lipoma/slenic incarceration\n",
      "                10 = displacement\n",
      "                0 = n/a\n",
      "  28: cp_data\n",
      "           - is pathology data present for this case?\n",
      "                1 = Yes\n",
      "                2 = No\n",
      "           - this variable is of no significance since pathology data\n",
      "             is not included or collected for these cases\n"
     ]
    }
   ],
   "source": [
    "print(metadata_df.loc[20]['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import langchain_core\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import uuid\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document_and_create_vector_store(metadata_df, persist_directory = \"./chroma_db/\", model_name = \"BAAI/bge-base-en-v1.5\", device = \"cpu\", normalize_embeddings = True, recreate_chroma = False) -> Chroma:\n",
    "    # load model\n",
    "    model_kwargs = {\"device\": device}\n",
    "    encode_kwargs = {\"normalize_embeddings\": normalize_embeddings}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    "    )\n",
    "\n",
    "    # if the directory already exists, load the vector store else create a new one\n",
    "    if os.path.exists(persist_directory) and not recreate_chroma:\n",
    "        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "        return db\n",
    "    else:\n",
    "        # load data\n",
    "        # might need to chunk if the descriptions are too large, fine for now\n",
    "        loader = DataFrameLoader(metadata_df, page_content_column=\"description\")\n",
    "        documents = loader.load() \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "        documents = text_splitter.split_documents(documents)\n",
    "       \n",
    "        ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in documents]\n",
    "        unique_ids = list(set(ids))\n",
    "        # Ensure that only docs that correspond to unique ids are kept and that only one of the duplicate ids is kept\n",
    "        seen_ids = set()\n",
    "        unique_docs = [doc for doc, id in zip(documents, ids) if id not in seen_ids and (seen_ids.add(id) or True)]\n",
    "\n",
    "        # db = Chroma.from_documents(\n",
    "        #     unique_docs, embedding=embeddings, persist_directory=persist_directory, ids = unique_ids)\n",
    "        # db.persist()\n",
    "\n",
    "        # add to chroma db in batches\n",
    "        # https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist\n",
    "        db = Chroma(embedding_function=embeddings)\n",
    "        for i in tqdm(range(0, len(unique_docs), 100)):\n",
    "            db.add_documents(unique_docs[i:i+100], ids = unique_ids[i:i+100])\n",
    "        db.persist()\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "  8%|â–Š         | 5/59 [01:21<14:40, 16.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mload_document_and_create_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# vectordb = load_document_and_create_vector_store(metadata_df, recreate_chroma=True)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[96], line 35\u001b[0m, in \u001b[0;36mload_document_and_create_vector_store\u001b[0;34m(metadata_df, persist_directory, model_name, device, normalize_embeddings, recreate_chroma)\u001b[0m\n\u001b[1;32m     33\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma(embedding_function\u001b[38;5;241m=\u001b[39membeddings)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(unique_docs), \u001b[38;5;241m100\u001b[39m)):\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munique_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m db\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m db\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_core/vectorstores.py:138\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    137\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/vectorstores/chroma.py:276\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/langchain_community/embeddings/huggingface.py:98\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     96\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:371\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    368\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 371\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m truncate_embeddings(\n\u001b[1;32m    373\u001b[0m         out_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtruncate_dim\n\u001b[1;32m    374\u001b[0m     )\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/sentence_transformers/models/Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    981\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    982\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    983\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    987\u001b[0m )\n\u001b[0;32m--> 988\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    572\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    573\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         output_attentions,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    511\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    512\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 514\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:526\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 526\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 426\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectordb = load_document_and_create_vector_store(metadata_df)\n",
    "# vectordb = load_document_and_create_vector_store(metadata_df, recreate_chroma=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever_and_llm(vectordb,model_repo_id = \"HuggingFaceH4/zephyr-7b-beta\", num_return_documents = 50,search_type = \"similarity\"):\n",
    "    HUGGINGFACEHUB_API_KEY = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    # use export HUGGINGFACEHUB_API_TOKEN=your_token_here to set the token (in the shell)\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_type=search_type, search_kwargs={\"k\": num_return_documents})\n",
    "    llm = HuggingFaceHub(repo_id=model_repo_id,model_kwargs={\"temperature\": 0.1, \"max_length\": 512}, huggingfacehub_api_token=HUGGINGFACEHUB_API_KEY)\n",
    "    return retriever, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rqa_prompt_template = \"This database is a list of dataset metadata. Use the following pieces of context to find the relevant document. Answer only from the context given using the {question} given. If you do not know the answer, say you do not know. {context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_chain_and_query(vectordb,retriever, llm, prompt_template = \"Answer {question} from the following context: {context}\"):\n",
    "    RQA_PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    rqa_chain_type_kwargs = {\"prompt\": RQA_PROMPT}\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        # chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=rqa_chain_type_kwargs,\n",
    "        return_source_documents=True,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_documents_from_results(results, metadata_df):\n",
    "    return [doc.metadata['did'] for doc in results[\"source_douments\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever, llm = create_retriever_and_llm(vectordb)\n",
    "qa = create_llm_chain_and_query(vectordb=vectordb,retriever=retriever,llm=llm, prompt_template = rqa_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_result_dataframe(query, qa) -> pd.DataFrame:\n",
    "    results = qa.invoke({\"query\": query})\n",
    "    result_to_dict = {result.metadata['did']: result.page_content for result in results['source_documents']}\n",
    "    return pd.DataFrame(list(result_to_dict.items()),columns = ['did','description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which datasets would be useful for stock market information?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>did</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [did, description]\n",
       "Index: []"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = create_result_dataframe(query, qa)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: 43006\n",
      "Description: This datasets covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates. Sources and thorough description of features have been mentione\n",
      "\n",
      "\n",
      "Dataset ID: 43004\n",
      "Description: This datasets covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates. Sources and thorough description of features have been mentione\n",
      "\n",
      "\n",
      "Dataset ID: 43003\n",
      "Description: This datasets covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates. Sources and thorough description of features have been mentione\n",
      "\n",
      "\n",
      "Dataset ID: 43005\n",
      "Description: This datasets covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates. Sources and thorough description of features have been mentione\n",
      "\n",
      "\n",
      "Dataset ID: 43555\n",
      "Description: Check the movement of the financial market through this dataset\n",
      "Use your creativity and external information\n",
      "\n",
      "\n",
      "Dataset ID: 43385\n",
      "Description: This dataset contains Apple's (AAPL) stock data for the last 10 years (from 2010 to date). I believe insights from this data can be used to build useful price forecasting algorithms to aid investment. I would like to thank Nasdaq for providing access to this rich dataset. I will make sure I update t\n",
      "\n",
      "\n",
      "Dataset ID: 42201\n",
      "Description: analysis of stocks\n",
      "\n",
      "\n",
      "Dataset ID: 43000\n",
      "Description: It covers features from various categories of technical indicators, futures contracts, price of commodities, important indices of markets around the world, price of major companies in the U.S. market, and treasury bill rates. Sources and thorough description of features have been mentioned in the pa\n",
      "\n",
      "\n",
      "Dataset ID: 43488\n",
      "Description: Context\n",
      "Information about more than 4600 companies tradable on Robinhood website\n",
      "Source\n",
      "The information is scraped from Robinhood website\n",
      "Inspiration\n",
      "The dataset contains useful information about Public Companies are being traded in US Stock market. Information about company size, market cap, PE rat\n",
      "\n",
      "\n",
      "Dataset ID: 43402\n",
      "Description: Context\n",
      "The Stock Market forecasting and Modelling has always been the problem most researched by analysts, here we will present a dataset of the Indian Stock Exchange - Nifty50 Index . for use for modelling and forecasting ability by machine learning .\n",
      "Content\n",
      "The Data consists of 9 Rows, directly \n",
      "\n",
      "\n",
      "Dataset ID: 45927\n",
      "Description: Description:\n",
      "This dataset contains information about the products, stock levels, prices, and locations of sales.\n",
      "\n",
      "Columns description:\n",
      "- Product: the name of the products sold.\n",
      "- Stock: the quantity of each product in stock.\n",
      "- Price: the price of each product.\n",
      "- Place: the location where the product\n",
      "\n",
      "\n",
      "Dataset ID: 43741\n",
      "Description: I have created this dataset to showcase the use of predictive modeling using the stock market as a case study. This dataset is designed to help and predict tomorrow's Amazon stock price. If you want to get the most updated dataset you will need to pull them in real time. I have shared my code to pul\n",
      "\n",
      "\n",
      "Dataset ID: 42636\n",
      "Description: Data for an stock long position\n",
      "\n",
      "\n",
      "Dataset ID: 45930\n",
      "Description: Use case:\n",
      "This dataset can be valuable for retailers, supply chain managers, and market analysts. Retailers can use the information on stock levels and prices to make strategic pricing and stocking decisions. Supply chain managers can optimize inventory levels based on product popularity and locatio\n",
      "\n",
      "\n",
      "Dataset ID: 43848\n",
      "Description: Context\n",
      "This Data is gathered from NSE website  for the past three months I am posting this here so people can analyse this data \n",
      " and gather meaningful insights from this.\n",
      "Example -  Probability of Stock ending up at Max Pain with the help of Open Interest.\n",
      "Content\n",
      "The dataset contains stock symbol\n",
      "\n",
      "\n",
      "Dataset ID: 45952\n",
      "Description: Use Case:\n",
      "This dataset is invaluable for researchers and analysts focusing on labor market trends, HR professionals seeking comparative analysis for salary benchmarking or recruiting strategies, and job seekers looking to understand the dynamics of the job market. It can also be used to develop mach\n",
      "\n",
      "\n",
      "Dataset ID: 43801\n",
      "Description: This dataset contains information about used motorcycles\n",
      "This data can be used for a lot of purposes such as price prediction to exemplify the use of linear regression in Machine Learning.\n",
      "The columns in the given dataset are as follows:\n",
      "\n",
      "name\n",
      "selling price\n",
      "year\n",
      "seller type\n",
      "owner\n",
      "km driven\n",
      "ex showro\n",
      "\n",
      "\n",
      "Dataset ID: 43740\n",
      "Description: Context\n",
      "The stock prices dataset of  a ticker is a good start to slice and dice and good for forecasting of the stock prices.\n",
      "The GOOG ticker data is taken\n",
      "Content\n",
      "Dataset is comprising of the below columns and each row having the days data, an year data is pulled into csv file.\n",
      "Date   Open    High \n",
      "\n",
      "\n",
      "Dataset ID: 43522\n",
      "Description: Content\n",
      "This is a dataset I started building for my future personal projects, as I think this kind of data is quite hard to acquire for free and in short time. I started acquiring data on March 21st, 2020 and intend to keep doing that constantly.\n",
      "What you'll have inside this are news extracted from \n",
      "\n",
      "\n",
      "Dataset ID: 45553\n",
      "Description: Further information can be retrieved from the [FICO website](https://community.fico.com/s/explainable-machine-learning-challenge).\n",
      "\n",
      "**Notes**\n",
      "* We have obtained the dataset from [Kaggle](https://www.kaggle.com/datasets/averkiyoliabev/home-equity-line-of-creditheloc)\n",
      "* This is a cleaned version of th\n",
      "\n",
      "\n",
      "Dataset ID: 43144\n",
      "Description: **Dataset description**\n",
      "\n",
      "\n",
      "Dataset ID: 43779\n",
      "Description: Context\n",
      "This Dataset contains the value of the Bitcoin stock from 14th September 2014 till Date \n",
      "Content\n",
      "It is a very simple dataset to both explore and understand the columns are themselves descriptive in nature\n",
      "Acknowledgements\n",
      "SOURCE:\n",
      "https://yahoofinance.com/ \n",
      "Inspiration\n",
      "Just Explore the datase\n",
      "\n",
      "\n",
      "Dataset ID: 45732\n",
      "Description: A brief description of your dataset.\n",
      "\n",
      "\n",
      "Dataset ID: 43485\n",
      "Description: Context\n",
      "To build a AI to predict the stock price of the Dollar currency on IBOVESPA I had to make this dataset.\n",
      "All information collected here is from a standard graphic for stock prices.\n",
      "Content\n",
      "The data is organized by prices and infos per minute.\n",
      "Each row contains:\n",
      "date, open price, maximim value\n",
      "\n",
      "\n",
      "Dataset ID: 43558\n",
      "Description: Context\n",
      "This dataset is a playground for fundamental and technical analysis. This can serve as basic tutorial for time-series data analysis.\n",
      "Content\n",
      "Dataset consists of following files:\n",
      "LTFinanceHoldingsLtdStockPrice2017to2020.csv: The data related to LT Finance Holdings Ltd Stock Price from Feb 201\n",
      "\n",
      "\n",
      "Dataset ID: 294\n",
      "Description: Data Set Information:\n",
      "\n",
      "\n",
      "Dataset ID: 43605\n",
      "Description: The Story\n",
      "This data set was part of my online course material for Data Analysis using Python over at Udemy.\n",
      "The Contents\n",
      "The dataset is very useful for beginners and novice number crunchers looking to run queries in a relatable and easy-to-understand dataset. It includes the data about shoppers of a\n",
      "\n",
      "\n",
      "Dataset ID: 43308\n",
      "Description: Content\n",
      "This dataset is a record of 7 common different fish species in fish market sales. With this dataset, a predictive model can be performed using machine friendly data and estimate the weight of fish can be predicted.\n",
      "\n",
      "Acknowledgements\n",
      "Thanks to all who make Kernels using this dataset and also \n",
      "\n",
      "\n",
      "Dataset ID: 43666\n",
      "Description: Context\n",
      "The data is of National Stock Exchange of India.\n",
      "The data is compiled to felicitate Machine Learning, without bothering much about Stock APIs.\n",
      "Content\n",
      "The data is of National Stock Exchange of India's stock listings for each trading day of 2016 and 2017.\n",
      "A brief description of columns.\n",
      "SYMBO\n",
      "\n",
      "\n",
      "Dataset ID: 43460\n",
      "Description: If you reach this DATASET, please UPVOTE this dataset to show your appreciation\n",
      "DATASET DETAILS:\n",
      "  \n",
      "1.Previous Close:        264.29,\n",
      "2.Open:                265.58,\n",
      "3.Bid:                266.06 x 800,\n",
      "4.Ask:                266.06 x 900,\n",
      "5.Day's Range:            265.31 - 266.27,\n",
      "6.52 Week Range:     \n",
      "\n",
      "\n",
      "Dataset ID: 45951\n",
      "Description: Use Case:\n",
      "This dataset is invaluable for chess enthusiasts, researchers, and developers interested in developing chess-related algorithms, studying patterns and trends in game outcomes and strategies, or creating predictive models on game results based on player ratings and opening moves. It can als\n",
      "\n",
      "\n",
      "Dataset ID: 43769\n",
      "Description: Acknowledgements\n",
      "We wouldn't be here without the help of our web scraping and data mining experts at PromptCloud and DataStock. \n",
      "Inspiration\n",
      "The inspiration for this dataset came from Buzzfeed itself. We thought long and hard about the informative articles that we have on Buzzfeed. So we came up wit\n",
      "\n",
      "\n",
      "Dataset ID: 45961\n",
      "Description: Use Case:\n",
      "This dataset can be instrumental for researchers and data scientists focusing on natural language processing (NLP), sentiment analysis, and trend spotting. It offers a rich resource for training machine learning models aimed at understanding public sentiment, detecting shifts in societal c\n",
      "\n",
      "\n",
      "Dataset ID: 42682\n",
      "Description: data from yahoo finance\n",
      "\n",
      "\n",
      "Dataset ID: 43705\n",
      "Description: Context\n",
      "This is the dataset used in the second chapter of Aurlien Gron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understan\n",
      "\n",
      "\n",
      "Dataset ID: 43463\n",
      "Description: This Dataset is something I found online when I wanted to practice regression models. It is an openly available online dataset at multiple places. Though I do not know the exact origin and collection methodology of the data, I would recommend this dataset to everybody who is just beginning their jou\n",
      "\n",
      "\n",
      "Dataset ID: 43038\n",
      "Description: Some of these attributes might not be use used in your research.  They were\n",
      "originally added to our database to perform calculations.  (Brown, Pelosi & Dirska, 2013) used percent_change_price, percent_change_volume_over_last_wk, days_to_next_dividend,  and percent_return_next_dividend.  We left the \n",
      "\n",
      "\n",
      "Dataset ID: 43846\n",
      "Description: Context\n",
      "This dataset was created to make the project \"AI Learn to invest\" for SaturdaysAI - Euskadi 1st edition. The project can be found in https://github.com/ImanolR87/AI-Learn-to-invest\n",
      "Content\n",
      "More than 400.000 random investments were created with the data from the last 10 years from the NYSE ma\n",
      "\n",
      "\n",
      "Dataset ID: 43591\n",
      "Description: The datasets talk about Stock Market,  in two Industry and each Industry has ten different companies with six columns and 600 rows  ,in the same date for all the 20 companies the period from \n",
      "2020-02-06 to 2020-03-18 one month 30 day .\n",
      "Columns means :\n",
      "Date column : the perioid 2020-02-06 to 2020-03-\n",
      "\n",
      "\n",
      "Dataset ID: 223\n",
      "Description: **Author**:   \n",
      "**Source**: Unknown -   \n",
      "**Please cite**:   \n",
      "\n",
      "This is a dataset obtained from the StatLib repository. Here is the included description:\n",
      "\n",
      " The data provided are daily stock prices from January 1988 through October 1991, for ten aerospace companies.\n",
      "\n",
      " Source: collection of regression da\n",
      "\n",
      "\n",
      "Dataset ID: 43563\n",
      "Description: Context\n",
      "Howdy folks! \n",
      "I have prepared a starter dataset for time series practice. This is my 1st upload. Any questions/feedback are welcome. \n",
      "Content\n",
      "\n",
      "The data was prepared using Alpha Vantage API\n",
      "The data represents historical daily time series for a digital currency (BTC) traded on the Saudi marke\n",
      "\n",
      "\n",
      "Dataset ID: 43479\n",
      "Description: I will be updating this dataset for attributes that might reveal some more information about this dataset.\n",
      "Thanks for stopping by.\n",
      "Peace.\n",
      "\n",
      "\n",
      "Dataset ID: 43426\n",
      "Description: Context\n",
      "This is historical data on cryptocurrency tradings for the period from 2016-01-01 to 2021-02-21.\n",
      "If you enjoy this dataset please upvote so I can see it is popular and I need to update it.\n",
      "Thank you!\n",
      "Content\n",
      "This dataset will be good for data analysis in predicting the price for digital cryp\n",
      "\n",
      "\n",
      "Dataset ID: 43391\n",
      "Description: Content\n",
      "The dataset consists of ETH prices from March-2016 to the current date(1813 days) and the dataset will be updated on a weekly basis. \n",
      "Information regarding the data\n",
      "The data totally consists of 1813 records(1813 days) with 7 columns. The description of the features is given below\n",
      "\n",
      "\n",
      "\n",
      "No\n",
      "Colum\n",
      "\n",
      "\n",
      "Dataset ID: 43369\n",
      "Description: ### Acknowledgements\n",
      "This dataset is a compilation of multiple datasets found on Inside Airbnb.\n",
      "\n",
      "### Inspiration\n",
      "* Can we predict the price of each house in different regions? \n",
      "* Can we describe a region using the names of listings in that region? \n",
      "* What can we learn about different regions from th\n",
      "\n",
      "\n",
      "Dataset ID: 700\n",
      "Description: More information about the data sets and the book can be\n",
      "obtained via gopher at the address\n",
      "swis.stern.nyu.edu\n",
      "\n",
      "The information is filed under\n",
      "---> Academic Departments & Research Centers\n",
      "---> Statistics and Operations Research\n",
      "---> Publications\n",
      "---> A Casebook for a First Course in Statistics and D\n",
      "\n",
      "\n",
      "Dataset ID: 42200\n",
      "Description: dataset for feature extraction\n",
      "\n",
      "\n",
      "Dataset ID: 43706\n",
      "Description: Uber vs Lyft\n",
      "This is a very beginner-friendly dataset. It does contain a lot of NA values. It is a good dataset if you want to use a Linear Regression Model to see the pattern between different predectors such as hour and price.\n",
      "A really amazing part of this dataset is that I have included the corre\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the first 300 characters of each description\n",
    "for i in range(len(results)):\n",
    "    print(f\"Dataset ID: {results.loc[i]['did']}\")\n",
    "    print(f\"Description: {results.loc[i]['description'][:300]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
