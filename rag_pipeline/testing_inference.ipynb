{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.llm import *\n",
    "from modules.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config_and_device(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"type_of_data\"] = \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Metadata files do not exist. Please run the training pipeline first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/CODE/Github/scripts/rag_pipeline/modules/utils.py:159\u001b[0m, in \u001b[0;36mcreate_metadata_dataframe\u001b[0;34m(openml_data_object, data_id, all_dataset_metadata, config)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/all_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype_of_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_description.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    160\u001b[0m         all_data_description_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/all_dataset_description.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m openml_data_object, data_id, all_dataset_metadata \u001b[38;5;241m=\u001b[39m get_all_metadata_from_openml(\n\u001b[1;32m      3\u001b[0m        config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m      4\u001b[0m    )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the combined metadata dataframe\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m metadata_df, all_dataset_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_metadata_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m       \u001b[49m\u001b[43mopenml_data_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_dataset_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# This also downloads the embedding model if it does not exist\u001b[39;00m\n\u001b[1;32m     12\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m load_document_and_create_vector_store(metadata_df, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m~/Documents/CODE/Github/scripts/rag_pipeline/modules/utils.py:163\u001b[0m, in \u001b[0;36mcreate_metadata_dataframe\u001b[0;34m(openml_data_object, data_id, all_dataset_metadata, config)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m all_data_description_df, all_dataset_metadata\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata files do not exist. Please run the training pipeline first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_of_data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mException\u001b[0m: Metadata files do not exist. Please run the training pipeline first."
     ]
    }
   ],
   "source": [
    " # Download the data if it does not exist\n",
    "openml_data_object, data_id, all_dataset_metadata = get_all_metadata_from_openml(\n",
    "        config=config\n",
    "    )\n",
    "# Create the combined metadata dataframe\n",
    "metadata_df, all_dataset_metadata = create_metadata_dataframe(\n",
    "        openml_data_object, data_id, all_dataset_metadata, config=config\n",
    "    )\n",
    "\n",
    "# Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage.\n",
    "# This also downloads the embedding model if it does not exist\n",
    "vectordb = load_document_and_create_vector_store(metadata_df, config=config)\n",
    "\n",
    "# Setup llm chain, initialize the retriever and llm, and setup Retrieval QA\n",
    "qa = initialize_llm_chain(vectordb=vectordb, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>page_content</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18090</td>\n",
       "      <td>sklearn.pipeline.Pipeline(step_0=automl.compon...</td>\n",
       "      <td>id - 18090, full_name - sklearn.pipeline.Pipel...</td>\n",
       "      <td>https://www.openml.org/api/v1/json/flow/18090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18431</td>\n",
       "      <td>sklearn.pipeline.Pipeline(step_0=automl.compon...</td>\n",
       "      <td>id - 18431, full_name - sklearn.pipeline.Pipel...</td>\n",
       "      <td>https://www.openml.org/api/v1/json/flow/18431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18401</td>\n",
       "      <td>sklearn.pipeline.Pipeline(step_0=automl.compon...</td>\n",
       "      <td>id - 18401, full_name - sklearn.pipeline.Pipel...</td>\n",
       "      <td>https://www.openml.org/api/v1/json/flow/18401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18433</td>\n",
       "      <td>sklearn.pipeline.Pipeline(step_0=automl.compon...</td>\n",
       "      <td>id - 18433, full_name - sklearn.pipeline.Pipel...</td>\n",
       "      <td>https://www.openml.org/api/v1/json/flow/18433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18229</td>\n",
       "      <td>sklearn.pipeline.Pipeline(step_0=automl.compon...</td>\n",
       "      <td>id - 18229, full_name - sklearn.pipeline.Pipel...</td>\n",
       "      <td>https://www.openml.org/api/v1/json/flow/18229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               name  \\\n",
       "0  18090  sklearn.pipeline.Pipeline(step_0=automl.compon...   \n",
       "1  18431  sklearn.pipeline.Pipeline(step_0=automl.compon...   \n",
       "2  18401  sklearn.pipeline.Pipeline(step_0=automl.compon...   \n",
       "3  18433  sklearn.pipeline.Pipeline(step_0=automl.compon...   \n",
       "4  18229  sklearn.pipeline.Pipeline(step_0=automl.compon...   \n",
       "\n",
       "                                        page_content  \\\n",
       "0  id - 18090, full_name - sklearn.pipeline.Pipel...   \n",
       "1  id - 18431, full_name - sklearn.pipeline.Pipel...   \n",
       "2  id - 18401, full_name - sklearn.pipeline.Pipel...   \n",
       "3  id - 18433, full_name - sklearn.pipeline.Pipel...   \n",
       "4  id - 18229, full_name - sklearn.pipeline.Pipel...   \n",
       "\n",
       "                                            urls  \n",
       "0  https://www.openml.org/api/v1/json/flow/18090  \n",
       "1  https://www.openml.org/api/v1/json/flow/18431  \n",
       "2  https://www.openml.org/api/v1/json/flow/18401  \n",
       "3  https://www.openml.org/api/v1/json/flow/18433  \n",
       "4  https://www.openml.org/api/v1/json/flow/18229  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %time\n",
    "# query = \"Which datasets would be useful for stock market support?\"\n",
    "# query = \"Which datasets would be useful for heart disease\"\n",
    "# query = \"Which datasets would be useful for flowers\"\n",
    "# query = \"Which datasets would be useful for image classification\"\n",
    "# query = \"My supervisor wants me to work on cloud cover, which datasets can I use\"\n",
    "# query = \"Are there any datasets from the netherlands?\"\n",
    "# query = \"Are there any datasets about farm animals?\"\n",
    "# query = \"Find chinese authors\"\n",
    "query = \"Which flow can I use for classifying categories of data efficiently\"\n",
    "result_data_frame = get_result_from_query(\n",
    "        query=query, qa=qa, config=config\n",
    "    )\n",
    "result_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id - 18090, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectPercentile,step_2=sklearn.tree._classes.DecisionTreeClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectPercentile,step_2=sklearn.tree._classes.DecisionTreeClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18431, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectKBest,step_2=sklearn.tree._classes.DecisionTreeClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectKBest,step_2=sklearn.tree._classes.DecisionTreeClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18401, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=automl.util.sklearn.StackingEstimator(estimator=sklearn.tree._classes.DecisionTreeClassifier),step_2=sklearn.naive_bayes.BernoulliNB)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=automl.util.sklearn.StackingEstimator(estimator=sklearn.tree._classes.DecisionTreeClassifier),step_2=sklearn.naive_bayes.BernoulliNB), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18433, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=automl.util.sklearn.StackingEstimator(estimator=sklearn.tree._classes.DecisionTreeClassifier),step_2=sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=automl.util.sklearn.StackingEstimator(estimator=sklearn.tree._classes.DecisionTreeClassifier),step_2=sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18229, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.preprocessing._data.MinMaxScaler,step_2=sklearn.tree._classes.DecisionTreeClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.preprocessing._data.MinMaxScaler,step_2=sklearn.tree._classes.DecisionTreeClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18440, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectKBest,step_2=sklearn.ensemble._weight_boosting.AdaBoostClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectKBest,step_2=sklearn.ensemble._weight_boosting.AdaBoostClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18112, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.cluster._agglomerative.FeatureAgglomeration,step_2=sklearn.tree._classes.DecisionTreeClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.cluster._agglomerative.FeatureAgglomeration,step_2=sklearn.tree._classes.DecisionTreeClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 20050, full_name - sklearn.pipeline.Pipeline(columntransformer=sklearn.compose._column_transformer.ColumnTransformer(simpleimputer=sklearn.impute._base.SimpleImputer,onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder),decisiontreeclassifier=sklearn.tree._classes.DecisionTreeClassifier)(9), name - sklearn.pipeline.Pipeline(columntransformer=sklearn.compose._column_transformer.ColumnTransformer(simpleimputer=sklearn.impute._base.SimpleImputer,onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder),decisiontreeclassifier=sklearn.tree._classes.DecisionTreeClassifier), version - 9, external_version - openml==0.14.2,sklearn==1.3.2, uploader - 37571,',\n",
       "       'id - 18130, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.preprocessing._discretization.KBinsDiscretizer,step_2=sklearn.tree._classes.DecisionTreeClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.preprocessing._discretization.KBinsDiscretizer,step_2=sklearn.tree._classes.DecisionTreeClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,',\n",
       "       'id - 18100, full_name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.decomposition._factor_analysis.FactorAnalysis,step_2=sklearn.linear_model._stochastic_gradient.SGDClassifier)(1), name - sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.decomposition._factor_analysis.FactorAnalysis,step_2=sklearn.linear_model._stochastic_gradient.SGDClassifier), version - 1, external_version - automl==0.0.1,openml==0.10.2,sklearn==0.22.1, uploader - 12269,'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_frame['page_content'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://www.openml.org/api/v1/json/flow/18090',\n",
       "       'https://www.openml.org/api/v1/json/flow/18431',\n",
       "       'https://www.openml.org/api/v1/json/flow/18401',\n",
       "       'https://www.openml.org/api/v1/json/flow/18433',\n",
       "       'https://www.openml.org/api/v1/json/flow/18229',\n",
       "       'https://www.openml.org/api/v1/json/flow/18440',\n",
       "       'https://www.openml.org/api/v1/json/flow/18112',\n",
       "       'https://www.openml.org/api/v1/json/flow/20050',\n",
       "       'https://www.openml.org/api/v1/json/flow/18130',\n",
       "       'https://www.openml.org/api/v1/json/flow/18100'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_frame['urls'].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectPercentile,step_2=sklearn.tree._classes.DecisionTreeClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectKBest,step_2=sklearn.tree._classes.DecisionTreeClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=automl.util.sklearn.StackingEstimator(estimator=sklearn.tree._classes.DecisionTreeClassifier),step_2=sklearn.naive_bayes.BernoulliNB)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=automl.util.sklearn.StackingEstimator(estimator=sklearn.tree._classes.DecisionTreeClassifier),step_2=sklearn.ensemble._hist_gradient_boosting.gradient_boosting.HistGradientBoostingClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.preprocessing._data.MinMaxScaler,step_2=sklearn.tree._classes.DecisionTreeClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.feature_selection._univariate_selection.SelectKBest,step_2=sklearn.ensemble._weight_boosting.AdaBoostClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.cluster._agglomerative.FeatureAgglomeration,step_2=sklearn.tree._classes.DecisionTreeClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(columntransformer=sklearn.compose._column_transformer.ColumnTransformer(simpleimputer=sklearn.impute._base.SimpleImputer,onehotencoder=sklearn.preprocessing._encoders.OneHotEncoder),decisiontreeclassifier=sklearn.tree._classes.DecisionTreeClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.preprocessing._discretization.KBinsDiscretizer,step_2=sklearn.tree._classes.DecisionTreeClassifier)',\n",
       "       'sklearn.pipeline.Pipeline(step_0=automl.components.feature_preprocessing.multi_column_label_encoder.MultiColumnLabelEncoderComponent,step_1=sklearn.decomposition._factor_analysis.FactorAnalysis,step_2=sklearn.linear_model._stochastic_gradient.SGDClassifier)'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data_frame['name'].values[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
