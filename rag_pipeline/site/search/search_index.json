{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"modules/general_utils/","title":"General utils","text":""},{"location":"modules/general_utils/#general_utils.find_device","title":"<code>find_device(training=False)</code>","text":"<p>Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.</p> <p>Input: training (bool) : Whether the pipeline is being used for training or not.</p> <p>Returns: device (str) : The device to use for the pipeline.</p> Source code in <code>modules/general_utils.py</code> <pre><code>def find_device(training: bool = False ) -&gt; str:\n    \"\"\"\n    Description: Find the device to use for the pipeline. If cuda is available, use it. If not, check if MPS is available and use it. If not, use CPU.\n\n    Input: training (bool) : Whether the pipeline is being used for training or not.\n\n    Returns: device (str) : The device to use for the pipeline.\n    \"\"\"\n    print(\"[INFO] Finding device.\")\n    if torch.cuda.is_available():\n        return \"cuda\"\n    elif torch.backends.mps.is_available():\n        if training == False:\n            # loading metadata on mps for inference is quite slow. So disabling for now.\n            return \"cpu\"\n        return \"mps\"\n    else:\n        return \"cpu\"\n</code></pre>"},{"location":"modules/general_utils/#general_utils.load_config_and_device","title":"<code>load_config_and_device(config_file, training=False)</code>","text":"<p>Description: Load the config file and find the device to use for the pipeline.</p> <p>Input: config_file (str) : The path to the config file. training (bool) : Whether the pipeline is being used for training or not.</p> <p>Returns: config (dict) : The config dictionary + device (str) : The device to use for the pipeline.</p> Source code in <code>modules/general_utils.py</code> <pre><code>def load_config_and_device(config_file: str, training: bool = False) -&gt; dict:\n    \"\"\"\n    Description: Load the config file and find the device to use for the pipeline.\n\n    Input: config_file (str) : The path to the config file.\n    training (bool) : Whether the pipeline is being used for training or not.\n\n    Returns: config (dict) : The config dictionary + device (str) : The device to use for the pipeline.\n    \"\"\"\n    # Check if the config file exists and load it\n    if not os.path.exists(config_file):\n        raise Exception(\"Config file does not exist.\")\n    with open(config_file, \"r\") as f:\n        config = json.load(f)\n\n    # Find device and set it in the config between cpu and cuda and mps if available\n    config[\"device\"] = find_device(training)\n    print(f\"[INFO] Device found: {config['device']}\")\n    return config\n</code></pre>"},{"location":"modules/llm_module/","title":"Llm module","text":""},{"location":"modules/llm_module/#llm.add_documents_to_db","title":"<code>add_documents_to_db(db, unique_docs, unique_ids)</code>","text":"<p>Description: Add documents to the vector store in batches of 200.</p> <p>Input: db (Chroma), unique_docs (list), unique_ids (list)</p> <p>Returns: None</p> Source code in <code>modules/llm.py</code> <pre><code>def add_documents_to_db(db, unique_docs, unique_ids):\n    \"\"\"\n    Description: Add documents to the vector store in batches of 200.\n\n    Input: db (Chroma), unique_docs (list), unique_ids (list)\n\n    Returns: None\n    \"\"\"\n    bs = 512\n    if len(unique_docs) &lt; bs:\n        db.add_documents(unique_docs, ids=unique_ids)\n    else:\n        for i in tqdm(range(0, len(unique_docs), bs)):\n            db.add_documents(unique_docs[i : i + bs], ids=unique_ids[i : i + bs])\n</code></pre>"},{"location":"modules/llm_module/#llm.create_vector_store","title":"<code>create_vector_store(metadata_df, chroma_client, config, embeddings, collection_name)</code>","text":"<p>Description: Create the vector store using Chroma db. The documents are loaded and processed, unique documents are generated, and the documents are added to the vector store.</p> <p>Input: metadata_df (pd.DataFrame), chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)</p> <p>Returns: db (Chroma)</p> Source code in <code>modules/llm.py</code> <pre><code>def create_vector_store(\n    metadata_df: pd.DataFrame, chroma_client:ClientAPI, config: dict, embeddings: HuggingFaceEmbeddings, collection_name: str \n) -&gt; Chroma:\n    \"\"\"\n    Description: Create the vector store using Chroma db. The documents are loaded and processed, unique documents are generated, and the documents are added to the vector store.\n\n    Input: metadata_df (pd.DataFrame), chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)\n\n    Returns: db (Chroma)\n    \"\"\"\n\n    db = Chroma(\n        client=chroma_client,\n        embedding_function=embeddings,\n        persist_directory=config[\"persist_dir\"],\n        collection_name=collection_name,\n    )\n\n    documents = load_and_process_data(\n        metadata_df, page_content_column=\"Combined_information\"\n    )\n    if config[\"testing_flag\"]:\n        # subset the data for testing\n        if config[\"test_subset_2000\"] == True:\n            print(\"[INFO] Subsetting the data to 2000 rows.\")\n            documents = documents[:2000]\n    unique_docs, unique_ids = generate_unique_documents(documents, db)\n\n    print(\n        f\"Number of unique documents: {len(unique_docs)} vs Total documents: {len(documents)}\"\n    )\n    if len(unique_docs) == 0:\n        print(\"No new documents to add.\")\n        return db\n    else:\n        # db.add_documents(unique_docs, ids=unique_ids)\n        add_documents_to_db(db, unique_docs, unique_ids)\n\n    return db\n</code></pre>"},{"location":"modules/llm_module/#llm.generate_unique_documents","title":"<code>generate_unique_documents(documents, db)</code>","text":"Generate unique documents by removing duplicates. This is done by generating unique IDs for the documents and keeping only one of the duplicate IDs. <p>Source: https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist</p> <p>Input: documents (list)</p> <p>Returns: unique_docs (list), unique_ids (list)</p> Source code in <code>modules/llm.py</code> <pre><code>def generate_unique_documents(documents: list, db: Chroma) -&gt; tuple:\n    \"\"\"\n    Description: Generate unique documents by removing duplicates. This is done by generating unique IDs for the documents and keeping only one of the duplicate IDs.\n        Source: https://stackoverflow.com/questions/76265631/chromadb-add-single-document-only-if-it-doesnt-exist\n\n    Input: documents (list)\n\n    Returns: unique_docs (list), unique_ids (list)\n    \"\"\"\n\n    # Remove duplicates based on ID (from database)\n    new_document_ids = set([str(x.metadata[\"did\"]) for x in documents])\n    print(f\"[INFO] Generating unique documents. Total documents: {len(documents)}\")\n    try:\n        old_dids = set([str(x[\"did\"]) for x in db.get()[\"metadatas\"]])\n    except KeyError:\n        old_dids = set([str(x[\"id\"]) for x in db.get()[\"metadatas\"]])\n\n    new_dids = new_document_ids - old_dids\n    documents = [x for x in documents if str(x.metadata[\"did\"]) in new_dids]\n    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS,doc.page_content)) for doc in documents]\n\n    # Remove duplicates based on document content (from new documents)\n    unique_ids = list(set(ids))\n    seen_ids = set()\n    unique_docs = [\n            doc\n            for doc, id in zip(documents, ids)\n            if id not in seen_ids and (seen_ids.add(id) or True)\n        ]\n\n    return unique_docs, unique_ids\n</code></pre>"},{"location":"modules/llm_module/#llm.get_collection_name","title":"<code>get_collection_name(config)</code>","text":"<p>Description: Get the collection name based on the type of data provided in the config.</p> <p>Input: config (dict)</p> <p>Returns: str</p> Source code in <code>modules/llm.py</code> <pre><code>def get_collection_name(config: dict) -&gt; str:\n    \"\"\"\n    Description: Get the collection name based on the type of data provided in the config.\n\n    Input: config (dict)\n\n    Returns: str\n    \"\"\"\n    return {\"dataset\": \"datasets\", \"flow\": \"flows\"}.get(\n        config[\"type_of_data\"], \"default\"\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.initialize_llm_chain","title":"<code>initialize_llm_chain(vectordb, config)</code>","text":"<p>Description: Initialize the LLM chain and setup Retrieval QA with the specified configuration.</p> <p>Input: vectordb (Chroma), config (dict)</p> <p>Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)</p> Source code in <code>modules/llm.py</code> <pre><code>def initialize_llm_chain(\n    vectordb: Chroma,\n    config : dict\n) -&gt; langchain.chains.retrieval_qa.base.RetrievalQA:\n    \"\"\"\n    Description: Initialize the LLM chain and setup Retrieval QA with the specified configuration.\n\n    Input: vectordb (Chroma), config (dict)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    return vectordb.as_retriever(\n        search_type=config[\"search_type\"],\n        search_kwargs={\"k\": config[\"num_return_documents\"]},\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.load_and_process_data","title":"<code>load_and_process_data(metadata_df, page_content_column)</code>","text":"<p>Description: Load and process the data for the vector store. Split the documents into chunks of 1000 characters.</p> <p>Input: metadata_df (pd.DataFrame), page_content_column (str)</p> <p>Returns: chunked documents (list)</p> Source code in <code>modules/llm.py</code> <pre><code>def load_and_process_data(metadata_df: pd.DataFrame, page_content_column: str) -&gt; list:\n    \"\"\"\n    Description: Load and process the data for the vector store. Split the documents into chunks of 1000 characters.\n\n    Input: metadata_df (pd.DataFrame), page_content_column (str)\n\n    Returns: chunked documents (list)\n    \"\"\"\n    # Load data\n    loader = DataFrameLoader(metadata_df, page_content_column=page_content_column)\n    documents = loader.load()\n\n    # Split documents\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n    documents = text_splitter.split_documents(documents)\n\n    return documents\n</code></pre>"},{"location":"modules/llm_module/#llm.load_document_and_create_vector_store","title":"<code>load_document_and_create_vector_store(metadata_df, chroma_client, config)</code>","text":"<p>Loads the documents and creates the vector store. If the training flag is set to True, the documents are added to the vector store. If the training flag is set to False, the vector store is loaded from the persist directory.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_df</code> <code>DataFrame</code> <p>The metadata dataframe.</p> required <code>chroma_client</code> <code>PersistentClient</code> <p>The Chroma client.</p> required <code>config</code> <code>dict</code> <p>The configuration dictionary.</p> required <p>Returns:</p> Name Type Description <code>Chroma</code> <code>Chroma</code> <p>The Chroma vector store.</p> Source code in <code>modules/llm.py</code> <pre><code>def load_document_and_create_vector_store(metadata_df: pd.DataFrame, chroma_client:ClientAPI , config: dict) -&gt; Chroma:\n    \"\"\"\n    Loads the documents and creates the vector store. If the training flag is set to True,\n    the documents are added to the vector store. If the training flag is set to False,\n    the vector store is loaded from the persist directory.\n\n    Args:\n        metadata_df (pd.DataFrame): The metadata dataframe.\n        chroma_client (chromadb.PersistentClient): The Chroma client.\n        config (dict): The configuration dictionary.\n\n    Returns:\n        Chroma: The Chroma vector store.\n    \"\"\"\n    embeddings = load_model(config)\n    collection_name = get_collection_name(config)\n\n    if not config[\"training\"]:\n        return load_vector_store(chroma_client, config, embeddings, collection_name)\n\n    return create_vector_store(\n        metadata_df, chroma_client, config, embeddings, collection_name\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.load_model","title":"<code>load_model(config)</code>","text":"<p>Description: Load the model using HuggingFaceEmbeddings.</p> <p>Input: config (dict)</p> <p>Returns: HuggingFaceEmbeddings</p> Source code in <code>modules/llm.py</code> <pre><code>def load_model(config: dict) -&gt; HuggingFaceEmbeddings | None:\n    \"\"\"\n    Description: Load the model using HuggingFaceEmbeddings.\n\n    Input: config (dict)\n\n    Returns: HuggingFaceEmbeddings\n    \"\"\"\n    print(\"[INFO] Loading model...\")\n    model_kwargs = {\"device\": config[\"device\"], \"trust_remote_code\": True}\n    encode_kwargs = {\"normalize_embeddings\": True}\n    embeddings = HuggingFaceEmbeddings(\n        model_name=config[\"embedding_model\"],\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs,\n        show_progress = True,\n        # trust_remote_code=True\n    )\n    print(\"[INFO] Model loaded.\")\n    return embeddings\n</code></pre>"},{"location":"modules/llm_module/#llm.load_vector_store","title":"<code>load_vector_store(chroma_client, config, embeddings, collection_name)</code>","text":"<p>Description: Load the vector store from the persist directory.</p> <p>Input: chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)</p> <p>Returns: Chroma</p> Source code in <code>modules/llm.py</code> <pre><code>def load_vector_store(chroma_client: ClientAPI, config: dict, embeddings: HuggingFaceEmbeddings, collection_name: str) -&gt; Chroma:\n    \"\"\"\n    Description: Load the vector store from the persist directory.\n\n    Input: chroma_client (chromadb.PersistentClient), config (dict), embeddings (HuggingFaceEmbeddings), collection_name (str)\n\n    Returns: Chroma\n    \"\"\"\n    if not os.path.exists(config[\"persist_dir\"]):\n        raise Exception(\n            \"Persist directory does not exist. Please run the training pipeline first.\"\n        )\n\n    return Chroma(\n        client=chroma_client,\n        persist_directory=config[\"persist_dir\"],\n        embedding_function=embeddings,\n        collection_name=collection_name,\n    )\n</code></pre>"},{"location":"modules/llm_module/#llm.setup_vector_db_and_qa","title":"<code>setup_vector_db_and_qa(config, data_type, client)</code>","text":"<p>Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage. This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.</p> <p>Input: config (dict), data_type (str), client (chromadb.PersistentClient)</p> <p>Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)</p> Source code in <code>modules/llm.py</code> <pre><code>def setup_vector_db_and_qa(config: dict, data_type: str, client:ClientAPI) -&gt; langchain.chains.retrieval_qa.base.RetrievalQA:\n    \"\"\"\n    Description: Create the vector database using Chroma db with each type of data in its own collection. Doing so allows us to have a single database with multiple collections, reducing the number of databases we need to manage.\n    This also downloads the embedding model if it does not exist. The QA chain is then initialized with the vector store and the configuration.\n\n    Input: config (dict), data_type (str), client (chromadb.PersistentClient)\n\n    Returns: qa (langchain.chains.retrieval_qa.base.RetrievalQA)\n    \"\"\"\n\n    config[\"type_of_data\"] = data_type\n    # Download the data if it does not exist\n    openml_data_object, data_id, all_metadata = get_all_metadata_from_openml(\n        config=config\n    )\n    # Create the combined metadata dataframe\n    metadata_df, all_metadata = create_metadata_dataframe(\n        openml_data_object, data_id, all_metadata, config=config\n    )\n    # Create the vector store\n    vectordb = load_document_and_create_vector_store(\n        metadata_df, config=config, chroma_client=client\n    )\n    # Initialize the LLM chain and setup Retrieval QA\n    qa = initialize_llm_chain(vectordb=vectordb, config=config)\n    return qa\n</code></pre>"},{"location":"modules/metadata_module/","title":"Metadata module","text":""},{"location":"modules/metadata_module/#metadata_utils.combine_metadata","title":"<code>combine_metadata(all_dataset_metadata, all_data_description_df)</code>","text":"<p>Description: Combine the descriptions with the metadata table.</p> <p>Input: all_dataset_metadata (pd.DataFrame) : The metadata table, all_data_description_df (pd.DataFrame) : The descriptions</p> <p>Returns: The combined metadata table.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def combine_metadata(all_dataset_metadata: pd.DataFrame, all_data_description_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Combine the descriptions with the metadata table.\n\n    Input: all_dataset_metadata (pd.DataFrame) : The metadata table,\n    all_data_description_df (pd.DataFrame) : The descriptions\n\n    Returns: The combined metadata table.\n    \"\"\"\n    # Combine the descriptions with the metadata table\n    all_dataset_metadata = pd.merge(\n        all_dataset_metadata, all_data_description_df, on=\"did\", how=\"inner\"\n    )\n\n    # Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n\n    all_dataset_metadata[\"Combined_information\"] = all_dataset_metadata.apply(\n        merge_all_columns_to_string, axis=1\n    )\n    return all_dataset_metadata\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.create_combined_information_df","title":"<code>create_combined_information_df(data_id, descriptions, joined_qualities, joined_features)</code>","text":"<p>Description: Create a dataframe with the combined information of the OpenML object.</p> <p>Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object</p> <p>Returns: The dataframe with the combined information of the OpenML object.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def create_combined_information_df(\n    # data_id, descriptions, joined_qualities, joined_features\n    data_id: int| Sequence[int], descriptions: Sequence[str], joined_qualities: Sequence[str], joined_features: Sequence[str]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create a dataframe with the combined information of the OpenML object.\n\n    Input: data_id (int) : The data id, descriptions (list) : The descriptions of the OpenML object, joined_qualities (list) : The joined qualities of the OpenML object, joined_features (list) : The joined features of the OpenML object\n\n    Returns: The dataframe with the combined information of the OpenML object.\n    \"\"\"\n    return pd.DataFrame(\n        {\n            \"did\": data_id,\n            \"description\": descriptions,\n            \"qualities\": joined_qualities,\n            \"features\": joined_features,\n        }\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.create_metadata_dataframe","title":"<code>create_metadata_dataframe(openml_data_object, data_id, all_dataset_metadata, config)</code>","text":"<p>Creates a dataframe with all the metadata, joined columns with all information for the type of data specified in the config. If training is set to False, the dataframes are loaded from the files. If training is set to True, the dataframes are created and then saved to the files.</p> <p>Parameters:</p> Name Type Description Default <code>openml_data_object</code> <code>list</code> <p>The list of OpenML objects.</p> required <code>data_id</code> <code>list</code> <p>The list of data ids.</p> required <code>all_dataset_metadata</code> <code>DataFrame</code> <p>The metadata table.</p> required <code>config</code> <code>dict</code> <p>The config dictionary.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The combined metadata dataframe.</p> <code>DataFrame</code> <p>pd.DataFrame: The updated metadata table.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def create_metadata_dataframe(\n    # openml_data_object, data_id, all_dataset_metadata, config\n    openml_data_object: Sequence[Union[openml.datasets.dataset.OpenMLDataset, openml.flows.flow.OpenMLFlow]], data_id: Sequence[int], all_dataset_metadata: pd.DataFrame, config: dict\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Creates a dataframe with all the metadata, joined columns with all information\n    for the type of data specified in the config. If training is set to False,\n    the dataframes are loaded from the files. If training is set to True, the\n    dataframes are created and then saved to the files.\n\n    Args:\n        openml_data_object (list): The list of OpenML objects.\n        data_id (list): The list of data ids.\n        all_dataset_metadata (pd.DataFrame): The metadata table.\n        config (dict): The config dictionary.\n\n    Returns:\n        pd.DataFrame: The combined metadata dataframe.\n        pd.DataFrame: The updated metadata table.\n    \"\"\"\n    # use os.path.join to ensure compatibility with different operating systems\n    file_path = os.path.join(\n        config[\"data_dir\"], f\"all_{config['type_of_data']}_description.csv\"\n    )\n\n    if not config[\"training\"]:\n        return load_metadata(file_path), all_dataset_metadata\n\n    if config[\"type_of_data\"] == \"dataset\":\n        return process_dataset_metadata(\n            openml_data_object, data_id, all_dataset_metadata, file_path\n        )\n\n    if config[\"type_of_data\"] == \"flow\":\n        return process_flow_metadata(openml_data_object, data_id, file_path)\n\n    raise ValueError(f\"Unsupported type_of_data: {config['type_of_data']}\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.extract_attribute","title":"<code>extract_attribute(attribute, attr_name)</code>","text":"<p>Description: Extract an attribute from the OpenML object.</p> <p>Input: attribute (object) : The OpenML object</p> <p>Returns: The attribute value if it exists, else an empty string.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def extract_attribute(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Extract an attribute from the OpenML object.\n\n    Input: attribute (object) : The OpenML object\n\n    Returns: The attribute value if it exists, else an empty string.\n    \"\"\"\n    return getattr(attribute, attr_name, \"\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_all_metadata_from_openml","title":"<code>get_all_metadata_from_openml(config)</code>","text":"<p>Description: Gets all the metadata from OpenML for the type of data specified in the config. If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.</p> <p>This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.</p> <p>Input: config (dict) : The config dictionary</p> <p>Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def get_all_metadata_from_openml(config: dict) -&gt; Tuple[pd.DataFrame, Sequence[int], pd.DataFrame] | None:\n    \"\"\"\n    Description: Gets all the metadata from OpenML for the type of data specified in the config.\n    If training is set to False, it loads the metadata from the files. If training is set to True, it gets the metadata from OpenML.\n\n    This uses parallel threads (pqdm) and so to ensure thread safety, install the package oslo.concurrency.\n\n\n    Input: config (dict) : The config dictionary\n\n    Returns: all the data descriptions combined with data ids, data ids, and the raw openml objects in a dataframe.\n    \"\"\"\n\n    # save_filename = f\"./data/all_{config['type_of_data']}_metadata.pkl\"\n    # use os.path.join to ensure compatibility with different operating systems\n    save_filename = os.path.join(\n        config[\"data_dir\"], f\"all_{config['type_of_data']}_metadata.pkl\"\n    )\n    # If we are not training, we do not need to recreate the cache and can load the metadata from the files. If the files do not exist, raise an exception.\n    # TODO : Check if this behavior is correct, or if data does not exist, send to training pipeline?\n    if config[\"training\"] == False or config[\"ignore_downloading_data\"] == True:\n        # print(\"[INFO] Training is set to False.\")\n        # Check if the metadata files exist for all types of data\n        if not os.path.exists(save_filename):\n            raise Exception(\n                \"Metadata files do not exist. Please run the training pipeline first.\"\n            )\n        print(\"[INFO] Loading metadata from file.\")\n        # Load the metadata files for all types of data\n        return load_metadata_from_file(save_filename)\n\n    # If we are training, we need to recreate the cache and get the metadata from OpenML\n    if config[\"training\"] == True:\n        print(\"[INFO] Training is set to True.\")\n        # Gather all OpenML objects of the type of data\n        all_objects = get_openml_objects(config[\"type_of_data\"])\n\n        # subset the data for testing\n        if config[\"test_subset_2000\"] == True:\n            print(\"[INFO] Subsetting the data to 2000 rows.\")\n            all_objects = all_objects[:2000]\n\n        data_id = [int(all_objects.iloc[i][\"did\"]) for i in range(len(all_objects))]\n\n        print(\"[INFO] Initializing cache.\")\n        initialize_cache(config[\"type_of_data\"], data_id)\n\n        print(f\"[INFO] Getting {config['type_of_data']} metadata from OpenML.\")\n        openml_data_object = get_metadata_from_openml(config, data_id)\n\n        print(\"[INFO] Saving metadata to file.\")\n        save_metadata_to_file((openml_data_object, data_id, all_objects), save_filename)\n\n        return openml_data_object, data_id, all_objects\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_dataset_description","title":"<code>get_dataset_description(dataset_id)</code>","text":"<p>Get the dataset description from OpenML using the dataset id</p> <p>Input: dataset_id (int) : The dataset id</p> <p>Returns: data (openml.datasets.dataset.OpenMLDataset) : The dataset object from OpenML</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def get_dataset_description(dataset_id) -&gt; openml.datasets.dataset.OpenMLDataset:\n    \"\"\"\n    Get the dataset description from OpenML using the dataset id\n\n    Input: dataset_id (int) : The dataset id\n\n    Returns: data (openml.datasets.dataset.OpenMLDataset) : The dataset object from OpenML\n    \"\"\"\n    # TODO : Check for objects that do not have qualities being not downloaded properly\n    # try:\n    data = openml.datasets.get_dataset(\n        dataset_id=dataset_id,\n        download_data=False,\n        download_qualities=True,\n        download_features_meta_data=True,\n    )\n\n    return data\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_flow_description","title":"<code>get_flow_description(flow_id)</code>","text":"<p>Get the flow description from OpenML using the flow id</p> <p>Input: flow_id (int) : The flow id</p> <p>Returns: data (openml.flows.flow.OpenMLFlow) : The flow object from OpenML</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def get_flow_description(flow_id: int) -&gt; openml.flows.flow.OpenMLFlow:\n    \"\"\"\n    Get the flow description from OpenML using the flow id\n\n    Input: flow_id (int) : The flow id\n\n    Returns: data (openml.flows.flow.OpenMLFlow) : The flow object from OpenML\n    \"\"\"\n    return openml.flows.get_flow(flow_id=flow_id)\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_metadata_from_openml","title":"<code>get_metadata_from_openml(config, data_id)</code>","text":"<p>Get metadata from OpenML using parallel processing.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def get_metadata_from_openml(config, data_id: Sequence[int]):\n    \"\"\"\n    Get metadata from OpenML using parallel processing.\n    \"\"\"\n    if config[\"type_of_data\"] == \"dataset\":\n        return pqdm(\n            data_id, get_dataset_description, n_jobs=config[\"data_download_n_jobs\"]\n        )\n    elif config[\"type_of_data\"] == \"flow\":\n        return pqdm(\n            data_id, get_flow_description, n_jobs=config[\"data_download_n_jobs\"]\n        )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.get_openml_objects","title":"<code>get_openml_objects(type_of_data)</code>","text":"<p>Get OpenML objects based on the type of data.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def get_openml_objects(type_of_data: str):\n    \"\"\"\n    Get OpenML objects based on the type of data.\n    \"\"\"\n    if type_of_data == \"dataset\":\n        return openml.datasets.list_datasets(output_format=\"dataframe\")\n    elif type_of_data == \"flow\":\n        all_objects = openml.flows.list_flows(output_format=\"dataframe\")\n        return all_objects.rename(columns={\"id\": \"did\"})\n    else:\n        raise ValueError(\"Invalid type_of_data specified\")\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.initialize_cache","title":"<code>initialize_cache(type_of_data, data_id)</code>","text":"<p>Initialize cache for the OpenML objects.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def initialize_cache(type_of_data: str, data_id: Sequence[int]) -&gt; None:\n    \"\"\"\n    Initialize cache for the OpenML objects.\n    \"\"\"\n    if type_of_data == \"dataset\":\n        get_dataset_description(data_id[0])\n    elif type_of_data == \"flow\":\n        get_flow_description(data_id[0])\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.join_attributes","title":"<code>join_attributes(attribute, attr_name)</code>","text":"<p>Description: Join the attributes of the OpenML object.</p> <p>Input: attribute (object) : The OpenML object</p> <p>Returns: The joined attributes if they exist, else an empty string. example: \"column - value, column - value, ...\"</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def join_attributes(attribute: object, attr_name: str) -&gt; str:\n    \"\"\"\n    Description: Join the attributes of the OpenML object.\n\n    Input: attribute (object) : The OpenML object\n\n    Returns: The joined attributes if they exist, else an empty string.\n    example: \"column - value, column - value, ...\"\n    \"\"\"\n\n    return (\n        \" \".join([f\"{k} : {v},\" for k, v in getattr(attribute, attr_name, {}).items()])\n        if hasattr(attribute, attr_name)\n        else \"\"\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.load_metadata_from_file","title":"<code>load_metadata_from_file(save_filename)</code>","text":"<p>Load metadata from a file.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def load_metadata_from_file(save_filename: str) -&gt; Tuple[pd.DataFrame, Sequence[int], pd.DataFrame]:\n    \"\"\"\n    Load metadata from a file.\n    \"\"\"\n    with open(save_filename, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.merge_all_columns_to_string","title":"<code>merge_all_columns_to_string(row)</code>","text":"<p>Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> <p>Input: row (pd.Series) : The row of the dataframe</p> <p>Returns: The combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def merge_all_columns_to_string(row: pd.Series) -&gt; str:\n    \"\"\"\n    Description: Create a single column that has a combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n\n    Input: row (pd.Series) : The row of the dataframe\n\n    Returns: The combined string of all the metadata and the description in the form of \"column - value, column - value, ... description\"\n    \"\"\"\n\n    return \" \".join([f\"{col} - {val},\" for col, val in zip(row.index, row.values)])\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.process_dataset_metadata","title":"<code>process_dataset_metadata(openml_data_object, data_id, all_dataset_metadata, file_path)</code>","text":"<p>Description: Process the dataset metadata.</p> <p>Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path</p> <p>Returns: The combined metadata dataframe and the updated metadata table.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def process_dataset_metadata(\n    openml_data_object: Sequence[openml.datasets.dataset.OpenMLDataset], data_id: Sequence[int], all_dataset_metadata: pd.DataFrame, file_path: str\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Description: Process the dataset metadata.\n\n    Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, all_dataset_metadata (pd.DataFrame) : The metadata table, file_path (str) : The file path\n\n    Returns: The combined metadata dataframe and the updated metadata table.\n    \"\"\"\n    descriptions = [\n        extract_attribute(attr, \"description\") for attr in openml_data_object\n    ]\n    joined_qualities = [\n        join_attributes(attr, \"qualities\") for attr in openml_data_object\n    ]\n    joined_features = [join_attributes(attr, \"features\") for attr in openml_data_object]\n\n    all_data_description_df = create_combined_information_df(\n        data_id, descriptions, joined_qualities, joined_features\n    )\n    all_dataset_metadata = combine_metadata(\n        all_dataset_metadata, all_data_description_df\n    )\n\n    all_dataset_metadata.to_csv(file_path)\n\n    return (\n        all_dataset_metadata[[\"did\", \"name\", \"Combined_information\"]],\n        all_dataset_metadata,\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.process_flow_metadata","title":"<code>process_flow_metadata(openml_data_object, data_id, file_path)</code>","text":"<p>Description: Process the flow metadata.</p> <p>Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, file_path (str) : The file path</p> <p>Returns: The combined metadata dataframe and the updated metadata table.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def process_flow_metadata(openml_data_object: Sequence[openml.flows.flow.OpenMLFlow], data_id: Sequence[int], file_path: str) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Description: Process the flow metadata.\n\n    Input: openml_data_object (list) : The list of OpenML objects, data_id (list) : The list of data ids, file_path (str) : The file path\n\n    Returns: The combined metadata dataframe and the updated metadata table.\n    \"\"\"\n    descriptions = [\n        extract_attribute(attr, \"description\") for attr in openml_data_object\n    ]\n    names = [extract_attribute(attr, \"name\") for attr in openml_data_object]\n    tags = [extract_attribute(attr, \"tags\") for attr in openml_data_object]\n\n    all_data_description_df = pd.DataFrame(\n        {\n            \"did\": data_id,\n            \"description\": descriptions,\n            \"name\": names,\n            \"tags\": tags,\n        }\n    )\n\n    all_data_description_df[\"Combined_information\"] = all_data_description_df.apply(\n        merge_all_columns_to_string, axis=1\n    )\n    all_data_description_df.to_csv(file_path)\n\n    return (\n        all_data_description_df[[\"did\", \"name\", \"Combined_information\"]],\n        all_data_description_df,\n    )\n</code></pre>"},{"location":"modules/metadata_module/#metadata_utils.save_metadata_to_file","title":"<code>save_metadata_to_file(data, save_filename)</code>","text":"<p>Save metadata to a file.</p> Source code in <code>modules/metadata_utils.py</code> <pre><code>def save_metadata_to_file(data, save_filename: str):\n    \"\"\"\n    Save metadata to a file.\n    \"\"\"\n    with open(save_filename, \"wb\") as f:\n        pickle.dump(data, f)\n</code></pre>"},{"location":"modules/result_gen/","title":"Result gen","text":""},{"location":"modules/result_gen/#results_gen.aggregate_multiple_queries_and_count","title":"<code>aggregate_multiple_queries_and_count(queries, qa_dataset, config, group_cols=['id', 'name'], sort_by='query')</code>","text":"<p>Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results</p> Input <p>queries: List of queries group_cols: List of columns to group by</p> <p>Returns: Combined dataframe with the results of all queries</p> Source code in <code>modules/results_gen.py</code> <pre><code>def aggregate_multiple_queries_and_count(\n    queries, qa_dataset, config, group_cols=[\"id\", \"name\"], sort_by=\"query\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Aggregate the results of multiple queries into a single dataframe and count the number of times a dataset appears in the results\n\n    Input:\n        queries: List of queries\n        group_cols: List of columns to group by\n\n    Returns: Combined dataframe with the results of all queries\n    \"\"\"\n    combined_df = pd.DataFrame()\n    for query in tqdm(queries, total=len(queries)):\n        result_data_frame = get_result_from_query(\n            query=query, qa=qa_dataset, type_of_query=\"dataset\", config=config\n        )\n        result_data_frame = result_data_frame[group_cols]\n        # Concat with combined_df with a column to store the query\n        result_data_frame[\"query\"] = query\n        combined_df = pd.concat([combined_df, result_data_frame])\n    combined_df = (\n        combined_df.groupby(group_cols)\n        .count()\n        .reset_index()\n        .sort_values(by=sort_by, ascending=False)\n    )\n    return combined_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.check_query","title":"<code>check_query(query)</code>","text":"<p>Description: Performs checks on the query - Replaces %20 with space character (browsers do this automatically when spaces are in the URL) - Removes leading and trailing spaces - Limits the query to 150 characters</p> <p>Input: query (str)</p> <p>Returns: None</p> Source code in <code>modules/results_gen.py</code> <pre><code>def check_query(query: str) -&gt; str:\n    \"\"\"\n    Description: Performs checks on the query\n    - Replaces %20 with space character (browsers do this automatically when spaces are in the URL)\n    - Removes leading and trailing spaces\n    - Limits the query to 150 characters\n\n    Input: query (str)\n\n    Returns: None\n    \"\"\"\n    if query == \"\":\n        raise ValueError(\"Query cannot be empty.\")\n    query = query.replace(\n        \"%20\", \" \"\n    )  # replace %20 with space character (browsers do this automatically when spaces are in the URL)\n    # query = query.replace(\"dataset\", \"\")\n    # query = query.replace(\"flow\", \"\")\n    query = query.strip()\n    query = query[:200]\n    return query\n</code></pre>"},{"location":"modules/result_gen/#results_gen.create_output_dataframe","title":"<code>create_output_dataframe(dict_results, type_of_data, ids_order)</code>","text":"<p>Description: Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.</p> <p>Input: dict_results (dict), type_of_data (str)</p> <p>Returns: A dataframe with the results and duplicate names removed.</p> Source code in <code>modules/results_gen.py</code> <pre><code>def create_output_dataframe(dict_results: dict, type_of_data: str, ids_order: list) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Create an output dataframe with the results. The URLs are API calls to the OpenML API for the specific type of data.\n\n    Input: dict_results (dict), type_of_data (str)\n\n    Returns: A dataframe with the results and duplicate names removed.\n    \"\"\"\n    output_df = pd.DataFrame(dict_results).T.reset_index()\n    # order the rows based on the order of the ids\n    output_df[\"index\"] = output_df[\"index\"].astype(int)\n    output_df = output_df.set_index(\"index\").loc[ids_order].reset_index()\n    # output_df[\"urls\"] = output_df[\"index\"].apply(\n    #     lambda x: f\"https://www.openml.org/api/v1/json/{type_of_data}/{x}\"\n    # )\n    # https://www.openml.org/search?type=data&amp;sort=runs&amp;status=any&amp;id=31\n    output_df[\"urls\"] = output_df[\"index\"].apply(\n        lambda x: f\"https://www.openml.org/search?type={type_of_data}&amp;id={x}\"\n    )\n    output_df[\"urls\"] = output_df[\"urls\"].apply(make_clickable)\n    # data = openml.datasets.get_dataset(\n    # get rows with unique names\n    if type_of_data == \"data\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"dataset = openml.datasets.get_dataset({x})\"\n        )\n    elif type_of_data == \"flow\":\n        output_df[\"command\"] = output_df[\"index\"].apply(\n            lambda x: f\"flow = openml.flows.get_flow({x})\"\n        )\n    output_df = output_df.drop_duplicates(subset=[\"name\"])\n    # order the columns\n    output_df = output_df[[\"index\", \"name\", \"command\", \"urls\", \"page_content\"]].rename(\n        columns={\"index\": \"id\", \"urls\": \"OpenML URL\", \"page_content\": \"Description\"}\n    )\n    return output_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.fetch_results","title":"<code>fetch_results(query, qa, type_of_query, config)</code>","text":"<p>Description: Fetch results for the query using the QA chain.</p> <p>Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str), config (dict)</p> <p>Returns: results[\"source_documents\"] (list)</p> Source code in <code>modules/results_gen.py</code> <pre><code>def fetch_results(query: str, qa: langchain.chains.retrieval_qa.base.RetrievalQA, type_of_query: str, config: dict) -&gt; Sequence[Document]:\n    \"\"\"\n    Description: Fetch results for the query using the QA chain.\n\n    Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str), config (dict)\n\n    Returns: results[\"source_documents\"] (list)\n    \"\"\"\n    results = qa.invoke(\n        input=query,\n        config={\"temperature\": config[\"temperature\"], \"top-p\": config[\"top_p\"]},\n    )\n    if config[\"long_context_reorder\"] == True:\n        results = long_context_reorder(results)\n    id_column = {\"dataset\": \"did\", \"flow\": \"id\", \"data\": \"did\"}\n    id_column = id_column[type_of_query]\n\n    if config[\"reranking\"] == True:\n        try:\n            print(\"[INFO] Reranking results...\")\n            ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/tmp/\")\n            rerankrequest = RerankRequest(\n                query=query,\n                passages=[\n                    {\"id\": result.metadata[id_column], \"text\": result.page_content}\n                    for result in results\n                ],\n            )\n            ranking = ranker.rerank(rerankrequest)\n            ids = [result[\"id\"] for result in ranking]\n            ranked_results = [\n                result for result in results if result.metadata[id_column] in ids\n            ]\n            print(\"[INFO] Reranking complete.\")\n            return ranked_results\n        except Exception as e:\n            print(f\"[ERROR] Reranking failed: {e}\")\n            return results\n\n    else:\n        return results\n</code></pre>"},{"location":"modules/result_gen/#results_gen.get_result_from_query","title":"<code>get_result_from_query(query, qa, type_of_query, config)</code>","text":"<p>Description: Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.</p> <p>Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str)</p> <p>Returns: output_df (pd.DataFrame)</p> Source code in <code>modules/results_gen.py</code> <pre><code>def get_result_from_query(query, qa, type_of_query, config) -&gt; pd.DataFrame:\n    \"\"\"\n    Description: Get the result from the query using the QA chain and return the results in a dataframe that is then sent to the frontend.\n\n    Input: query (str), qa (langchain.chains.retrieval_qa.base.RetrievalQA), type_of_query (str)\n\n    Returns: output_df (pd.DataFrame)\n    \"\"\"\n    if type_of_query == \"dataset\":\n        # Fixing the key_name for dataset because of the way the OpenML API returns the data\n        type_of_query = \"data\"\n    elif type_of_query == \"flow\":\n        type_of_query = \"flow\"\n    else:\n        raise ValueError(f\"Unsupported type_of_data: {type_of_query}\")\n\n    # Process the query\n    query = check_query(query)\n    if query == \"\":\n        return pd.DataFrame()\n    source_documents = fetch_results(\n        query, qa, config=config, type_of_query=type_of_query\n    )\n    dict_results, ids_order = process_documents(source_documents)\n    output_df = create_output_dataframe(dict_results, type_of_query, ids_order)\n\n    return output_df\n</code></pre>"},{"location":"modules/result_gen/#results_gen.long_context_reorder","title":"<code>long_context_reorder(results)</code>","text":"<p>Description: Lost in the middle reorder: the less relevant documents will be at the middle of the list and more relevant elements at beginning / end. See: https://arxiv.org/abs//2307.03172</p> <p>Input: results (list)</p> <p>Returns: reorder results (list)</p> Source code in <code>modules/results_gen.py</code> <pre><code>def long_context_reorder(results: Sequence[Document]) -&gt; Sequence[Document]:\n    \"\"\"\n    Description: Lost in the middle reorder: the less relevant documents will be at the\n    middle of the list and more relevant elements at beginning / end.\n    See: https://arxiv.org/abs//2307.03172\n\n    Input: results (list)\n\n    Returns: reorder results (list)\n    \"\"\"\n    print(\"[INFO] Reordering results...\")\n    reordering = LongContextReorder()\n    results = reordering.transform_documents(results)\n    print(\"[INFO] Reordering complete.\")\n    return results\n</code></pre>"},{"location":"modules/result_gen/#results_gen.make_clickable","title":"<code>make_clickable(val)</code>","text":"<p>Description: Make the URL clickable in the dataframe.</p> Source code in <code>modules/results_gen.py</code> <pre><code>def make_clickable(val : str) -&gt; str:\n    \"\"\"\n    Description: Make the URL clickable in the dataframe.\n    \"\"\"\n    return '&lt;a href=\"{}\"&gt;{}&lt;/a&gt;'.format(val, val)\n</code></pre>"},{"location":"modules/result_gen/#results_gen.process_documents","title":"<code>process_documents(source_documents)</code>","text":"<p>Description: Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.</p> <p>Input: source_documents (list), key_name (str)</p> <p>Returns: dict_results (dict)</p> Source code in <code>modules/results_gen.py</code> <pre><code>def process_documents(source_documents : Sequence[Document]) -&gt; Tuple[OrderedDict, list]:\n    \"\"\"\n    Description: Process the source documents and create a dictionary with the key_name as the key and the name and page content as the values.\n\n    Input: source_documents (list), key_name (str)\n\n    Returns: dict_results (dict)\n    \"\"\"\n    dict_results = OrderedDict()\n    for result in source_documents:\n        dict_results[result.metadata[\"did\"]] = {\n            \"name\": result.metadata[\"name\"],\n            \"page_content\": result.page_content,\n        }\n    ids = [result.metadata[\"did\"] for result in source_documents]\n    return dict_results, ids\n</code></pre>"}]}