{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from en-core-web-lg==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: jinja2 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/taniya_das/Documents/scripts/venv/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import backoff\n",
    "import spacy\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "\n",
    "!python -m spacy download en_core_web_lg # Using word vectors using a large model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all OpenML datasets for semantic tagging\n",
    "all_datasets = openml.datasets.list_datasets(output_format=\"dataframe\")\n",
    "\n",
    "# List dataset 'did' to be used as an identifier \n",
    "data_id = []\n",
    "for i in range(len(all_datasets)):\n",
    "  data_id.append(all_datasets.iloc[i]['did'])\n",
    "\n",
    "# dictonary to hold {'did': dataset_decription}\n",
    "all_data_description = dict.fromkeys(data_id, \"\") \n",
    "\n",
    "\n",
    "for i in range(len(all_datasets)):\n",
    "  dataset_name = all_datasets.iloc[i]['name']\n",
    "  try:\n",
    "    data = openml.datasets.get_dataset(dataset_name, download_data = False, download_features_meta_data = False, download_qualities = False)\n",
    "    all_data_description[all_datasets.iloc[i]['did']] = data.description\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "print(len(all_data_description))\n",
    "\n",
    "# # # Save data description as pickle file\n",
    "# # with open('dataset_description.pickle', 'wb') as f:\n",
    "# #     pickle.dump(all_data_description, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatvely all openml dataset description saved in a file (dataset_description.pickle), can be used here\n",
    "\n",
    "# Load data decription \n",
    "infile = open('data/dataset_description.pickle','rb')\n",
    "all_data_description = pickle.load(infile)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tags\n",
    "\n",
    "tags = [\n",
    "\"Agriculture\",\n",
    "\"Astronomy\",\n",
    "\"Chemistry\",\n",
    "\"Computational Universe\",\n",
    "\"Computer Systems\",\n",
    "\"Culture\",\n",
    "\"Demographics\",\n",
    "\"Earth Science\",\n",
    "\"Economics\",\n",
    "\"Education\",\n",
    "\"Geography\",\n",
    "\"Government\",\n",
    "\"Health\",\n",
    "\"History\",\n",
    "\"Human Activities\",\n",
    "\"Images\",\n",
    "\"Language\",\n",
    "\"Life Science\",\n",
    "\"Machine Learning\",\n",
    "\"Manufacturing\",\n",
    "\"Mathematics\",\n",
    "\"Medicine\",\n",
    "\"Meteorology\",\n",
    "\"Physical Sciences\",\n",
    "\"Politics\",\n",
    "\"Social Media\",\n",
    "\"Sociology\",\n",
    "\"Statistics\",\n",
    "\"Text & Literature\",\n",
    "\"Transportation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prompting GPT-3.5-turbo for picking semantic tag from the given list of tags, given dataset description.\n",
    "Exponential backoff strategy used when prompting. Can use other strategies to speed up (more information - https://backoff-utils.readthedocs.io/en/latest/using.html)\n",
    "\"\"\"\n",
    "\n",
    "# set up OpenAI API key\n",
    "client = OpenAI(api_key = \" \")\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "@backoff.on_exception(backoff.expo,\n",
    " (openai.InternalServerError, openai.OpenAIError, openai.APIStatusError, openai.RateLimitError, openai.APIError, openai.APIConnectionError, openai.Timeout, openai.APIResponseValidationError),\n",
    "                      max_tries = MAX_RETRIES,\n",
    "                      on_backoff=lambda details: print(f\"Retrying in {details} seconds...\"),\n",
    "                      on_giveup=lambda e: print(f\"Max retries reached. Unable to complete the request: {e}\")\n",
    "                      )\n",
    "def completion_using_message(messages, model = \"gpt-3.5-turbo\"):\n",
    "  completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=0.5,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def clean(response):\n",
    "  \"\"\"\n",
    "  clean GPT response\n",
    "  \"\"\"\n",
    "  response = response.strip().replace(\"Tags\",'').replace(\"Tag\", '').strip(\":\").strip().strip(\"'\").strip(\"[\").strip(\"]\").strip(\"'\").strip()\n",
    "  response = response.split(\",\")\n",
    "  for i in range(len(response)):\n",
    "    response[i] = response[i].strip().strip(\"'\").strip(\"'\").strip()\n",
    "\n",
    "  return response\n",
    "\n",
    "\n",
    "def find_closest_tag(word, tags):\n",
    "  \"\"\"\n",
    "  Fing tag semantically similar to GPT generated word by calculating similarity\n",
    "  \"\"\"\n",
    "\n",
    "  nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "  word_token = nlp(word)\n",
    "\n",
    "  highest_similarity = 0\n",
    "  semantic_tag = []\n",
    "  for i in range(len(tags)):\n",
    "    tag_token = nlp(tags[i])\n",
    "    simli = word_token.similarity(tag_token)\n",
    "    if simli > highest_similarity:\n",
    "      highest_similarity = simli\n",
    "      semantic_tag = tags[i]\n",
    "\n",
    "  return semantic_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query GPT one by one for each dataset description to get corresponding semantic tag. If the dataset decsription == '' or None, semantic_tag == ['No description']\n",
    "\n",
    "output = {} # dictionay with {'did':[semantic tags]}\n",
    "null_output = [] #  response by GPT == NULL and dataset description != None\n",
    "exceptions_dict = {} #  error produced by GPT, contains {'did':[Expection]}\n",
    "no_description = [] #  if dataset description == None\n",
    "\n",
    "max_token = 4097 # 1 token ~= 4 chars in English, source: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them \n",
    "\n",
    "with open('output_files/temp.txt', 'w') as writefile:\n",
    "  for key, value in all_data_description.items():\n",
    "    \n",
    "    semantic_tag = []\n",
    "    \n",
    "    if value == None:\n",
    "      semantic_tag = [\"No description\"]\n",
    "      output.update({key:['No description']})\n",
    "      no_description.append(key)\n",
    "\n",
    "    else:\n",
    "      \n",
    "      try:\n",
    "        \n",
    "        # To avoid token limit error, the dataset description is shortened to len = 8000, determined experimentally. \n",
    "        if len(value) > 8000: \n",
    "          value = value[:8000]\n",
    "\n",
    "        # GPT Prompt attempt 1\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert in assigning one or two semantic tags from given list of tags to each given dataset description. You will reply by only picking tags from the given list of tags. Answer as concisely as possible.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Dataset Description: {value}\\nTags: {tags}\\nTags: \",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        response = completion_using_message(messages)\n",
    "\n",
    "        # GPT Prompt attempt 2\n",
    "        if response == []:\n",
    "          messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert in assigning one semantic tag from given list of tags to each given dataset description. You will reply by only picking one tag from the given list of tags. Only pick tag from the given list of tags. Answer as concisely as possible.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Dataset Description: {value}\\nTags: {tags}\\n Assign one Tag to the dataset description: \",\n",
    "            },\n",
    "        ]\n",
    "          response = completion_using_message(messages)\n",
    "\n",
    "\n",
    "        if response != []:\n",
    "          response = clean(response)\n",
    "          for r in response:\n",
    "            r = r.strip()\n",
    "            if r not in tags:\n",
    "              word = find_closest_tag(r, tags)\n",
    "              if word not in semantic_tag and word != []:\n",
    "                semantic_tag.append(word)\n",
    "            else:\n",
    "              semantic_tag.append(r)\n",
    "\n",
    "        # Each dataset can have a maximum of 2 semantic tags\n",
    "        if len(semantic_tag) > 2:\n",
    "          semantic_tag = semantic_tag[:2]\n",
    "\n",
    "        if semantic_tag == []:\n",
    "          null_output.append(key)\n",
    "\n",
    "        print(f\"Tag for dataset: {key} is {semantic_tag}\")\n",
    "        output.update({key:semantic_tag})\n",
    "\n",
    "        break\n",
    "      except openai.OpenAIError as e:\n",
    "        exceptions_dict.update({key:e})\n",
    "        print(f\"exception for dataset: {key} is {e}\")\n",
    "        \n",
    "      except Exception as e:\n",
    "        exceptions_dict.update({key:e})\n",
    "        print(f\"exception for dataset: {key} is {e}\")\n",
    "\n",
    "\n",
    "  # Save the output as a json object.\n",
    "  json_object = json.dumps(output, indent=4)\n",
    "\n",
    "  writefile.write('{}\\n'.format(json_object))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
